import os
from langchain_core.documents import Document
from langchain_community.vectorstores import FAISS
from langchain_community.vectorstores import Chroma
from langchain_openai import OpenAIEmbeddings
from langchain.text_splitter import RecursiveCharacterTextSplitter
from dotenv import load_dotenv

# –ó–∞–≥—Ä—É–∂–∞–µ–º –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ –æ–∫—Ä—É–∂–µ–Ω–∏—è (–Ω–∞–ø—Ä–∏–º–µ—Ä, OPENAI_API_KEY)
os.environ["OPENAI_API_KEY"] = "..."

# –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º —ç–º–±–µ–¥–¥–∏–Ω–≥–∏
embedding_model = OpenAIEmbeddings(model="text-embedding-3-large")

# –ü–∞–ø–∫–∞ —Å —Ç–µ–∫—Å—Ç–æ–≤—ã–º–∏ —Ñ–∞–π–ª–∞–º–∏
DATA_DIR = "data"

def load_and_split_documents(data_dir: str) -> list[Document]:
    documents = []

    for filename in os.listdir(data_dir):
        if filename.endswith(".txt"):
            with open(os.path.join(data_dir, filename), "r", encoding="utf-8") as f:
                text = f.read()

            # –°–æ–∑–¥–∞–µ–º Document
            documents.append(Document(
                page_content=text,
                metadata={"source": filename}
            ))

    # –†–∞–∑–±–∏–≤–∞–µ–º –Ω–∞ —á–∞–Ω–∫–∏
    text_splitter = RecursiveCharacterTextSplitter(
        chunk_size=5000,
        chunk_overlap=500,
        separators=["\n\n", "\n", ".", " "]
    )
    split_docs = text_splitter.split_documents(documents)
    return split_docs

def build_vectorstore(docs: list[Document]) -> Chroma:
    vectorstore = Chroma.from_documents(docs,embedding=embedding_model,
        collection_metadata={"hnsw:space": "l2"})
    return vectorstore

def main():
    print("–ó–∞–≥—Ä—É–∂–∞–µ–º –∏ –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –¥–æ–∫—É–º–µ–Ω—Ç—ã...")
    docs = load_and_split_documents(DATA_DIR)

    print(f"–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —á–∞–Ω–∫–æ–≤: {len(docs)}")

    # –ü–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ –≤–µ–∫—Ç–æ—Ä–Ω–æ–≥–æ —Ö—Ä–∞–Ω–∏–ª–∏—â–∞
    vectorstore = build_vectorstore(docs)
    retriever = vectorstore.as_retriever(search_kwargs={"k": 4})

    # –ü—Ä–∏–º–µ—Ä –∑–∞–ø—Ä–æ—Å–∞
    while True:
        query = input("–í–≤–µ–¥–∏—Ç–µ –∑–∞–ø—Ä–æ—Å (–∏–ª–∏ 'exit'): ").strip()
        if query.lower() == "exit":
            break

        results = retriever.get_relevant_documents(query)

        print("\nüîç –ù–∞–π–¥–µ–Ω–Ω—ã–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã:\n")
        for i, doc in enumerate(results):
            print(f"--- –†–µ–∑—É–ª—å—Ç–∞—Ç {i+1} ---")
            print(doc.page_content)
            print(f"–ò—Å—Ç–æ—á–Ω–∏–∫: {doc.metadata.get('source')}")
            print()

        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ —Ç–æ—á–Ω–æ–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ
        print("üîé –ü–æ–∏—Å–∫ —Ç–æ—á–Ω–æ–≥–æ –≤—Ö–æ–∂–¥–µ–Ω–∏—è —Ü–∏—Ç–∞—Ç—ã –≤ –±–∞–∑–µ...")
        found = False
        for doc in docs:
            if query in doc.page_content:
                print("‚úÖ –ù–∞–π–¥–µ–Ω–æ —Ç–æ—á–Ω–æ–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ!")
                print(doc.page_content)
                print(f"–ò—Å—Ç–æ—á–Ω–∏–∫: {doc.metadata}")
                found = True
                break
        if not found:
            print("‚ùå –¢–æ—á–Ω–æ–≥–æ —Å–æ–≤–ø–∞–¥–µ–Ω–∏—è –Ω–µ –Ω–∞–π–¥–µ–Ω–æ.")

if __name__ == "__main__":
    main()