Preface:
Long after its discovery in 1920, x-ray absorption fine structure (XAFS) was the domain of physicists who explored and theorized about the mysterious energy-dependent structures that appeared at energies above the absorption edge in x-ray spectra. In 1971, Dale Sayers, Ed Stern, and Farrel Lytle finally provided a satisfactory description of the physical process that created these features. This description, which incorporated the application of Fourier transforms, provided a readily understood connection to the geometry of the material being measured, immediately suggesting that XAFS could be a powerful tool for characterizing a wide range of materials.A short time thereafter, x-rays from synchrotron light sources began to be used to generate x-ray absorption spectra, dramatically improving the speed and accuracy of data collection. The next two decades featured a rapid development of the theory connecting structure to spectrum. This, along with the rapidly increasing computing power available to the typical scientist, spurred the development and dissemination of software that could aid in the analysis of XAFS.But even with the most powerful software, XAFS is not a black box.The Fourier transform provides an evocative connection to structure, but it does not provide a method by which the structure of a substance can be read directly. The analysis of XAFS is a skill that has to be learned.XAFS is no longer the sole domain of physicists. It is used as a tool in fields as diverse as materials science, synthetic chemistry, environmental science, structural biology, and cultural fields such as archaeology and art conservation.The "Everyone" in XAFS for Everyone, therefore, includes physicists and archaeologists, undergraduates and mid-career scientists, front-line researchers and referees and heads of research programs. I wrote this book expecting that many of my readers would have no more knowledge of physics than what is covered in a freshman class, and I used calculus only sparingly. However, I did not hesitate to borrow examples and terminologies from multiple disciplines, so that you will find "dopants" and "cyclic voltammetry" jostling side by side with "ligands," soil samples, and operando experiments in catalysis.Most likely, however, the first thing that struck you when you leafed through this book was the cartoon characters. Have we really come, you may have wondered, to the point that sophisticated scientific ideas have to be livened up by talking kangaroos and monocle-wearing ducks?xv Actually, using colorful characters to disseminate scientific ideas is not new. Galileo, for instance, in his preface to the Dialogue Concerning the Two Chief World Systems, writes:I have thought it most appropriate to explain these concepts in the form of dialogues, which, not being restricted to the rigorous observance of mathematical laws, make room also for digressions which are sometimes no less interesting than the principle argument.The question of whether it is better to prepare a sample for XAFS measurements by spreading powder on tape or by diluting it and pressing it into a pellet may not be as weighty as whether the Sun circles the Earth or vice versa, but it lends itself to a similar treatment. While XAFS experts usually agree on the result of an analysis, there is great variety in the strategies they use to get to that result. Rather than clutter this book up with a formal discussion of each approach, I put them in the mouths of seven characters, each of whom is then free to speak its mind without fear of derailing the primary narrative. The characters can also use asides to speak to different slices of the broad audience for which the book is aimed; a physicist might want to see the details of a mathematical derivation, a biologist particular tricks for dealing with fragile samples, an undergraduate something about the culture of scientific research, and a graduate student writing his or her first article a little about conventions in publication.Galileo's approach, however, is not without its perils. One of his characters was the hapless Simplicio, an earnest follower of the old, Aristotelian ideas. His role in the dialogues was to advance common but incorrect arguments that could then be knocked down by the others. Unfortunately, Galileo's enemies suggested to Pope Urban VIII that Simplicio was in fact modeled after His Holiness the Pope. This did not prove helpful to Galileo's subsequent career.While I don't expect this work to lead to a trial for heresy, I am alert to the possibility that some readers may attempt to identify the characters with particular experts in the field. It seems wise, therefore, to briefly discuss how they were developed and how they are used.The characters were created by three of my undergraduate research assistants: Blaine Alleluia, Sydney Alvis-Jennings, and Lauren Glowzenski. These three had a practical knowledge of XAFS and used it to develop the list of characters, including the basic approach each would express and the animal that would represent them. Another undergraduate student, Kirin Furst (now the book's illustrator), then wrote an early draft of what is now Chapter 3, including comments from the characters. With their personalities thus established, I have gone on to develop them in the remaining chapters. Since these students had little knowledge or contact with others in the larger XAFS community, the characters are based on ideas, not individuals.

xvi Preface:
Of course, they sometimes express sentiments that have been emphasized by one member or another of the XAFS community. But real experts incorporate traits from all of the characters. Whenever one of us emphasizes physical processes we are like Carvaka the owl; when we choose to begin with a fixed protocol we are like Robert the duck; when we think about XAFS in the context of a particular research problem we are like Kitsune the lemur, and so on. I have personally said things that come out of the mouths of each of the characters and that includes my own incarnation of Simplicio, now a cartoon dinosaur, who often expresses ideas that I held at one point or another in my career and have since abandoned.I have occasionally been asked about the names of the characters. Besides Simplicio, a tribute to Galileo's character of the same name, they are named after the late mathematician Benoît Mandelbrot, whom I once had the pleasure of seeing speak; Brigadier General Henry Martyn Robert, the American military engineer who authored Robert's Rules of Order; the Cārvāka (pronounced "Charvaka") school of ancient materialistic philosophy in India; the ancient Greek goddess Dysnomia; and Kitsune (pronounced Kit-soo-nay), the fox of Japanese folklore. Mr. Handy's name needs no explanation.

Introduction XAFS For My BIrthdAy:
Aside from the Preface and a brief word at the end, this is the only time in this book that I will speak to you directly as the author. For the rest of the book, you will see conventional text accompanied by a panel of cartoon characters providing a running commentary.Each of those characters represents one facet of my own approach to x-ray absorption fine structure (XAFS)-actually one facet of the approach of any successful practitioner in the field. When I sit at the beamline trying to make as much of my limited time as I can and I find myself tempted to take a shortcut, it is Dysnomia, the irreverent kangaroo, hopping on my shoulder to whisper in my ear. When I find myself wondering, as I analyze the spectra from a sample, what is physically happening in the sample, that is Carvaka, my grumpy owl. Mr. Handy, the excitable octopus, warns me when I am thoughtlessly about to make a mistake that has been made a hundred times before and reminds me how to correct it if I do. And so on down the lineyou will become familiar with these voices as you work your way through this book, and before you know it, they will be voices in your head, too.But while they can spin tales that bring out particular features of XAFS collection and analysis, I thought it would be best to start with a story from my own career, something that gives you a taste of what this technique can-and can't-do.I was a postdoc; it was my birthday, and somehow I found myself in the laboratory I had worked in as a graduate student, working feverishly on a problem whose solution had eluded me for years.Years before, when I was a graduate student, one of the faculty members in my department (let's call him S.) had been researching various lithium-ion batteries, including ones based on pyrite (FeS 2 ). While S.was not my advisor, the two had good relations, and our groups often helped each other out on projects. This particular project initially sounded rather manageable. An electrochemist overseas would measure the electrochemical properties of a lithium-pyrite electrochemical cell. We would then measure the iron XAFS at various voltages and after various numbers of cycles, hopefully confirming the speciation in each case.As is often the case in electrochemical work, the list of likely phases for the iron atoms in the cells was fairly small: FeS 2 , FeS, Li 2 FeS 2 , Li x FeS 2 , and metallic iron.At first, things went smoothly. Metallic iron exhibits XAFS spectra that are very different from the oxidized phases. For one thing, the fact that xxiii the iron atoms are reduced in comparison to the other phases results in an earlier onset of the x-ray absorption edge. In addition, scattering of ejected photoelectrons off of neighboring iron atoms produces a characteristic set of wiggles in the x-ray absorption spectra well above the edge (the EXAFS region). I quickly identified metallic iron as present in a few of the most highly cycled samples. In Chapter 6 of this book, we'll learn more about fingerprinting approaches such as this.The other phases, however, were not so easy to untangle. If I had possessed good specimens of each of the phases in their pure form (standards), I could have used linear combination analysis (Chapter 7) to determine how much of each was present. But although I had good standards for FeS 2 , FeS, and metallic iron, those for the lithiated phases proved too hard to come by. My standard for Li 2 FeS 2 , for instance, had not had its composition verified by other techniques-the researcher who provided it was essentially guessing that it might be the desired material. In addition, the literature suggested that the structure of Li 2 FeS 2 found in electrochemical cells was quite different from the bulk crystalline form. In fact, when I compared an extended x-ray absorption fine structure (EXAFS) spectrum calculated using the published structure of bulk Li 2 FeS 2 to the spectrum of my standard, I found them to be considerably different. But without more information, it wasn't clear whether this was because the structure of electrochemically generated Li 2 FeS 2 was that different from the bulk or whether my standard was not Li 2 FeS 2 .I now know that principal component analysis (Chapter 8) would have been a good tool for this problem, but at the time I was unfamiliar with that technique.And so I resorted to modeling (Chapter 9), a powerful but time-consuming technique that was (and still is) my specialty. To model the spectra, I had to make guesses as to what each sample contained. For instance, I might presume a sample contained some metallic iron, some FeS, and some Li 2 FeS 2 . I allowed for modifications to the structure relative to the bulk crystalline forms, such as different coordination numbers and bond lengths. And I then used computer software to simulate the EXAFS spectra for my model and compare it to the experimental data.This approach soon allowed me to rule out FeS in all of the samples-it just would not fit the data, no matter how I modified the structure. This was satisfying, as earlier work in the literature had argued against the presence of FeS as an intermediate. FeS 2 was also absent in all of the measured cells. (This may seem surprising at first, as these were nominally FeS 2 cells! But they had all been cycled at least 40 times, and it was well known that significant irreversible changes occurred during the first few cycles, suggesting that the starting material of FeS 2 was never regenerated.)The approach also revealed that some of the spectra were consistent with either Li 2 FeS 2 (an electrochemical form somewhat different from the xxiv Introduction bulk crystalline structure) or mixtures of metallic iron and Li 2 FeS 2 . But about half of the electrodes showed evidence of an additional phase, both in the charge-discharge curves and in the XAFS. This additional phase was not any of the initial suspects.And thus I was stumped. I would return to the problem again and again, only to be continually defeated. S. took to crying out one word whenever he saw me in the halls: "PYRITES!" This was his subtle way of reminding me I should continue to work on the problem.Eventually, I wrote my dissertation (which thankfully had nothing to do with pyrites), graduated, and secured a postdoc hundreds of miles away. But still the calls and e-mails came: "PYRITES!" This project was not going to die a quiet death.At last, I happened to be passing through the city where I had done my graduate work at the same time as the electrochemist from overseas. S. seized the opportunity to get us to agree to spend an afternoon sitting in my old lab discussing the problem. (He is a very persuasive fellow!) It just so happened that this one available day was my birthday, and that's how it turned out that I spent my birthday back in my old laboratory, working on a problem that had hounded me for years.I had never met the electrochemist in person before. And we were both tired of this project. I had reached a dead end in my analysis, as had she.We weren't sure what we were supposed to be talking about.So I began to describe my dead end. I explained how the samples with the mystery phase appeared to include fully reduced iron, but that neither conventional iron metal nor any of its allotropes (face-centered cubic iron, for instance) were consistent with the EXAFS. It was as if there were neutral iron atoms without iron neighbors, but of course that didn't make any sense.Except that the electrochemist thought it might make sense. She remembered seeing work in the electrochemical literature suggesting that isolated neutral atoms of metal could form and coordinate to materials like oxides or sulfides.So I tried modeling the unknown phase as neutral iron atoms coordinated to the oxygen of the polyethylene oxide (PEO) we used as a matrix in our electrodes. And it worked! The model could reproduce the features of the measured spectra quite well.I also tried a model where the neutral iron atoms coordinated to the sulfur in the lithium sulfide formed by the cell. This model also workedwith all the freedom that I was allowing my models (bond lengths, phase fractions, coordination numbers, etc.), the two models could not be distinguished solely based on the ability to reproduce the measured spectra.

Introduction xxv:
But my colleague and I had additional knowledge about the system. When I used the model with iron bonded to oxygen, a good fit required a coordination number of 8 ± 1 oxygen atoms. When I used sulfur instead, the fit required 7 ± 1 sulfur atoms coordinated to the iron, a similar result. The PEO, however, was a preexisting matrix in our electrode, and the amount of iron was significant. Requiring each iron to have even six nearest-neighbor oxygen atoms would completely disrupt the PEO matrix, a result that seemed to us to be implausible. The Li 2 S, on the other hand, was forming and being consumed every time the cell was cycled anyway.It was not unreasonable that it could end up in an atomic scale mixture with neutral iron atoms, so that an iron-sulfur-lithium nanocomposite was formed.The result made physical and chemical sense, was in accord with the electrochemical measurements, and was consistent with the XAFS. We soon published the resultsMost XAFS analyses don't stretch on for years, but my experience with the pyrites project includes many aspects that are endemic to the technique. In XAFS for Everyone, we will see that the technique is not a "black box," providing definite answers to narrow questions. Rather it is a powerful scientific tool, requiring judgment and knowledge to wield effectively. Although nothing substitutes for the experience of applying XAFS to your own projects, this book is designed to shorten the learning curve, incorporating real-world examples, helpful tips, and general principles into a practical introduction to the technique. xxvi Introduction

Something for Simplicio:
That was a nice story from our author. I appreciate how the electrochemistry and XAFS worked together to answer a real-world question.Note how it was necessary to think about what was actually happening within the system physically to distinguish between the two models.I just like that it worked! Pardon me, but I'm new to all of this. I know some chemistry and a little physics, but I don't know anything about XAFS. I got the gist of the story, but only in a fuzzy way. Could you help me learn more?Friends! We must not leave Simplicio behind. Let us help him out by providing a simple overview of XAFS now, leaving the more complicated details for later.Thanks, Mandelbrant! 11.1 X-RAY ABSORPTION SPECTRA

X-Ray Absorption Spectroscopy:
In an x-ray absorption experiment, a sample of interest is bombarded with x-rays of definite energy. Some of these x-rays are absorbed by the atoms in the sample, causing the excitation or ejection of a core electron. This absorption can be quantified by comparing the intensity of the incident beam to that of the transmitted beam, by measuring the fluorescence given off by the excited atoms as an electron fills the now-vacant core orbital (the core hole), or by measuring the ejected electrons as the core hole is filled (Auger electrons). Once the absorption is determined for one energy of incident x-rays, the energy is changed slightly and the process repeated. By stepping through a range of energies in this way, a spectrum is created. An example of an x-ray absorption spectrum is shown in FigureSeveral types of features can be seen in this spectrum:• A gradual, nearly linear trend toward lower absorption with energy.• A sharp rise in absorption above incident energies of about 7100 eV.• Several small peaks and shoulders near or on that sharp rise.• A gradual oscillation up and down relative to the general trend downward, seen from about 7150 eV to at least 7400 eV, and perhaps beyond. The amplitude of this effect decreases with increasing energy, and the oscillations also increase in breadth.These features, or something like them, are common to most x-ray absorption spectra.4 Chapter 1 -XAFS in a Nutshell

Background:
The gradual trend toward lower absorption seen in FigureOf course, there are many exceptions to this basic rule. And those exceptions tell us something about the material being examined.Since the exceptions are what's interesting, the slow trends seen in x-ray absorption spectra are usually referred to as the background.

X-Ray Absorption Near-Edge Structure:
The peaks, shoulders, and other features near or on the edge are known as x-ray absorption near-edge structure (XANES). The interpretation of XANES is addressed in Chapter 6.

Extended X-Ray Absorption Fine Structure:
The gradual oscillations above the edge are known as extended x-ray absorption fine structure (EXAFS). This phenomenon is discussed in more detail in Section 1.2.

Fermi's Golden Rule:
According to quantum mechanics, systems, when measured, exist in definite states. An atom may be in its ground state, for instance, or one of its electrons may be excited to a higher orbital, or an electron may be missing from the bound states of the atoms altogether, resulting in an ion.A measurement designed to find the energy states of electrons in an atom will always yield definite answers: the atom either is in its ground state or is not; it has either 0, 1, or 2 electrons occupying its 1s orbital; and so on.1.2 Basics of EXAFS Theory 5Welcome, gentle reader! Allow me to introduce myself. My name is Robert, and I shall often comment in order to provide additional details that might detract from the flow of the main text. Allow me to note, for example, that XAFS spectra measured by fluorescence often do not show the decrease with energy discussed here, as we shall see in Chapters 3 through 5. You look unhappy, Carvaka. Is there something you would like to add?I would like to make it clear that when you say fluorescence, spectra often don't "show" a decrease with energy; it's an artifact of the way the measurement is performed, and not a sign of the fundamental physics of absorption. An atom doesn't "know" whether you're measuring in fluorescence or transmission mode, and in fact you may be measuring in both.But when a system is disturbed, it can exist for a time as a superposition of these possibilities. Only when it is measured in some way is one of the possibilities selected.For instance, when an x-ray enters the region occupied by an atom, several things could happen: the x-ray could pass through unchanged, it could scatter off of the atom, it could be absorbed by an electron in the 1s orbital of the atom, and so on. Until a measurement is made, the system exists as a superposition of these possibilities, each with its own probability. According to Fermi's Golden Rule, the probability of each possibility coming to pass depends on the similarity of the proposed final state to the indeterminate state prior to measurement-the more similar, the more likely

EXAFS Is Due to Interference of an Electron with Itself:
If an x-ray ejects an electron from an atom, you can think of that electron (called a photoelectron) as a wave radiating out in all directions, with a wavelength λ given bywhere h is Planck's constant and p is the momentum of the electron.The electron wave can scatter off of nearby atoms, returning to the original, absorbing atom. Since it's a wave, it could interfere constructively or destructively at that point. If it interferes constructively, then there is more electron density at the absorbing atom, and the system looks more like the (neutral atom + photon) state that occurs before measurement, than if it were to interfere destructively. That means that constructive interference increases the likelihood of finding this state after measurement rather than, say, the state where the photon continues on without being absorbed.If we treat the scattering of the ejected photoelectron off of a nearby atom as if it were a plane wave bouncing off of a soft boundary, we find that constructive interference at the location of the absorbing atom is achieved if the round-trip distance is a whole number of wavelengths, that is,where D is the distance from the absorbing atom to the scattering atom and n is an integer. (The number 2 is there because the electron needs to travel from the absorbing atom to the scattering atom and back.)6 Chapter 1 -XAFS in a NutshellIn this context, "measurement" doesn't necessarily imply human intervention.If, for instance, the x-ray goes on to interact with another atom farther down the beam path that provides indirect evidence that it was not absorbed by the first atom; a "measurement" has occurred.Welcome, all! My name is Kitsune. We're not going to go into the mysteries of quantum mechanics deeply in this book. If you'd like to know more, you might want to consult an introductory text on the subject.Since we are modeling the photoelectron as a plane wave (Box 1.1), and a plane wave is sinusoidal, the interference pattern should be as well. This leads to the probability of absorption being modulated by a factor χ:Note that this function gives a maximum when Equation 1.2 is satisfied, as we expect.Wavenumber k is a parameter defined by

MANDELBRANT'S PLAN:
So the bottom line is that the probability of the absorption of an x-ray is enhanced if it leads to constructive interference of the ejected electron at the location of the absorbing atom, and the probability is reduced if it would lead to destructive interference there.Oh, sorry, I don't think I introduced myself. My name's Dysnomia, and I like knowing the bottom line. I never liked derivations much; I leave that stuff to folks like Mandelbrant.But friend Dysnomia, we can learn so much from thinking things through! Derivations are nothing more than ways of starting from simple assumptions to learn more complicated things. We can begin to learn about EXAFS, for instance, by thinking of the electron scattering off of a nearby atom as if it were a plane wave bouncing off of a soft boundary.Mandelbrant, the photoelectron produced in EXAFS is not a plane wave, it's spherical! And "soft boundary" is a little vague.True, friend Carvaka. But even though air resistance prevents a feather from falling as fast as a cannon ball, we are first taught in introductory physics that they fall at the same rate. Only later do we add in adjustments for phenomena such as air resistance, important though they may be. In just such a way, let us begin by treating the EXAFS photoelectron as a plane wave bouncing off of a soft boundary that does not change the phase of the wave, and then add back in complications one by one.

Relationship of k to Photon Energy:
Equations 1.1 and 1.4, along with the definition  ≡ h 2π , can be combined to yield the useful relationshipUsing basic physics, the momentum p of the photoelectron can, in turn, be related to its kinetic energy T:(1.7)where m e is the mass of an electron.Finally, the kinetic energy of the photoelectron is equal (in our simple model) to the energy E of the incident photon, less whatever energy was necessary to remove the photoelectron from the absorbing atom, which we'll call E o :Combining Equations 1.6 through 1.8 yields the result

EXAFS and Structure:
We can already see why EXAFS can give us information about the structure of the material. As we scan through photon energy in the EXAFS region of an x-ray absorption spectrum, we are also scanning through photoelectron wavenumber. According to the proportionality in EquationIn our simple model, the oscillations will be spaced regularly in k, which means, according to Equation 1.9, that they'll be more and more spread out as we go to higher energies. An examination of FigureBut what makes these oscillations useful for characterization is that the spacing of the oscillations also depends on D, the distance between the absorbing and scattering atoms. The smaller the value of D is, the slower the oscillations occur as a function of k (and E).

Scattering Probability:
The heuristic derivation in Section 1.2.2 is based on the model that the photoelectron scatters elastically off of a nearby atom at a distance D from the absorber. Of course, many other things could happen to the photoelectron. It could, for instance, scatter inelastically off of a nearby atom. If that happens, some of the energy of the photoelectron is lost, its wavelength changes, and its interference pattern is different. Or it could fail to scatter off of the atom at all. In the usual manner of quantum mechanics, each of these possibilities has a probability. We can, therefore, write the proportionality in Equation

(1.10)Here, f(k) is a proportionality constant that is itself proportional to the possibility of scattering elastically off of the atom, but may also include other factors, such as geometrical ones. We've noted explicitly that the probability of scattering elastically depends on the wavenumber k (and thus the momentum) of the photoelectron.A big atom with lots of electrons, such as lead, will usually scatter with much higher probability than a small atom with few electrons, such as oxygen. In addition, the dependence of f(k) on k is different for different elements, as is explained in Chapter 10. Thus, EXAFS can yield information about the type of atoms nearby the absorbing atom, as well as the distances to them.Equation 1.10 is our first stab at "the EXAFS equation," but it is a very, very preliminary one. We'll spend the following several sections refining it.

Multiple Neighbors:
One correction to EquationEach of these scattering events contributes separately to modulating the absorption probability. In other words, we can describe the modulation as a sum of the modulations from scattering off of each neighboring atom (we'll label the different scattering possibilities by the subscript i):1.2 Basics of EXAFS Theory 9Before we go on, think of as many ways as you can in which EquationEven as we continue to fill in details, see if you can think of more. By doing that, you will come to better understand the EXAFS equation and its limitations.Of course, sometimes several of those atoms will be of the same species at the same average distance. For example, consider a block of copper metal. Almost every copper atom is surrounded by 12 identical copper atoms, each the identical distance away (on average, anyway). We can group those 12 atoms together, by noting that they have 12 times the effect a single atom would have. This degeneracy is noted by the symbol N (see Chapter 10 for a more thorough discussion of degeneracy). Our equation now readsThus, EXAFS also provides information on the number of scattering atoms, as well as their identities and distances.

Multiple Scattering:
So far we've discussed events where the photoelectron scatters elastically off of a nearby atom and then returns to the absorbing atom. This is known as single scattering or direct scattering. We could also, however, imagine the photoelectron scattering elastically off of one nearby atom, then off of another, and only then returning to the absorbing atom. This is known as multiple scattering. Multiple scattering does not require us to rewrite Equation 1.12, as long as we generalize the meaning of the factors. For a single scattering path, D i was the distance from the absorbing atom to the scattering atom. Since the photoelectron has to get from the absorber to the scatterer and back again, it is half of the total distance traveled by the photoelectron; that is, it is half of the path length.For multiple scattering, we'll simply define D i in the same way, as half the total distance traveled by the photoelectron when following that path. For instance, suppose the photoelectron begins at absorber A and then scatters off of atoms X and Y before returning to the absorber A. To get the value of D i for that path, we would add up the distance from A to X, from X to Y, and from Y back to A, and then divide the total by 2.For a discussion of how the degeneracy N i is defined for multiple scattering paths, see Chapter 10.

Phase Shifts:
Equation 1.12 treats scattering atoms as if they're some sort of ideal "soft boundary," which do not change the phase of the photoelectron wave and yet reverse its direction instantaneously. The reality is, of course, more complicated; we'll examine it in detail in Chapter 10. For now, we'll simply observe that there should be a phase shift introduced into our equation. As long as we're putting in a phase shift anyway, we'll also switch from cosine to sine. (A sine function is, after all, just a phase-shifted 10 Chapter 1 -XAFS in a NutshellMultiple scattering can include more complicated sequences. Even diatomic gases exhibit some multiple scattering, such as the path that travels from the absorber to the scatterer, then back to the absorber, back to the scatterer again, and finally once more to the absorber.Yeah, but scattering back and forth and back and forth like that doesn't usually contribute much to your spectrum. We'll talk more about this in Sections 9.3.1 of Chapter 9 and 14.5, and of Chapter 14. cosine function.) While we could equally well continue to use cosine, for historical reasons it is traditional to use sine in the EXAFS equation.

Spherical Waves:
The photoelectron wave is not a plane wave, of course. It spreads out isotropically; that is, it is a spherical wave. One important ramification of this is that the wave spreads out, and thus the scattering probability drops as the square of the distance. We acknowledge that explicitly by writingThis means f i (k) is defined differently than it was before (also see Box 1.2).In this form, and properly accounting for spherical wave effects on scattering, it is sometimes given the additional subscript "eff," short for "effective." Rather than clutter up the equation more than necessary, we'll follow common practice and leave it as f i (k), with context making it clear that it is the form appropriate for spherical waves.

Incomplete Overlap:
Regardless of what the photoelectron does, the final state of the absorbing atom is not the same as the initial state. The photoelectron has been ejected from the atom, leaving behind a core hole. All of the other electrons thus feel more positive charge from the nucleus (i.e., it is not as well shielded), and the orbitals adjust to this change. The incomplete overlap can be phenomenologically modeled by an element-dependent constant, the amplitude reduction factor S o 2 :This effect is modest but not negligible; S o

Mean Free Path:
Aside from scattering elastically off of nearby atoms, other fates can befall the photoelectron. It could scatter inelastically, perhaps exciting a valence electron from one of the scattering atoms or a phonon (vibration) in the crystal. Any event of this sort will remove energy from the photoelectron, thus changing its wavelength and the resulting interference condition. Because the number of different specific outcomes for this kind of event is very great, and because some will result in constructive 1.2 Basics of EXAFS Theory 11Like f(k), the phase shift δ(k) depends on the species of the scattering atom.See Chapter 10 for a discussion of the limitations of the S o interference at the absorbing atom and some destructive interference, the net result is to suppress part of the main channel of the EXAFS signal; that is, suppress χ(k) in EquationIn addition, the core hole isn't going to wait around forever. Sooner or later, an electron in a higher orbital will fall into the core hole, likely resulting either in fluorescence or in the ejection of another electron 12 Chapter 1 -XAFS in a Nutshell

BOX 1.2 ThERE IS NO SUCh ThING AS "ThE EXAFS EQUATION":
I get the D i 2 part of Equation 1.14, but where did that k in the denominator come from?If you don't like it, it doesn't have to be there. I mean, f i (k) is already a function of k, so we could get rid of it by just redefining f i (k). Or we could redefine f i (k) so that there is a k 2 in the denominator. Or a k in the numerator. All those things have been done, and they've all appeared in the literature at one time or another.What Dysnomia is trying to say is that, unlike the other factors we've discussed, k in the denominator doesn't correspond to a physical effect. It was chosen to make the definition and units of f i (k) consistent with those used in some other contexts. But depending on what you try to make the definition of f i (k) consistent with, you'll end up with different powers of k out front.Perhaps, Carvaka. But it is good to have symbols mean the same thing to everyone. Thus, the International Union of Crystallography has stepped in (IUCr 2011) and made the version with k in the denominator the standard one, meaning that f i (k) has units of length.But then the IUCr follows that up by talking about "other definitions" that have a k 2 in the denominator instead.As we'll see, friends, the idea that there is such a thing as "the EXAFS equation" is a bit of a myth. There are many versions of the equation, written using somewhat different conventions, and emphasizing different aspects of EXAFS phenomena. While we will continue to talk about "the EXAFS equation," we should keep in mind that different versions are in use.(i.e., an Auger electron). Either way, the absorbing atom will be in a different state, and thus the overlap between initial and final states used for Fermi's Golden Rule will be different. Once again, signal will be removed from the main channel.Like the suppression due to inelastic scattering of the photoelectron, suppression due to decay of the core hole will also depend on the length of the photoelectron's path: the farther it has to travel, the longer it takes, and the longer it takes, the more likely the core hole won't be around anymore when it gets back. So both of the effects can be lumped together into a single factor:λ(k) is called the mean free path of the photoelectron. It is primarily because of this term that EXAFS is a local phenomenon; it ensures that scattering off of atoms more distant than around 10 Å is usually negligible.

EXAFS Is an Average:
So far, we've been discussing an EXAFS experiment as if a single photon is absorbed by a single atom, resulting in a single photoelectron. But in an actual experiment, there are millions of x-rays being absorbed by millions of atoms, and we're collecting the average. (In fact, that's how we measure a probability!)If every absorbing atom were in an identical environment, then Equation 1.16 would be fine. But in a real material, the environments may differ. We'll divide those differences into four categories:1. The absorbing element may be in more than one crystallographic environment. This may be because more than one phase is present (e.g., zinc metal and zinc oxide) or because a single phase has different kinds of sites (e.g., spinel crystal structures have both tetrahedrally and octahedrally coordinated cation sites). 2. There may be local differences in the environment of absorbing atoms. For example, a crystal might have defects that are scattered essentially at random throughout the material. Amorphous materials and liquids, where we can no longer discern a long-range crystal at all, are extreme cases of this. Local differences of these kinds are called static disorder. 3. There may be gradients within a material; for example, the composition of an amorphous film might change gradually with depth. These may be modeled as different crystallographic environments (e.g., for a nanoparticle, surface atoms might be distinguished from core atoms), or as if it were static disorder, or in some other way.

Basics of EXAFS Theory 13:
Section 1.2.11 provides a reasonable mental model of why we use a mean free path. But it's taking some liberties with the actual quantum mechanics. The photoelectron and the core hole are entangled; that is why Fermi's Golden Rule applies to this process in the first place. Some processes, such as corehole decay, can destroy that entanglement.Why do we call it "mean free path" if it also depends on the core-hole lifetime?Think of it as short for "mean free path of the photoelectron before something happens that messes up the main EXAFS channel."4. Even the environment of a single atom is not constant over time.At room temperature, most chemical bonds have vibration frequencies on the order of 10 13 Hz. Typical core-hole lifetimes for x-ray spectroscopy are about 10 -15 seconds, or about 1% of the time it takes a chemical bond to vibrate. By sampling millions of atoms, we will thus be sampling the variations in absorberscatterer distance caused by those thermal vibrations as well. This effect is called thermal disorder.When the absorbing atom is present in different crystallographic environments (case 1 from the list), each environment can be modeled as including its own set of scattering paths, and all sets of paths are then included in the sum in EquationIf thermal (case 4) and static (case 2) disorders are small, that is, not nearly large enough to change the interference from completely constructive to completely destructive (or vice versa), then they can be modeled by something called the cumulant expansion (see Chapter 10). To a first approximation, that leads to the EXAFS equation to be modified with an additional factor dependent on the mean square radial displacement, symbolized by σ 2 :(1.17)We will discuss this factor, and other aspects of the cumulant expansion, in more detail in Chapter 10 (also see Box 1.3).

BOX 1.3 A LITTLE MORE ABOUT σ:
σ is a pretty complicated looking factor, and Chapter 10 is a long way off. Can't we talk a little more about it now? Sure thing, Simplicio. σ i 2 is the variance in D i due to disorder. That means you could think of there being a parameter called σ i that would be the standard deviation of that distance, although almost nobody ever actually does that in the literature. Since k is proportional to the number of photoelectron wavelengths per angstrom, kσ i is proportional to the spread of the distribution of D i measured in wavelengths. For the approximation to work at all, kσ i needs to be much less than one; in other words, the disorder should shift the path lengths by much less than one wavelength of the photoelectron. From there, it's not surprising (to a mathematical physicist, anyway) that a first-order approximation involves a suppression with a Gaussian form.14 Chapter 1 -XAFS in a Nutshell "Crystallographic environment," in this context, doesn't necessarily mean the material has to be crystalline. An amorphous spinel-like structure, for instance, still has tetrahedral and octahedral sites. And while the protein you're working with might never have been successfully crystallized, if it has two zinc sites, it is clear that if it ever were crystallized those sites would be crystallographically distinct.If either thermal or static disorder is not small, then we can use a strategy somewhat like that used for different crystallographic environments, summing over paths that represent scatterers at various differences from the absorber.Finally, the case of a gradient (case 3) can be handled in either manner, or by a combination of the two. If the effect of the gradient is modest, then treating it as if it were static disorder is reasonable. If it is more significant, then modeling it as a sum over different paths (e.g., "core" paths and "surface" paths) sometimes works well.

Enough for Now:
Except for a few differences in notation, EquationThat does not mean, however, that there are not further refinements that are sometimes made. For instance, the cumulant expansion can be taken further, allowing for somewhat more disorder to be accounted for. Another common modification is to include the effects of the polarization of the x-ray beam. We will discuss these considerations in future chapters.

SOME TERMINOLOGY:
Certain aspects of XAFS terminology are not well standardized, and can be confusing. While most terms will be introduced as we need them, there are a few worth clarifying up front.

Edge:
The IUCr Dictionary defines an absorption edge as "the energy at which there is a sharp rise (discontinuity) in the (linear) absorption coefficient of X-rays by an element" (IUCr 2011). This is, intentionally, not a very precise definition, as the sharp rise occurs over a range of energy-perhaps 10 or 20 eV."Edge" is often used to describe the sharp rise itself, rather than just the energy at which it occurs. For example, it could be said that the edge in Figure"Edge" can also be used to identify the core hole corresponding to a given spectrum. Thus, FigureFinally, "edge" can be used as a synonym for E o (see Section 1.3.7).

XANES:
XANES stands for x-ray absorption near-edge structure. In Section 1.1.4, we mentioned that features "near or on the edge" are known as XANES. But what exactly qualifies as "near or on the edge"?In actuality, there is not a sharp boundary between XANES and EXAFS, so choosing an energy of demarcation is somewhat arbitrary. The IUCr Dictionary suggests around 50 eV above E o (IUCr 2011), but 30 eV above E o is also commonly used.

NEXAFS:
NEXAFS stands for near-edge x-ray absorption fine structure. Technically, NEXAFS is a synonym for XANES. In practice, the term NEXAFS is generally only used for low-energy edges, typically those below 1000 eV.

EXAFS:
EXAFS stands for extended x-ray absorption fine structure; that is, the oscillations present at energies above the XANES region. Notice that only the oscillations themselves are properly referred to as EXAFS, and not the gradual background. If you want to talk about the entire behavior of the spectrum at energies higher than those for XANES, including the background, glitches in the data, and so on, refer to "the EXAFS region" rather than just "the EXAFS."

White Line:
Often, a spectrum will exhibit a sharp feature at the top of the edge, showing absorption much higher than what is seen in the EXAFS region (see Figure16 Chapter 1 -XAFS in a Nutshell So you can either call the near-edge region XANES for both lowand high-energy edges, or you can call it NEXAFS for low-energy edges and XANES for high-energy edges.The term "white line" comes from the days when x-ray spectra were collected on film. Strong absorption at a given energy left the photographic negative unexposed, creating a literal white line.

Pre-Edge:
Pre-edge is used in at least two distinct ways: 1. To identify the fairly featureless part of the spectrum before the sharp rise associated with the edge. 2. To identify small features below the midpoint of the rising portion of the spectrum, such as the small sharp peak near 7115 eV shown in FigureIt is perhaps lamentable that the multiple ways in which the terms "edge" and "pre-edge" are used can lead to a "pre-edge feature" occurring at a higher energy than a point described as the "edge." In practice, however, this strange terminology rarely causes confusion.

E o:
We introduced the parameter E o in Section 1.2.3, calling it the energy "necessary to remove the photoelectron from the atom." This definition is neither precise (remove to where?) nor easy to directly measure. In fact, E o is used to symbolize a variety of related quantities:• It may be defined operationally, that is, in terms of the shape of the spectrum. For example, the first inflection point in the XANES region might be chosen, or the steepest inflection point, or a point half way up the edge (see Section 4.3.3 of Chapter 4).• It may be taken from a table (E o for the iron K-edge is 7112 eV).• It may be chosen so as to make the EXAFS equation for a model structure match the data as well as possible (see Section 10.1.5 of Chapter 10).These ways of defining E o typically result in values that differ from each other by several electron volts. Failure to pay attention to which definition of E o is being used can cause considerable confusion. The same spectrum as in Figure

DATA REDUCTION:
Section 1.2 outlined the theory of EXAFS: given a material with a known arrangement of atoms, how would we predict what the spectrum looks like? But of course, we're usually more interested in the inverse problem: given a spectrum, how can we extract information about the atomic arrangement? For the rest of this chapter, we'll provide an outline of how that is done, once again leaving the details to chapters that follow.The initial steps toward extracting information from XAFS spectra are to manipulate the data so as to present them in a standardized formhopefully, one which emphasizes the features we are most interested in analyzing.

From Raw Data to χ(k):
When presented with a spectrum such as that shown in FigureFor XANES, data reduction is now done. But for EXAFS, additional steps are generally necessary.Since EXAFS refers only to the oscillatory part of the spectrum, and not the gradual trends, a smooth background function is drawn through the EXAFS portion of the data, as has been done in FigureNext, the background is subtracted, yielding just EXAFS, symbolized by χ(E). This is shown in FigureSince the e k i -2 2 2 σ factor in the EXAFS equation causes χ(k) to drop off in amplitude with k, χ(k) is often multiplied by k, k 2 , or k 3 . This is called k-weighting and results in a plot with more uniform amplitude, as can be seen in FigureIt is also worth noting that the k-weighted data goes to zero at k = 0, which means that the XANES region is de-emphasized. Since consistent background subtraction within the XANES region is difficult to achieve, this is desirable.For some purposes, the data are now ready for analysis. But in many cases, an additional transformation is employed. A more recent colloquial term is "k-space graph," and that's the informal term we'll use in this book.

Fourier Transform:
An inspection of FigureIron metal, unlike the highly disordered substance which yielded the data in FigureFortunately, the problem of decomposing a function into constituent sine waves is an old one, with solutions going back to Joseph Fourier's work in the early nineteenth centuryThe differences we saw in the k-space graphs are reflected in the magnitudes of the Fourier transforms. The spectrum that is approximately sinusoidal in FigureWith the discussion of our panel (Box 1.4) in mind, we can now return to Figure

BOX 1.4 FOURIER TRANSFORMS ARE NOT RADIAL DISTRIBUTION FUNCTIONS:
The x-axis in FigureI'm here, Simplicio! And you are about to make a really big, and common, mistake! The magnitude of the Fourier transform is not a radial distribution function! The peaks aren't at the right distances, they're not the right sizes, and they're not the right shape.Right. The magnitude of the Fourier transform is not a radial distribution function. But it's related to one. For example, the first big peak in the magnitude of the Fourier transform of the iron foil is at higher R than the first big peak in the sample with the iron-sulfur composite, because the iron-iron nearest neighbor distance is larger than the iron-sulfur distance.For the magnitude of the Fourier transform to be a radial distribution function, the EXAFS equation would have to be simply χ( ) k N kD∑ sin 2 , with no multiple-scattering paths, no k-weighting, no disorder, no dependence of scattering amplitude on species or k, an infinite mean free path, and an infinite amount of data with which to work. None of those things are true. If one compares the equation I just gave with the EXAFS equation developed in Section 1.2, one can see why there is some similarity between the magnitude of the Fourier transform and a radial distribution function. But the magnitude of the Fourier transform is not a radial distribution function.OK. But I have another question. We wrote the EXAFS equation using the symbol D for the distance between an absorbing atom and a scattering atom, but I notice the Fourier transform figures use the symbol R instead. Why?We have done that, friend Simplicio, to remind us that the magnitude of the Fourier transform is not a radial distribution function. Our notation is not a standard one in the literature, but we find it useful for avoiding confusion.

XAFS IS NOT A BLACk BOX:
Section 1.2 outlines the theory of EXAFS spectra. The state-of-the art is a bit more advanced than what we have shown-see, for instance,The theory of XANES is less well developed, but strides are being made.We can now predict the general characteristics of XANES resulting from most structures.The problem is that, except as a matter of scientific curiosity, we are not particularly interested in predicting the spectra for a given atomic arrangement. Instead, we are interested in the inverse problem: given a spectrum which we have measured, what is the structure of the material? It turns out that this question is difficult.An examination of the EXAFS equation reveals why. For a completely unknown structure, there would be many unknown parameters. Each term in the EXAFS equation contains a D i , σ 2 i , and N i . What's more, there could be many important terms, corresponding to many important scattering paths. If the scattering elements are unknown, then f i (k) and δ i (k) are also unknown, and those are functions, not just values! Finally, S o2 and E o represent two more parameters that are unknown for a completely unknown structure.That's a lot of undetermined parameters, but is it too many? Look at the EXAFS equation again. If S o 2 is unknown and all the N i 's are unknown, the task is hopeless: different combinations of S o 2 and the N i 's could result in exactly the same spectrum, and thus those parameters cannot be individually determined.In Section 11.1.1 of Chapter 11, we can see that only limited information can be extracted from an EXAFS spectrum-perhaps 10 or 20 parameters. That's not enough to characterize a completely unknown structure. And there's no way around that. If you thought that you could put XAFS data through some procedure-a "black box"-which would yield the structure of your material, you were mistaken.So, if XAFS can't give you the structure of a completely unknown material, what good is it? How are people who use the technique getting anything out of it at all?The answer is that they take advantage of our ability to calculate fairly well the spectra corresponding to given structures. That means that XAFS is very good at ruling out structures. Suppose we think a material is probably either pure goethite or pure lepidocrocite (two allotropes of 1.5 XAFS Is Not a Black Box 23The simple model that we have developed in this chapter suggests that S o 2 and E o are the same for every path. This is a reasonable assumption, but we will examine its limitations in Chapter 10.FeOOH), but we don't know which. We can calculate what the spectrum of each of the proposed structures looks like, and see if either looks like our data. If only one does, that's a pretty good clue that it's the right structure, since we already had reasons (whether experimental or theoretical) for thinking our material was one of the two. If, on the other hand, neither calculated structure looks like the data, then our initial assumption was probably wrong, and we should think about other possibilities.The EXAFS equation allows us to go a little beyond just choosing between possible structures. If, for instance, we have a biological compound the structure of which is generally known, but where a key question is the length of a copper-sulfur bond at an active site, it might be possible to use the known information about the material to reduce the number of unknown parameters in the EXAFS equation to a handful, including the desired bond distance. Those parameters could then be varied in a systematic way and the resulting spectra examined for agreement with the data. If only certain copper-sulfur bond lengths produce spectra similar to the data, then we have our answer. Thus, while XAFS is not a black box, it can be a very useful tool for materials characterization. Section 1.6 provides an overview of the most common XAFS analysis techniques, each of which is then detailed in later chapters.

OvERvIEW OF APPROAChES:
TO XAFS ANALYSIS

Fingerprinting:
The most straightforward approach to analysis is fingerprinting; that is, directly comparing measured data to a calculation or to other known data.In either case, the spectrum to which the data is compared is called a standard.The example of distinguishing between goethite and lepidocrocite given in Section 1.5 involves comparing the data to two different calculations; that's fingerprinting using theoretical standards. Instead of comparing to calculations, the data could have been compared to the spectra of known samples of goethite and lepidocrocite: fingerprinting using empirical standards.This concept can be extended to circumstances where no single standard matches the data by looking at a series of standards and seeing where the data seem to fall in the series. For example, XANES spectra of sulfur in many different local environments and with various valences might be collected, creating a library of standards. It's often the case that examination of the spectra within the library, perhaps informed by theoretical considerations, can yield patterns that can be used to identify valence, symmetry, or other characteristics for an unknown spectrum, even when the unknown spectrum doesn't match any individual standard from the library.Chapter 6 provides more information about this technique.24 Chapter 1 -XAFS in a Nutshell

Linear Combination Analysis:
If 20% of the chlorine atoms in a sample are in one local environment, and 80% are in a second local environment, then the resulting normalized XAFS spectrum will be a sum of 20% of the normalized spectrum for a sample where all the chlorine atoms are in the first environment and 80% of the normalized spectrum for a sample where all the chlorine atoms are in the second environment. (This is also true in k-space but is not true for the magnitude of the Fourier transform.)The concept of linear combination analysis is therefore simple: assemble a library of standards, and allow a computer to sum them in various linear combinations, reporting those that match the data well. Linear combination analysis is thus very effective at finding the relative amount of known constituents that are present.For example, imagine a reaction in which the initial material and the end product are known, and it is suspected that there are no intermediates.It might be important to understand the kinetics: how fast does the reaction occur? At what point is it 50% complete? 90% complete? This could be discovered simply by measuring spectra at different periods and using linear combination analysis to determine the fraction of initial material and final product that are present at each time.Or, suppose you are investigating a soil containing iron minerals, but it is unknown which minerals and in what proportions. If you have a library of common iron minerals, you could use linear combination analysis to discover which mixtures are consistent with your data.If a linear combination analysis fails to find a good match to the data, it suggests that there is a constituent in the sample that is not in the library of standards. That can be useful information, but it also reveals the weakness of linear combination analysis: you have to have standards for everything that is in your mixture! Linear combination analysis is covered in more depth in Chapter 7.

Principal Component Analysis:
Since XAFS spectra of mixtures can be expressed as a linear combination of the pure spectra of the constituents, the machinery of linear algebra can be brought into play.Suppose you have collected a series of four soil samples, all from the same site. The first is from a depth of 5 cm, the second from 10 cm, the third from 15 cm, and the last from 20 cm. From each, you measure the XAFS for the iron edge.Those four spectra can be mixed together in a number of ways. For example, you're probably familiar with the idea of a moving average.

Overview of Approaches to XAFS Analysis 25:
Spectra 1 and 2 can be averaged to form spectrum A, spectra 2 and 3 to form spectrum B, and spectra 3 and 4 to form spectrum C. By doing this, signal to noise can be improved, but you pay a price: where you started with four spectra, you now have only three. That means that you've lost information. If you somehow misplaced the original spectra 1 through 4, you couldn't reproduce it from your new spectra A through C.But suppose you had made one more combination-it doesn't matter what, as long as it was independent of the other combinations. For argument's sake, let's say you averaged spectra 1, 2, and 3, and called the result D.You started with four spectra, and you still have four. Does that mean you could recover the original information?Yes. Spectrum 1 can be recovered from (D -B). Spectrum 2 is (A + B -D). Spectrum 3 is (D -A). And spectrum 4 is (C + A -D).A through D can now be thought of as components of the original data set, analogous to the components of a vector. And just as many different coordinate systems could be used to represent the same vectors, many different choices of components could be used to represent the same set of spectra.A particularly useful set of components to choose are what are called principal components. Under this system, the first component is chosen to be the average of all of the spectra, perhaps scaled by some constant.The second component is chosen so as to account for as much of the variability of the individual spectra from that average as possible; that is, linear combinations of the first and second component do as good a job of recovering each of the original spectra as any pair of components possibly could. The third component is chosen so as to account for as much of the remaining variability as possible and so on until there are as many components as there were spectra originally.The advantage of this kind of analysis is best seen when considering a fairly large set of spectra that are related in some way. Instead of just 4 soil samples, consider 20-perhaps they were not only collected at different depths but were also powdered and exposed to air for different periods.It is very likely that the variations between all of the samples can be described by a small number of free parameters. Perhaps some have more of the iron mineral hematite, and some more goethite. Perhaps in some cases the goethite is highly disordered, and in others more crystalline. But since the samples are all related, all 20 can probably be described pretty well in terms of only a few variables-maybe 3, 4, or 5. And because of the way principal components are chosen, it would only take the same number of principal components, plus one for the initial average, to reproduce all of the original spectra quite well. If the samples can be described in terms of three variables, then the most important four components will do the job. If it takes five variables, then six components would be needed. Components beyond that will just account for tiny variations, such as those caused by noise in the measurement. In short, the power of PCA is its ability to work with a system for which much is unknown. For this reason, it is often used as a preliminary technique, and then followed up with one or more of the other three (fingerprinting, linear combination analysis, or curve fitting to a theoretical standard) described in this book.PCA is covered in more depth in Chapter 8.

Curve Fitting to a Theoretical Standard:
The final technique goes by the rather unwieldy name curve fitting to a theoretical standard. Many people just refer to it as modeling or fitting, although technically linear combination analysis is also a kind of fitting process.In this technique, a candidate structure is first chosen by means of an educated guess-that is, the investigator's knowledge about the system identifies it as a likely possibility. (The candidate "structure" may in some cases be a mixture of materials.) The theoretical spectrum is then computed for the candidate structure, creating a theoretical standard.Next, the investigator decides how to describe modifications to the candidate structure in terms of a small number of parameters-perhaps the fraction of different phases that are present, or a thermal expansion, or the extent of some particular kind of disorder. The investigator then translates these parameters into the parameters of the EXAFS equation. For example, an investigator might be trying to find an unknown thermal expansion coefficient, and therefore wants to use it as a free parameter. According to the definition of "thermal expansion coefficient," that parameter multiplied by the temperature difference from the baseline temperature is the percentage change in all of the D i 's. Finally, a computer program is used to vary the free parameters in such a way as to create the best fit between the modified theoretical standard and the data.

Overview of Approaches to XAFS Analysis 27:
If the fit is poor, then the model is likely wrong-that is, either the candidate structure is not correct or the free parameters are not appropriate for the sample. If the fit is good, then that lends credence to the choice of candidate structure and yields best-fit values for the free parameters.Of all the techniques, curve fitting to a theoretical standard is the most complicated to learn. Doing it well takes careful thought, good judgment, experience, and a combination of scientific and common sense. For that reason, not only does Chapter 9 discuss curve fitting to a theoretical standard in more detail, but all of Part III is dedicated to it as well.While it is the most difficult of the techniques to gain proficiency with, it also has the potential to be the most powerful, and is what most scientists mean when they talk about EXAFS analysis. A combination of this book and a willingness to experiment and practice will be enough to get you started.

WhAT I'vE LEARNED IN ChAPTER 1, BY SIMPLICIO:
• EXAFS is due to interference of an electron with itself: the probability of the absorption of an x-ray is enhanced if it leads to constructive interference of the ejected electron at the location of the absorbing atom, and the probability is reduced if it would lead to destructive interference there.• The EXAFS equation attempts to incorporate modifications to the basic picture such as disorder, multiple atoms nearby the absorber, multiple scattering paths, and phenomena that remove signal from the main channel.• Some common terms related to XAFS have more than one definition. Context can usually make it clear which is meant.• For both XANES and EXAFS analysis, spectra must be normalized. For EXAFS, they are usually converted to k-space, and perhaps Fourier transformed as well.• XAFS is not a black box.• Fingerprinting means comparing data to a standard. Standards may be calculated (theoretical) or measured (empirical).• Linear combination analysis is a simple and effective analysis technique when you have standards for all the possible constituents in your sample.• Frankly, I'm not sure I understand PCA yet, but that's OK. It is explained in more detail in Chapter 8.• Curve fitting to a theoretical standard means making an educated guess as to the structure of the sample, choosing a few parameters that are free to vary, and then using a computer to match that model to the measured data as well as possible.

IDENTIFYING YOUR QUESTIONS:
The first step to planning an x-ray absorption fine structure (XAFS) experiment is to identify the scientific questions you want to answer.

SYNchROTRON LIGhT SOURcES:
To measure XAFS, you need x-rays, and you need to be able to scan them through different energies. Currently, the best way to do that is to use a synchrotron light source (often called either a synchrotron or a light source for short). These light sources inject electrons traveling near the speed of light into a storage ring to produce broad-spectrum x-rays (synchrotron radiation). The ring features straight sections alternating with curved 32 Chapter 2 - Planning the Experiment  sections. In the curved sections, magnetic fields created by bending magnets accelerate the electrons in a circular path, creating the desired broadspectrum x-rays. Arrays of magnets may also be arranged in the straight sections to form insertion devices, providing additional sources of x-rays.The x-rays generated by bending magnets or insertion devices can then be channeled down beamlines. The term "beamline" encompasses not only the tube down which the x-rays travel but also all the equipment used for measurement. Even the chairs experimenters sit in are considered part of the beamline! Synchrotron light sources are classed by generation• First-generation light sources feature a storage ring originally intended for another purpose, generally particle physics.• Second-generation light sources were designed and built from the start to be x-ray sources.• Third-generation light sources are designed and built to have insertion devices in the straight sections. In addition, they feature electron beams with significantly better collimation (Altarelli and Salam 2004).We are probably, however, nearing the end of the period of development in which light sources are designated by generation. For one thing, many first-and second-generation sources have been upgraded over the years, often with the addition of insertion devices. The generation of a synchrotron, or the year it was built, is not always a reliable guide to its capabilities.The successors to third-generation sources are available, but they are of very different types. Free electron lasersEnergy recovery linacsBoth free electron lasers and energy recovery linacs are called fourthgeneration sources, but given the dramatic differences between them, the technologies are better referred to explicitly.Currently, there are dozens of light sources scattered across five continents. In Table

Synchrotron Light Sources 33:
It used to be that light sources would have to interrupt the beam a couple of times per day to inject a new set of electrons and many still do.But some now operate in top-up mode, using frequent mini-injections to replace electrons lost to collisions. Top-up mode allows you to get beam on your sample without interruption.Free electron lasers and energy recovery linacs are not mutually exclusive; it is possible to use the linac to power the laser.34 Chapter 2 - Planning the Experiment

BENDING MaGNETS aND INSERTION DEvIcES:
While the capabilities of synchrotrons are important, it is the combination of synchrotron characteristics and the insertion device (or bending magnet) that constrains the capabilities of a particular beamline.

Brilliance:
Optical components can do a lot to modify the x-rays provided by a bending magnet or insertion device. The x-rays can be focused by mirrors, for example. As another example, a monochromator (Section 2.4) can be used to discard photons not in the desired energy range, leaving the beam more nearly monochromatic.But the principle of "conservation of étendue"The brilliance of several sources in the United States is shown, as a function of energy, in Figure

Bending Magnets and Insertion Devices 35:
As can be seen from the figure, bending magnets can achieve brilliance figures of 10 14 -10 15 (usual units) at around 10 keV (a typical energy for XAFS measurements).

Wigglers:
By placing a series of magnets of opposite polarity in a row, electrons can be made to wiggle back and forth, giving off more x-ray radiation than would be the case for the single turn caused by a bending magnet. This array of magnets is called a wiggler. The x-rays from wigglers are used in much the same way as the x-rays from bending magnets, but with a significant (perhaps two orders of magnitude) enhancement of brilliance.36 Chapter 2 - Planning the Experiment Though brilliance is the figure of merit for a source, the quantity that actually matters in most XAFS experiments is intensity in some bandwidth-that's brilliance without the "per square milliradian" part.

Undulators:
Like wigglers, undulators are insertion devices composed of magnets of opposite polarity. But the deflections caused by an undulator are small enough that the x-rays emitted from each turn overlap, and thus interfere. At some energies, this interference is constructive. This causes dramatic increases in the intensity (and thus the brilliance) at the energies for which the undulator is tuned, and almost complete cancellation at other energies.The peak brilliance at the tuned energies can easily exceed that of a bending magnet by four orders of magnitude (but more brilliance is not always better-see Box 2.1).(Continued)2.3 Bending Magnets and Insertion Devices 37

BOx 2.1 IS BRILLIaNT aLWaYS BETTER?:
Oooh! I want to study my system using an undulator!Are you sure about that, Simplicio? Undulators are kind of a pain, especially for extended XAFS (EXAFS). The constructive interference peaks only span about 100 eV in energyIf you're not careful, Simplicio, the x-ray intensity could change by quite a bit during an EXAFS scan from an undulator, and if your system is not entirely linear, that could make for a complicated background to subtract! (See Section 4.4 of Chapter 4.) In addition, undulator radiation is coherent, and thus becomes inhomogeneous spatially as it passes through beamline elements such as windows. This can exacerbate problems with sample inhomogeneity (Chapter 3).That's OK, I don't mind extra work, if it means a hundred times the brilliance of a wiggler! You're assuming brilliance is a good thing, Simplicio. For some samples, with very low concentrations and good resistance to beam damage, it is. It's also good if you're running quick-XAFS (QXAFS) (Section 2.4) with a very short time resolution. But many biological samples, as an example, will rapidly degrade under beams that strong. If you wanted to measure a sample like that, you would probably have to find a way of cutting down the intensity of the beam anyway!If you'd like to read more about insertion devices, the works of

MONOchROMaTORS (aND POLYchROMaTORS):
To collect the XAFS spectra, you need a way to select x-rays of particular energies (actually narrow ranges of energy). The most common way to do this is a monochromator, a device that uses diffraction from crystals (usually a pair) to ensure that only x-rays near the desired energy travel toward the sample. We'll discuss monochromators in more detail in Chapters 3 and 5.The combination of the crystals used for the monochromator, their orientation, and geometrical considerations will limit any beamline to a range of usable energies. This will generally be specified on the Web page dedicated to that beamline; for example, the DIFFABS beamline at Soleil can provide x-ray energies from 3 to 23 keV, while the SAMBA beamline at Soleil can provide x-ray energies from 4 to 40 keV. Thus, the iodine K edge (33 keV) could be measured on SAMBA but not on DIFFABS, although the iodine L 3 edge at 4.6 keV could be measured on either. The energy range that a beamline can provide is one of the most important characteristics when deciding whether to apply for time there!Most XAFS experiments are done by having the monochromator select a particular energy and remain there for a period of time (usually measured in seconds; see Section 5.8 of Chapter 5) before moving on to the next energy. For time-resolved measurements, however, this means that too much time is lost while the monochromator is moving from one energy to another without measurement being done. Thus, a technique was developed, known as QXAFS, in which the monochromator is continually rotated through the energy range of the scan, with data collected on the fly38 Chapter 2 - Planning the Experiment

IS BRILLIaNT aLWaYS BETTER? (Continued ):
Even if your sample can take it, at some point more flux doesn't bring you all that much. Suppose a wiggler gives you enough intensity so that you get decent signal to noise by measuring for 1 second at each energy (we'll talk more about scan times in Section 5.8 of Chapter 5). An undulator might let you get the same signal-to-noise ratio in 0.01 seconds.But if it takes a third of a second to change energy from one point to the next, then you're not really going a hundred times faster-you're only going three times faster. Add in the time to go from the end of one scan to the start of the next, the time for changing samples, and the like, and it's not as much help as it sounds.On the other hand, if you do have a sample that really needs an intense beam, either because the concentration of the absorbing element is very low or because you're going to be slewing the monochromator very quickly (QXAFS), then an undulator might make sense.We don't want to scare you away from undulators-just understand that brighter is not always better! Another occasional variation on the usual XAFS beamline features a polychromator. This device sends different energies of x-rays through the sample at different angles (see Figure

Measurement Modes:
All XAFS experiments require the determination of the fraction of x-ray photons absorbed as a function of energy. There are three common methods by which the amount of absorption is deduced:1. The measured intensity of the x-rays downstream of the sample can be compared to the measured intensity of the x-rays upstream x-ray beam is dispersed by the polychromator so that x-rays of different energies are traveling different paths, all of which converge onto the sample. The beams that emerge are then measured by a position-sensitive detector, generating an entire XAFS spectrum simultaneously.No single acronym for energy-dispersive EXAFS has taken hold. Just in the references cited in this section, it's referred to as ED-XAS, EDE, DEXAFS, and DXAS! Many experiments will use more than one of these modes at once! of the sample; the difference must be due to absorption. This is called the transmission geometry. 2. When an atom absorbs an x-ray, it is left in an excited state, with a vacancy in one of its core orbitals. This orbital can be filled by an electron from a higher level, resulting in the emission of a lower energy x-ray in a random direction (i.e., the probability is isotropic). A detector placed perpendicular to the direction of the beam can measure some of these photons, and the intensity compared to the intensity upstream of the sample. This is called a fluorescence measurement. 3. Instead of emitting a photon, the excited atom can return to the ground state by releasing a high-energy electron (called an Auger electron). The number of electrons emitted can be counted and compared to the intensity of x-rays upstream of the sample. This is called an electron yield measurement.

Ex Situ, In Situ, and Operando:
In an ex situ experiment, the sample is prepared especially for XAFS measurement; for example, it might be made into a powder and spread thinly on tape (Section 3.7.1 of Chapter 3). In an in situ experiment, it is left in its usual environment. A cathode measured while still part of a coin cell is thus in situ. If the sample is performing its usual function, such as a catalyst in its working state or a battery being investigated by cyclic voltammetry, then the experiment is operando.Although these categories can be a useful way of thinking about your experiment, there are no hard-and-fast lines between them, and for some types of experiments the distinction is not particularly relevant. If a cathode is powdered before measurement, it is certainly ex situ. But what if it is extracted from the battery but left intact? Does an engineered thin film meant as a passive protective coating fall neatly into any of the categories?

Ion chambers:
The most common type of detector is the ion chamber. X-rays passing through these gas-filled detectors ionize some of the gas; the resulting cascades of charged particles are collected and amplified to form a measurable current. These detectors, and this process, will be described in detail in Section 5.5 of Chapter 5.Ion chambers are simple, reliable, exhibit good linearity, and can handle high count rates. They are by far the most common detector used for the upstream intensity measurement needed for all measurement modes, and are also very common for the downstream measurement in transmission.A special design of ion chamber, known as a Lytle detector

Energy-Discriminating Fluorescence Detectors:
An ion chamber counts x-rays of every energy. But in the fluorescence geometry, we are only interested in photons of a few specific energies: the ones that correspond to fluorescence from the atom the edge of which we are measuring. Other x-rays, such as those scattered from the sample, constitute undesirable background. Energy-discriminating detectors solve this problem by counting only photons within a chosen energy range (subject to the resolution limits of the detector). The most common form of energydiscriminating detector is a multi-element semiconductor detector, using either germaniumIn these kinds of energy-discriminating detectors, the energy discrimination takes place after detection; that is, x-ray photons of all energies are counted, the energy of each determined, and then those that fall within the desired range reported to a measurement computer. Because these detectors count and analyze each photon separately, they can sometimes fail to count a photon while they are still analyzing the previous one (i.e., there is some dead time after a measurement), or they can count two photons as if they were one of higher energy (pileup). The total count rate (i.e., including background counts) of a single element of a detector of this kind is, therefore, limited to a few hundred thousand photons per second at most

current-Mode Semiconductor Detectors:
There are also semiconductor detectors designed to be used in current mode, where the total number of photons is counted without discriminating in energy. These detectors therefore are employed somewhat like ion chambers. One disadvantage of semiconductor detectors operated in this mode is that they are sensitive to visual photons and x-rays, and thus the sample and detector must be kept in the dark while measurements are being made (Bunker 2010).

Other Detectors:
While the x-ray detectors we've discussed so far are the most common in general use, many other types of detectors have been used, especially for specialized purposes such as spatially resolved detection.

Electron Yield:
Detectors for electron yield experiments must, of course, detect electrons rather than x-rays. As with fluorescence detectors, electron yield can be measured in total yield

Measurement Modes and Detectors 41:
One popular type of multielement semiconductor detector is the silicon drift detector, described in

Beamline Equipment:
Beamlines will always provide you a basic set of detectors such as ion chambers, and may also have available more specialized options. In some cases, the light source may also have a pool of detectors that can be reserved or borrowed for your beamtime.But it is not the case that all beamlines have, for example, energydiscriminating detectors. The detectors available when using a beamline are an important consideration when deciding where to apply for time.

Simultaneous Probes:
It is generally possible to make other measurements simultaneous with XAFS. Complementary probes could include almost any technique that you could use in your home laboratory: cyclic voltammetry, differential scanning calorimetry, infrared spectroscopy, x-ray diffraction-the list of possibilities is very long!In some cases, beamlines are set up to facilitate specific complementary techniques. In others, you may have to bring your own measurement equipment with you, adapting it to make sure it doesn't interfere excessively with the path of x-rays to the sample and from there to the x-ray (or electron yield) detector.

Microprobe:
On microprobe beamlines, the incident x-rays can be focused down to a spot on the scale of microns for hard x-rays (even smaller for soft x-rays). Spot size is therefore an important figure of merit for these lines, although of course not the only one. These beamlines include a motorized sample stage capable of positioning the sample precisely relative to the beam and frequently provide visual or electron microscopy to image a sample

ExPERIMENTaL DESIGN:
XAFS is best at comparing things: different model structures for a compound, different samples to each other, and different conditions for a single sample. Think about your experiment accordingly-don't just plan to measure your most important sample. Measure that important sample and other substances that it can be compared to.For many studies, you will spend more than half your time measuring standards; this is particularly true if you're planning to do linear combination analysis.For other studies, you want a series of samples that are related in some way: soil samples as a function of depth, thin films as a function of composition, and so on.Sometimes you will only measure one or two samples, but under a number of different conditions or as a function of time. Cyclic voltammetry, operando catalysis, and time-resolved chemical reactions are all examples where the number of distinct samples may be small, but the number of spectra is not.In addition to samples, you should also think about people. Who will go to the light source? Ideally, you'd like a team of two to four. For one thing, this will allow you to make the most of your beamtime, measuring both night and day. But in addition, the group should include people with experience and those just learning the ropes. Synchrotron experience and familiarity with the system are both important; often, the expert in synchrotron measurements is a novice to the system being studied and vice versa. Beamtime is a great time to learn from each other! You should also consider the question of who will analyze the data. It is important that there are open lines of communications between the people most familiar with the system, the person analyzing the XAFS, and the people conducting other kinds of complementary measurements.

GETTING BEaMTIME:
There are many ways to get beamtime at a light source, but the most common is through peer-reviewed general user programs. Under this kind of system, you submit a proposal explaining how you want to use the beamtime, which other scientists then review and rate. The strongest proposals are then awarded beamtime.While the proposal requirements for every synchrotron are different, here are some general pieces of advice when applying for general user time:• Consider your choice of beamlines carefully. At some light sources, you apply for time on a specific beamline, while at others you apply to the facility and let them choose the line. Either way, it helps to know what beamlines will work best for your experiment and which won't work at all.• Contact beamline scientists for lines you're considering before you apply to discuss the gist of your idea. They may have good suggestions for how to make the most of their line. On the other hand, don't send them a full proposal to prereview-they're busy people! Just a few paragraphs in an e-mail or a few minutes on the phone are sufficient to get the conversation started.• Your proposal should be specific about how you plan to use yourtime. Indicate information about the number of samples and how they differ, the number of standards, the edges that you will measure, and rough time estimates.

Getting Beamtime 43:
• Be realistic. Build in some time for sample changes, unexpected problems, and the like. If the experiment is straightforward and you intend on running 24 hours a day (hopefully there's more than one of you!), then adding about 50% to the actual scan time should account for setup, sample changes, and challenges. Of course, if the experiment is more involved (perhaps an operando experiment involving wet chemistry), then your cushion should be larger.• Realize your reviewers may not be specialists in your field. They probably know XAFS better than you do, but you know your system better. Describe the scientific questions you hope the experiment will answer clearly, briefly, and without jargon. Describe the importance of your investigation in a way that would make sense to someone outside your field. But don't go on at length about how wonderful XAFS is or how it works; the reviewers know that already!• Indicate why synchrotron radiation is important for your experiment. If the reviewers think your scientific questions can be answered in some other way they are less likely to recommend that you be awarded beamtime.     • The first thing we should do is identify the scientific questions we would like answered about our system.• A beamline can get x-rays (in order of increasing brilliance) from a bending magnet, a wiggler, or an undulator.• Undulators can produce more intense x-rays than wigglers and bending magnets, but there are additional complexities associated with using them for EXAFS, and not all experiments benefit from more intense x-rays.• Measurements may be performed in transmission, fluorescence, or by using electron yield.• Common detectors include ion chambers (able to handle high count rates) and multielement semiconductor detectors (able to discriminate the energy of x-ray photons).• XAFS experiments are best at comparing things, so we should be measuring more than just our best sample: either standards or a series of samples or a series of conditions.• Applications for beamtime should be clear, specific, and brief.

Sample Preparation:
Cut Twice, Measure Once Wait...is that right?Dear Simplicio, you don't want to have to take the same sample to a synchrotron to be measured more than once, do you?No, but...Would it make more sense, my friend, if we said "Do your physical sample preparation carefully, so that you don't waste beam time?" Oh, OK. But that wouldn't make a very good chapter title. I guess we should leave it the way it is.3

XAFS SAmPleS:
It's possible to perform XAFS analysis on nearly any sample of material. But the physical characteristics of the sample and the conditions under which you wish to measure it will affect how you prepare your sample and which measurement techniques you choose to use.Some examples of sample type are as follows:• Bulk solids   Although XAFS can work with almost any type of sample, that doesn't mean that all types work equally well or are equally easy to deal with. Some samples, for instance, will not be able to be prepared with an "ideal" thickness or concentration, resulting in a degradation of data quality or a trickier analysis.Likewise, some environments, such as corrosive atmospheres or extreme temperatures, will complicate your choice of sample holder and window. Finally, some materials (particularly organics) can be damaged by x-rays, and this may require modification of the experiment. But chances are someone has attempted something similar before, and checking the literature and online resources for similar experiments should be your first step in designing a XAFS experiment for an "unusual" sample.

Undesirable Photons:
In theory, a XAFS spectrum is supposed to represent the probability of photon absorption as a function of energy. Depending on the measurement technique employed, this will be some function of the ratio of the 48 Chapter 3 -Sample Preparation intensity of x-rays or electrons coming out of or through the sample compared with the intensity of x-rays heading into it. But a real experiment is not so simple. Frequently a detector will measure some photons that you aren't interested in. Likewise, photons that you would like to have counted might not be, particularly in fluorescence measurements.Here are the most common examples of photons that we don't want to count in a transmission experiment:• Scattered photons. Some of the photons that don't pass through the sample may nevertheless be deflected into the detector downstream of the sample (the I t detector).• Harmonics. Monochromators usually allow through some harmonics: photons with energies that are integer multiples of the desired energy (although not necessarily all of the integer multiples, depending on the monochromator crystal and its orientation).• Distribution tails. Monochromators, despite the name, do not produce perfectly monochromatic beams. In addition to harmonics, there will be a spread of energies around the desired energy; some photons with an electron volt or two higher or lower than the nominal energy are often present.The XAFS community uses the expression thickness effects to describe the impact of these photonsAlthough these real-life complications are always present to some extent, for some samples the effect on the spectrum is negligible, whereas in others the data will be severely distorted. Why the difference? Often, it comes down to sample thickness and uniformity. For instance, a transmission sample that is too thick may leave very few of the desired photons downstream of the sample to be counted, and most of the counts might then be due to the undesirable photons. On the other hand, a transmission sample that is too thin will absorb very few photons, and statistical noise could overwhelm the signal.

the Absorption Coefficient:
Our next step is to make this quantitative. TableThe absorption coefficient μ is a quantity that expresses the fraction of photons that are absorbed when an x-ray beam is passed through a given thickness of a particular material:

Absorption 49:
When XAFS scientists calculate " absorption," they are actually computing attenuation, that is, the reduction in intensity of a narrow beam. This is due mostly to bona fide absorption of photons (called photoelectric absorption), but also includes the reduction due to the small fraction of photons that scatter out of the beam.The absorption coefficient is dependent on the energy of the photons absorbed. Every chemical element absorbs progressively less as energy is increased (with the exception of the sharp jumps near its own edge energies). Also notice that it depends on the density of a material; a more densely packed material will have more atoms in a given thickness, and According to the table, it seems that μ could mean three different things. How are we supposed to know which one someone means?You can tell by the units! The mass absorption coefficient has units like cm 2 /g, the absorption coefficient has units like cm -1 , and absorption has no units.You really should be able to tell by the context of the equation-who cares what symbol someone uses?I care, dear Carvaka! It is confusing, and we should all try to make the world a less confusing place. Just because other people sometimes use the symbols in sloppy ways does not mean we should. In this book, we will always use the symbols in the first column. counts, V, A Intensity of fluorescent x-rays thus a higher absorption coefficient. Therefore, it is often useful to work with the mass absorption coefficient:μ m is directly proportional to the probability that a given atom in the material will absorb a photon and can generally be calculated from tabulated data. For example, the μ m for elemental arsenic, just above its K edge, is about 179 cm 2 /g

thickness, mass, and Absorption lengths:
The definition of absorption coefficient can be easily integrated to give the decrease in intensity found in transmission, a result known as Bouguer's Law:Colloquially, people often refer to μx for a particular sample as the number of absorption lengths it has.This allows you to find the thickness of a sample that corresponds to a desired number of absorption lengths. For example, consider a thin film made of zinc oxide (ZnO). Just above the zinc K edge, the mass absorption coefficient of zinc is 254 cm 2 /g and for oxygen it's 7 cm 2 /gBouguer's Law tells us that the fraction of the photon intensity that makes it through the sample is exponentially dependent on the number of absorption lengths present. Although it's a simple calculation to do yourself, TableTherefore, if you have a "thin film" of ZnO 87 μm (10 absorption lengths) thick and shine x-rays on it with energies just above the zinc K edge, virtually none of the x-ray photons will get through! There's little point in attempting a transmission measurement on such a sample. Fluorescence would be better, but that has its own perils, which are discussed later in this chapter. Better yet would be using a thinner sample, but that's not always possible or convenient.With samples that are thin films, solutions, gases, or sectioned solids, you generally know (or can choose) how thick they are. But what about powders? It is difficult to directly measure the thickness of a thin layer of powder, and there may be some air spaces between grains. Fortunately, by measuring the area over which a layer of powder is spread, it is easy to convert the thickness to a mass:As you can see, you don't even need to know the density of the material! So if, for example, you want a layer of ZnO powder that corresponds to one absorption length just above the zinc K edge, and you want the sample to be 2.0 cm 2 , then you need 9.8 mg of powder. If you use 100 mg of powder, you're barely going to get any signal through at all.

Uniformity:
It is extremely important that the sample be as uniform as possible over the width of the beam. This is because the log of sums is not the same as the sum of logs, and so unevenness results in distortion of the data (see Box 3.2). It is therefore imperative that you do whatever you can to your particular sample to make it as evenly distributed as you can.Pinholes, the usual term for spots where the beam can reach the I t detector without traversing any sample, are an obvious sign of nonuniformity. Less obvious, but potentially just as important, is the effect of particle shape and size. If the individual particles comprising your sample are spheres with diameters greater than about 0.2 absorption lengths, then the sample will be inherently uneven and substantial distortion will result.For a detailed analysis of these effects, see52 Chapter 3 -Sample Preparation If we consider a sample made of a single layer of spherical particles, then a photon coming in along the path shown by the arrow on the left would see a much thicker sample than one coming in along the arrow to the right.

Sample Characteristics for Transmission 53 boX 3.2 QUAntiFYinG tHe eFFeCt oF SAmPle nonUniFormitY:
Friends! Let us illuminate this idea further by exploring it quantitatively. For our example, consider a sample with two different regions within the beam, one of thickness x 1 and area A 1 , the other of thickness x 2 and area A 2 . The intensity in each region obeys Bouguer's Law, as we have learnedThis gives us a total transmitted intensity ofIf the usual experimental procedure is used, we would then calculate absorption to beBut in XAFS, our analyses are generally based on the energy dependence of absorption. So let us consider the ratio of the absorptions at two different energies:(3.9)Unless x 1 = x 2 , this ratio depends explicitly on A 1 , A 2 , x 1 , and x 2 , meaning that the shape of our XAFS spectrum no longer depends only on the absorption coefficient μ, but also on the details of the sample preparation in a way that is probably not known by us when we conduct the experiment. This will therefore distort our analysis.How large is this effect? To see, let us consider the case where half the sample is twice as thick as the other half, so that x 2 = 2x 1 andIf the sample were of uniform thickness x 1 , the absorption would be μx 1 ; if it were of uniform thickness x 2 = 2x 1 , then the absorption would be ln 2 + μx 1 . So we see that the -+ ( )is trying to provide an absorption somewhere between the two; the problem is that it is itself dependent on μ in a nonlinear way. To further explore this effect, let us suppose that at energy E a , μx 1 is 1.000, and at energy E b , μx 1 is 1.100. The ratio of absorptions measured with our hypothetical sample would not be 1.100/1.000 = 1.100, as desired, but 1.091.

(continued):
There are a couple of things you can do to assure that you are measuring a uniform region of your sample. One is to make the sample as uniform as possible in the first place; techniques for doing this will be covered later under the specific sample types. Another is to try to find a uniform section of a sample after it is made. If aberrations are noticeable visually, it will help to mount the sample in such a way that the uniform part is placed in the beam. You can also scan as a function of position of the sample to find its most uniform part (Section 5.7 of Chapter 5).

A Short Digression on noise:
As you probably know, noise refers to random, unbiased variations in a measurement; that is, it will average out if sufficient data are taken. But that's a broad definition. Let's start by discussing shot noise, also known as Poisson noise or noise due to counting statistics. This noise arises from concepts of probability: if on average 100 photons are absorbed in a chamber per second, then during a particular second, it might be 93 and during another second, it might be 104 or any other number fairly close to 100. But what's "fairly close?" It turns out that such measurements tend to vary as the square root of the countThis means that if we had more photons, say, an average of 1,000,000 per second, then the typical variation in the number of photons we count would be much greater (1,000, in this case), but the typical percentage variation would be smaller (1,000/1,000,000 = 0.1, as compared to 10/100 = 10 earlier). In fact, it's easy to convince yourself that the percentage variation goes inversely as the square root of the number of counts, so that if we 54 Chapter 3 -Sample Preparation Yeah, but in practice, that's not the way it works. It's really hard to make a sample thicker without making the variation in the thickness bigger too. Putting lots of uneven layers on top of each other makes the variation in thickness increase in just the right way to cancel out the improvement from having a thicker sample. And there are other problems with thick samples, as we'll see shortly.increase the number of counts by two orders of magnitude the percentage variation goes down by one order of magnitude.XAFS beamlines at modern light sources can easily produce beams strong enough to produce 10 10 photons per second in a detector, yielding shot noise on the order of 0.001% of the signal.But counting statistics isn't the only source of noise. Another prominent source, for instance, is the electronics that amplify and process the signals from the detectors. With modern synchrotrons and concentrated, wellprepared samples, electronic noise is likely to be much more significant than shot noise.There are also other sources of noise. A number of effects show up as systematic errors if they change slowly (or not at all) relative to the data collection time, but as noise if they fluctuate rapidly. In this category are effects such as changes in the detectors themselves (perhaps the temperature or pressure of the gas in an ion chamber) and effects tied to the orbit of the electrons in the storage ring.

optimum thickness for transmission:
It's clear that 10 absorption lengths are too thick for a transmission sample, because very few photons will make it through to be counted. But there's a similar problem for very thin samples: too few photons are then absorbed. But that's not the only problem with samples that are too thin or too thick.If a powder transmission sample is too thin, then,• Very few photons are absorbed by the sample. Why is this a problem? Consider the signal to noise ratio due to noise in I o and I t . Using standard methods for the propagation of uncertainty, we find the signal to noise ratio isAs we take the limit of thinner and thinner samples, μx (the signal) approaches zero, but the expression in the denominator (the noise) levels off at some nonzero value. The result is that the signal to noise ratio becomes very poor.

Sample Characteristics for Transmission 55:
EXAFS features are usually less than one tenth the amplitude of the major XANES features. So shot noise of 0.001% of the overall signal would be more than 0.01% of the EXAFS signal.Even with modern light sources, there are still times that shot noise is important. For instance, if you are taking time-resolved data or using a microprobe with a sample vulnerable to beam damage, the number of photons per data point measured in your transmission chamber could be fewer than a thousand. And once we start talking about fluorescence samples, shot noise is often the dominant source of noise.• The absorption of harmonics will be much less than for photons of the desired energy. A beam including 1% harmonics before traveling through a thick sample could easily be 70% harmonics after. The same is true of photons scattered within the hutch.• Limited monochromator resolution also affects thick samples more. Just above the edge of a thick, concentrated sample, nearly all the desired photons may be absorbed, but those just a few eV lower may make it through. Although this is similar to the effect of harmonics and scattered photons, monochromator resolution only has a noticeable effect on features that vary rapidly in energy, such as the edge and the white line.• It is difficult to judge uniformity by inspection, either with the naked eye or with a microscope.Over the years, XAFS experts have analyzed thickness effects and come up with recommendations for the "ideal" thickness of a transmission sample. A few of these recommendations are summarized in TableIt's worth noting that the recommendations for total absorption include all the material between the I o and I t detectors, not just whatever you deign to call the "sample." Windows, substrates, and air (particularly at low energies) also contribute to this absorption. We recommend ~1

Chapter 3 -Sample Preparation:
On the other hand, if it is convenient to do so, one should certainly strive to make transmission samples roughly 1-2.5 absorption lengths in thickness. With powders, that is not difficult to do, and is a good compromise between thin and thick.

edge Jump:
The difference in absorption between energies before the edge and energies just after the edge is referred to as the edge jump (or edge step) and is often written Δμx. In theory, the edge jump in transmission is directly proportional to the thickness of the sample and also to the concentration of the target element in the absorbing material. Because of undesirable photons, this won't be exactly true, particularly for thicker samples.Because fine structure is proportional to the edge jump, a tiny edge jump means tiny fine structure, and thus a poor signal to noise ratio. But a really large edge jump is sensitive to undesirable photons.Most XAFS experts recommend aiming for an edge jump of around 1 (but see Box 3.3). Beyond 1.5, it is important to take great care that thickness effects due to undesirable photons aren't distorting your spectrum.

Sample Characteristics for Transmission 57 boX eDGe JUmP or totAl AbSorPtion?:
You've given me two different criteria to worry about: the total absorption should be around 2 and the edge jump should be around 1. But often I can't do both! If my sample has a small concentration of the target element, then if I make the total absorption right, I'll get a small edge jump, but if I make the edge jump right, I'll get a big total absorption. What should I do?The total absorption is more important than the edge jump, Kitsune, because if the total absorption is too high, undesirable photons are almost certain to distort your spectrum. If there's not much of your target element in your sample, then you can make the sample a bit thicker than normal-maybe 2.5 absorption lengths-if you're really careful to keep harmonics out of the beam. If that means the edge jump is only 0.2 or even 0.02, that's still OK. And if your sample is even more dilute than that, you should probably just bite the bullet and use fluorescence. But it's so much easier to check if I've got the edge jump right and not worry about total absorption-all I have to do is look at the spectrum!! Measuring the total absorption is hard. No it's not! Just look at I t before you put the sample in and then look again after. If it drops by a factor of 20 or more, then your sample is at least 3 absorption lengths thick, which is probably thicker than you want.In this section, we haven't defined edge jump very rigorously, but for our current discussion, we don't need to. We'll be more careful about it in Section 4.3.4 of Chapter 4.

Chapter 3 -Sample Preparation boX 3.4 SCArY GrAPHS:
It's kind of traditional to show you graphs of "bad" data caused by "bad" sample preparation, maybe with the idea that it will scare you into making sure you always make a perfectly ideal sample. But your sample may not be the kind of thing you can make textbook perfect. And let's be honest; sedimentation, sieving, grinding, and weighing each take time. Those of us who don't have a big research group to fall back on often face a bottom-line decision: Is it better to get beautiful data on 5 samples or decent data on 20? That depends. If I were a graduate student and those five samples were the heart of my dissertation, and I were making my argument based on finding coordination numbers to within ± 10%, then I would do everything I could to get the best possible data. If instead I were doing some preliminary work to try to understand the speciation in a range of samples, then I might be happier measuring a larger number of samples.Whatever you decide, make sure you give a sense of what you did in any eventual publication. If you sieved the samples, for instance, say so. That way your audience can judge for themselves how far to trust the data. So we're not going to show you a comparison of "perfect" data to "bad" data. Instead, we're going to compare "typically good" data to data taken from samples that are extremely thin, thick, or uneven. That way you can clearly see what kinds of problems arise in each case.To illustrate the comparison, we are going to use graphs of χ(k) collected for at the zinc K edge of zinc ferrite. Although graphs of this type were introduced in Section 1.4.1 of Chapter 1, they will be discussed in more depth in Section 4.4 of Chapter 4.For our "typically good" data, we're using a single iron K-edge scan of zinc ferrite. The edge jump of the sample was 1.0, and the total absorption, counting the tape, was 2.9 absorption lengths. That's a little on the thick side, so we tried extra hard to reduce the harmonic content of the beam. The sample was ground finely and spread thinly on layers of tape, but not sieved or separated by sedimentation.Our first comparison is with a thin sample of the same material. For the thin sample, the edge jump was 0.09 and the total absorption was 0.5 absorption lengths.Why is the edge jump of the thin sample less than 10% of the edge jump of the "typically good" sample while the total absorption is more than 15%? Because there's some "overhead" associated with the layers of tape used to seal the sample.

boX 3.4 SCArY GrAPHS (Continued):
You can tell from the label on the y-axis of FigureIn this case, we can see that the thin sample is much noisier than the typically good sample (look at the jagged features visible in the thin sample's high-k spectrum). But despite the typically good sample being on the thick side, there's no evidence of trouble from harmonics, which would show up at low k as well as high.Although most of us can recognize noisy data from a single scan because of the point-topoint variation at the high-k end of a k-weighted spectrum, taking additional scans should show that the noise varies from scan to scan in an unpredictable way. One should always collect more than one scan on any sample one is interested in, if only to have a way of judging the amount of noise that is present.Next up is a really thick sample (FigureThere's not much evidence of noise this time, despite the small number of photons making it through to it-just a hint above 11 Å -1 . But see how the amplitude is suppressed? That's due to harmonics. Because the photons constituting the harmonics have a much higher energy than those in the fundamental, they have an easy time passing through the sample and are not affected by the edge or the fine structure. At energies with a lot of absorption, such as near the white line, many of the photons belong to the harmonics, and so the feature is suppressed. Pre-edge features, on the other hand, would not be suppressed nearly as much, because many more of the photons belonging to the fundamental make it through at those energies. This pattern is particularly easy to see if we look at the XANES (Figure

From incidence to Detection:
In a transmission experiment, the direct measurement is of photons that made it through the sample unscathed; absorption by the sample is deduced by comparing the intensity of photons downstream of the sample to those upstream.In contrast, fluorescence experiments measure photons emitted by atoms that have absorbed an incident x-ray. This introduces several subtleties into the measurement. To understand them, let's look at an overview of the process that brings a signal photon to the detector:1. As in transmission, some of the incident photons of the energy E selected by the monochromator are absorbed in the I o chamber. There are also a few undesirable photons (harmonics, etc.) in the beam. 2. Also as in transmission, on the way from the I o detector to the sample, photons may be absorbed or scattered by windows, air, and the like. 3. Once the photons penetrate the sample, many things can happen to them. They may be scattered elastically, scattered

Sample Characteristics for Fluorescence 61:
even more messed up than it was for either the thick or thin sample. As we'll get to in Chapter 5, uneven samples are particularly vulnerable to problems in I o or nonlinearity in the detectors, and that kind of thing is contributing to the problems we see here at high k.Honestly, uneven samples are the worst of all worlds; spots that are particularly thin or thick degrade the signal to noise ratio, thick spots exacerbate the effect of harmonics, and the lack of uniformity introduces additional distortions. Typically good Unevenfor two samples of zinc ferrite; one prepared fairly well and the other prepared to be uneven.

boX 3.4 SCArY GrAPHS (Continued):
inelastically, absorbed by the element of interest, or absorbed by another element. Or, if the sample is fairly thin, they may make it out the other side. The only one of these processes that we actually want to measure is absorption by the element of interest. 4. The atoms of the element of interest that do absorb a photon are now missing an electron in one of their lower energy states; they have a core hole. This hole may be filled in various ways; the case that we're interested in here is when an electron from a higher shell falls into the hole, with the difference in energy between the two energy levels appearing as a photon. Note that this fluorescent photon is at a lower energy (called E f ) than the incident photon was, because there's still a hole-it's just that the hole is now in a higher level. 5. Fluorescent photons are emitted isotropically; that is, the direction of any individual fluorescent photon is random. Most will not head toward your I f detector. 6. Those that do head toward the detector stand a chance of being absorbed or scattered again before emerging from the sample. They do have an advantage over the incident photons, though, because E f is below the absorption edge. 7. After emerging from the sample, the fluorescent photons still have the usual assortment of windows and air to penetrate. But in addition, some fluorescent setups include a filter. The filter is designed to screen out some of the undesirable photons. Some of the fluorescent photons we do want will themselves be absorbed or scattered by the filter. 8. Finally, the desired photons, along with a lot of undesirable background, will reach the detector. If the fluorescence detector is an energy-discriminating detector, then the detector itself is able to screen out photons with energies more than about 200 eV higher or lower than the desired energy, which is good. Unfortunately, detectors of this type have a limit on how quickly they can count photons, so that there are difficulties if the number of photons in the desired energy range is too high. For more information on detectors and filters, see Section 5.6 of Chapter 5.Compared to transmission, then, there's a lot going on! One of the things to notice is that there aren't going to be nearly as many photons going into the fluorescent detector as would go into the I t detector in a well-designed transmission experiment. After all, not all of the absorption generates fluorescent photons and then only some of those head in the right direction. We will examine the ramifications of this in Chapter 5.To understand how sample characteristics influence fluorescence measurements, let's consider four extreme cases.62 Chapter 3 -Sample Preparation 3.4.1.1 Limiting case: Thin and concentrated This case works a lot like transmission and in fact can often be measured simultaneously in that mode. Most of the photons make it through the sample, but those that do not interact primarily with the target element. The probability of interaction depends on the absorption coefficient ( ) * E µ of the target element at each energy, but the fluorescent photons don't have to worry much about getting absorbed on the way out to the detector.

Simple!:
We can use this logic to work out how ( ) * E µ depends on I f . Suppose a fraction a of the incident photons that aren't transmitted through the sample end up generating fluorescent photons in our fluorescence detector. In other words:or, solving for I t ,In the limit of a thin, concentrated sample, a is only weakly dependent on the energy of the incident photon, and will not itself contribute to XAFS, which includes only relatively rapid variations in ( ) * E µ .Substituting into Equation 3.4 gives us:Because the sample is thin, I f << aI o , and we can use a Taylor expansion to write:This leads directly to the simple result:3.4.1.2 Limiting case: Thin and dilute This is similar to the thin concentrated case, except that in step 3 in Section 3.4.1, we'll have relatively less absorption by the element of interest relative to other events. This leads to a weaker signal relative to the background, and thus the signal to noise ratio will be worse than in the thin concentrated case. is. And because the fluorescent photons are necessarily lower in energy than the absorption edge being studied, they will be able to make it out to the fluorescent detector with relative ease regardless of what ( ) * E µ is.

Sample:
The result is that your "signal" will show a jump at the edge, followed by a featureless horizontal line-no EXAFS! This is admittedly an oversimplification. The processes that are described in Section 3.4.1.4 for the thick dilute case apply weakly to this case as well, producing some EXAFS in the signal, but the features will be greatly suppressed compared to the other limiting cases.For historical reasons, the suppression of signal in thick concentrated samples is referred to as self-absorption or sometimes over-absorption.

Limiting case::
Thick and dilute Suppose we have a sample that is very thick, but that has only a small fraction of atoms of the element of interest.As in the thin dilute case, absorption and scattering by elements other than the element of interest contribute a substantial background. But in a surprising twist, the other elements help alleviate the self-absorption problem!One way in which they help is by absorbing many of the incident photons. Unlike in the thick concentrated case, where the element of interest is likely to absorb the photons whether ( ) * E µ is high or low, the element of interest has competition from the other elements. If at a particular energy it does not absorb well, the other elements are likely to get the photon first. This helps make the fluorescence from the element of interest lower at energies at which ( ) * E µ is low, which of course is the behavior we would like. But that's not all. At energies at which ( ) * E µ is low, photons absorbed by that element are likely to be absorbed at a greater depth. In turn, fluorescent photons from those absorption events have to travel through more of 64 Chapter 3 -Sample Preparation Correcting for selfabsorption in a measurement is sometimes possible, although you have to know a lot about your sample to do it successfully! Self-absorption correction schemes will be addressed in Section 4.3.5 of Chapter 4.the sample to get back out. Those fluorescent photons are lower in energy than the edge energy of the target element, and thus, they pass through other atoms of the target element easily on their way back out. But the other elements in the sample generally absorb more strongly at the energy of the fluorescent photons than they do at the incident energy, simply because in the absence of edges, absorption is greater at lower energies. Fluorescent photons absorbed at a relatively great depth therefore have less chance of making it back out of the sample than those absorbed at more shallow depths. Thus, this effect contributes to suppressing fluorescence at energies for which  where B e (E) is a slowly varying function of energy due to effects such as absorption by other elements in the sample. Once that background is subtracted out, Equation 3.17 is the same as EquationIt is important to realize that there is no physical reason that the proportionality for the cases in Sections 3.4.1.1 and 3.4.1.4 should have turned out to be the same-it's just one of those lucky happenstances of nature that it did.

Understanding Your Sample:
Thick dilute and thin concentrated samples are the least problematic when it comes to fluorescence, but as with transmission, sometimes you don't have control over the form of your sample. If you are measuring a new thin film technology, you may be stuck with something that is thin and dilute, for instance. And if you are measuring, say, a priceless ancient relic that is thick and concentrated, it is not an option to try to cut it down to size! In this section and in Chapter 5, we provide some methods for coping with self-absorption, background photons in fluorescence measurements, and low count rates. But it's important to realize ahead of time which issues are most likely to be major ones for your samples, so that you can be prepared to take the actions necessary to get usable data.There are many different circumstances that can arise when considering fluorescence experiments on samples, but they all face the issues we've

Sample Characteristics for Fluorescence 65:
The physics of the thick, dilute case is really tricky to understand. Most people just memorize that self-absorption is a problem with thick, concentrated samples measured in fluorescence, and that for both the thin concentrated and thick dilute cases, ( )You and your shortcuts, Dysnomia! Some of us like thinking through the physics...The assumption that B e (E) is a slowly varying function of energy, and thus can be subtracted out in a data processing step, is better for an energy-discriminating fluorescence detector than for something like a Lytle detector.already described. To show you how these issues can interact, let's take a few examples.3.4.2.1 A thin film on a thick substrate, such as some common solar cell designs If the film is thin compared to one absorption length of the material, then self-absorption will not be much of an issue. (We'll be quantitative about what "not much of an issue" means in Section 3.4.3.) But the photons that penetrate the film will hit the substrate, and it can fluoresce or scatter the photons. In that circumstance, the biggest problem will be background photons.If the film is several absorption lengths thick, then self-absorption will be the main issue. The person who made it may refer to it as a "thin film," and it may be less than half a millimeter thick, but it might as well be a boulder as far as the fluorescent signal is concerned.

A heterogenous soil sample with the element of interest concentrated in grains sparsely scattered through the material What matters here is the grain size. If the individual grains are several absorption lengths thick, then those parts of the sample are thick concentrated and will suffer from self-absorption. It doesn't matter that only a small part of the cross section of the beam is hitting a grain; the part that is, and thus the part that is measuring μ(E) for your target element, is seeing a concentrated sample.(To put it another way, there is little correlation between μ and the depth of the grain that does the absorbing.) In fact, this is potentially the worst of both worlds; the sample is concentrated in the sense that it suffers from self-absorption, but dilute in that your signal to noise ratio will be poor.There are a couple of methods for dealing with such a sample. One is to use a microprobe beamline capable of focusing the beam on to just one grain. You'll still have self-absorption effects to deal with, but at least the number of counts will improve.Another solution would be to grind up the sample, breaking up the grains into pieces much smaller than an absorption length in size. This makes the sample truly thin dilute and is probably the best route if it's feasible to do and if you don't need spatial information.

A material in solution:
Usually solutes qualify as dilute, and you are also usually able to choose how deep to make the sample cell. Therefore, the thick dilute limit should be achievable. Care must be taken, however, with the sample cell that is used to hold the solution.If part of the beam intersects the sample cell, it will contribute to the background. The author of this book learned that lesson the hard way, by spending several hours collecting spectra that turned out to be due to the stainless steel screws used to hold a solution cell together.

A gas:
If the thickness of the cell is chosen well, it may be possible to measure the gas as a thin concentrated sample. But in that case, the 66 Chapter 3 -Sample Preparation beam will penetrate to the back wall of the sample cell. Therefore, care must be taken to make the back wall a material that will not fluoresce extensively at the energies of interest. Even better would be a window at the back wall that allowed most of photons that made it that far through, so that scattering is also minimized.

the mathematics of Self-Absorption:
The dependence of the measured intensity at the fluorescence detector on several key factors is given in this proportionThe meaning of the symbols in this proportion is summarized in TableEven this rather complicated-looking expression is considerably simplified. For example, depending on the detection scheme, there may be more than one E f , as different electrons can fill the core hole. Likewise, there is actually a range of angles to the fluorescent detector; to maximize counts, fluorescent detectors are usually given a large area and placed near the sample. But this expression is still helpful for estimating the size of self-absorption effects and for understanding how to minimize them.For example, consider a thick sample. In that case, the exponential term approaches 0, simplifying the expression.Now consider a dilute sample. If the sample is dilute, then μ will be primarily due to atoms other than the target element. As long as those elements don't have edges near that of the target element, μ will be a slowly varying function of E, and thus I f will roughly be proportional to ( ) * E µ I o . This gives the happy result that, assuming the slowly varying 3.4 Sample Characteristics for Fluorescence 67 B e (E) can be subtracted off in some way (we'll see how this is done in Section 4.4.1 of Chapter 4), the energy dependence of the absorption of the target is now easy to measure:What about a thin concentrated sample? In that case, the exponential can be expanded in a Taylor series, with the result that the troublesome denominator cancels out, leading towhich also directly yields the energy dependence of the absorption of the target.

Geometrical Factors:
Because fluorescence is isotropic, it might seem that the fluorescence detector could be placed at any angle relative to the incident beam and be equally effective. But although fluorescence is isotropic, scattering is not. In the elastic limit, the probability of scattering in a given direction depends on the angle α relative to the incident direction and the angle β relative to the plane of polarization (Heitler 1954):Because synchrotron radiation is generally horizontally polarized, there should therefore be no elastic scattering if the fluorescent detector is mounted on the same table used for the sample (β = 0) and the I o and I t detectors, so that the line from it to the sample is perpendicular to the beam (α = 90°), as shown in Figure

Chapter 3 -Sample Preparation:
Proportionalities 3.19 and 3.20 could be written to look exactly the same, since x is a constant for a given sample and could thus be made part of the proportionality constant. But we left it this way to emphasize that the agreement of the two proportionalities is a mathematical coincidence. The physics of the two cases is completely different. In fact, doubling the thickness of a thin concentrated sample doubles the signal (as long as the thin approximation isn't violated!), whereas doubling the thickness of a thick dilute sample has no effect. For inelastic scattering, such as Compton scattering, the angular dependence is more complicated and the minimum shifts a bit toward higher α (Heitler 1954). The minimum is quite broad, so 90° is still an appropriate choice. Of course, fluorescent detectors are designed and placed so as to intercept a large solid angle, so that they can count a reasonable fraction of the fluorescent photons. Even though there may not be many photons scattered toward the center of the detector, the large solid angle intercepted by the detector prevents scattering from being completely eliminated.The desire to minimize scattering tells us where to put the detector, but it doesn't tell us how to orient the sample. Suppose we use π to represent the angle the incident beam makes with the surface of the sample, and θ to represent the angle the path from sample to fluorescence detector makes with the surface of the sample (Figures 3.6 and 3.7). The position of the fluorescence detector tells us that,but doesn't tell us which combination of π and θ to pick.Frequently, the geometry is chosen so that π = θ = 45°-in fact, sample holders are often made that way. But there's nothing particularly special about that choice, and sometimes you'll want to use a different geometry.Consider, for instance, one of our problematic limiting cases: a thick concentrated sample (Section 3.4.1.3). We can now see how self-absorption comes out of Equation 3.18; the problem is that the μ(E) in the denominator has a large contribution from ( ) * E µ and thus will partially cancel the ( ) * E µ in the numerator. But μ(E f ) is also in the denominator, and it does not contribute to canceling the energy dependence that we want to measure. After all, μ(E f ) is measured at a single energy-the fluorescent

Sample Characteristics for Fluorescence 69:
If you think about the discussion of thick concentrated samples earlier in Section 3.4.1.3, you'll see why a grazing exit works to minimize self-absorption effects. Because the problem was that fluorescent photons could make it out of the sample too easily, forcing the fluorescent photons to travel through more sample will alleviate the effect. And that, Dysnomia, is why I prefer to understand the physics. If I had tried to get by on memorization, I never would have understood grazing exit. This may be a good time to mention that the spectra of samples such as single crystals depend on their orientation relative to the polarization of the beamenergy-and is thus just a number. (A more realistic treatment would acknowledge that there are several different fluorescent energies for the target element, but that doesn't change the argument: When measuring a given edge, the energies of the fluorescent photons do not depend on the energy of the incident photons.) So if there was just a way of emphasizing the μ(E f ) term in the denominator at the expense of the μ(E) term, we could minimize the self-absorption effect.And, fortunately, there is! If we choose to make θ small, µ θ ( ) sin E f will be large, μ(E f ) will dominate the denominator, and self-absorption effects will be minimized. Since θ is small, EquationWhat about the other problematic case, a thin dilute sample? In that case, the best geometry is the opposite: grazing entry, shown in FigureIn grazing entry, the beam effectively sees a thick dilute sample (or at least thicker), which will improve the signal to noise characteristics.

How thick is "thick"?:
How thick does a concentrated sample have to be before self-absorption starts to become a problem?To answer that question, let's consider the ultimate in concentrated samples: a film composed of a single metal. In that case, µ µ ≈ * . Let's also assume that you're using a sample holder where π = θ = 45°.The Taylor series expansion of the exponential in Equation 3.18 would then beBecause we're just trying to see roughly how strongly the self-absorption effect depends on the sample thickness, let's make some more simplifications: ignore µ * ( ) E f , because the target absorbs much more poorly below the edge, and assume B e (E) can be removed. With those simplifications, and using the fact that sin 45 1 2 °= , substituting the first three terms of the expansion into Equation 3.18 givesThus, if a concentrated sample is 0.1 absorption lengths thick for photons at the incident energy, self-absorption will reduce the signal by about 5%, enough to introduce noticeable errors into the analysis. This means that for many common edges, a concentrated sample less than a micron thick may still be significantly affected by self-absorption!

How Concentrated is "Concentrated"?:
Now let's see how concentrated a thick sample can be before it starts to exhibit significant self-absorption. As before, let's assume π = θ = 45°. This time, we'll imagine a sample thick enough that the exponential term goes to zero. As usual, we'll assume B e (E) can be removed during data reduction.Finally, let's break μ(E) into two parts: ( ) * E µ , the absorption due to the target element, and μ other (E), the absorption due to everything else. That yields:The factor in parentheses is acting to damp out fine structure. You can see that if the target element is responsible for 1% of the total absorption at some energy, then it will reduce the signal by about 1% at that energy-acceptable for most experiments.

SAmPle ConSiDerAtionS For eleCtron YielD eXPerimentS:
As described in Section 2.5.1 of Chapter 2, the electron yield mode of measuring XAFS spectra is a cousin to the fluorescence mode, with Auger electrons being measured instead of fluorescent photons.The most distinctive characteristic of this mode of measurement is the short mean free paths of the Auger electrons-less than 100 nm in most cases• Interference from a substrate is less likely to be an issue than with fluorescence.• Because it is always more difficult for the Auger electrons to escape the sample than it was for the incident photons to get in, selfabsorption is not an issue. (Compare to grazing exit fluorescence.)• Sample homogeneity is not an issue.• Unless you're interested in surface effects, the surface of the sample must be kept pristine. A few nanometers of oxidation on that shiny new alloy you're measuring won't affect a transmission or fluorescence measurement much, but can significantly impact your electron yield data.

Sample Considerations for Electron Yield Experiments 71:
Great! So this soil sample I have that's 1% chromium shouldn't be a problem.Not so fast!! The mass absorption coefficient for chromium just above its edge is about 500 cm 2 /g. "Soil" can mean a lot of things, but typical soils are mostly oxygen (~30 cm 2 /g above the chromium edge), silicon (~150 cm 2 /g), aluminum (~120 cm 2 /g), and iron (~80 cm 2 /g). Depending on the proportions present in your sample, the 1% chromium by mass could be responsible for 10% of the absorption above its edge!The biggest limitation of electron yield measurements, however, is that the need to collect electrons from the sample makes it difficult to measure an insulating material. The necessity of having the sample intimately connected to the detector also complicates the design of many experiments, often (but not always) resulting in the sample being placed in vacuum or helium.

WHiCH teCHniQUe SHoUlD YoU CHooSe?:
In XAFS for Everyone, we have intentionally refrained from providing hard and fast "rules" for when fluorescence, transmission, or electron yield should be used. Instead, we've provided you with the tools for understanding what issues will crop up as you use samples that stray from the "ideal" sample for each technique.For example, suppose you have a powder that is dilute enough so that a sample one absorption length thick would only have an edge jump of 0.03. Measuring in transmission is not out of the question-pushing the thickness to 2.5 absorption lengths would give an edge jump of 0.08, which probably will be OK in terms of signal to noise. But with a sample that thick in absorption, you'll have to be careful about harmonics. You could also choose to make a thick sample and measure in fluorescence, but it's a little on the concentrated side for that, so you'd have to be careful about self-absorption, perhaps using a shallow angle of incidence if the surface can be made very smooth. Another good approach would be to measure the sample both ways; since harmonics and self-absorption both tend to reduce the size of EXAFS features, you could then pick the spectrum where they were suppressed less, or feel confident in the data quality if they both match!

PrePArinG SAmPleS:
One of the most important things you can do, especially when working with reactive chemicals, is to test run everything you can before you get to the synchrotron. Set up as much of your system as possible at your home institution. This will ensure that your Teflon containers aren't going to dissolve unexpectedly; or that the adhesive on your Kapton tape won't melt when you try to measure a temperature series; or that a tube with glacial acetic acid pumping through it won't come loose and spray everywhere, causing every scientist in the facility to stop by at some point and say, "Oh, so you're the one responsible for that awful smell." The experimental floors of light sources are usually very open places, and the ramifications of a catastrophic failure are often much worse than they would be in the familiar environment of a dedicated academic or industrial laboratory. At the very least, failures of this sort make a mess and jeopardize the experiment and at the worst could compromise the safety of people and the environment.While we're on the subject, we should stress that one thing we will not try to do in XAFS for Everyone is to provide safety guidelines for hazardous samples. Hazards vary in type and severity, and precautions that are adequate for a suspected carcinogen might not be appropriate for a strong oxidizer. And although safety personnel at synchrotrons can help you plan your experiment, no one there is likely to be as familiar with your materials as you are. In the open environment on a synchrotron experimental floor, precautions may be necessary that aren't required in your home laboratory and safety personnel can help guide you through those issues. But when it comes to questions like the chemical compatibility of your sample holder with your sample, you shouldn't trust that someone else is going to foresee hazards that might occur.

Powder:
Powder is a common sample form in the XAFS world and can produce good data if you prepare it properly. If you are planning to run the sample in transmission, the hardest part is making sure the individual grains are all considerably smaller than one absorption length-if they aren't, you will not be able to make a uniform sample and the spectra will be distorted.There are several approaches that can help make sure the grains are small. Depending on how much sample you have, you may find it helpful to use a sieve to select for the smaller particles first. However, 635 mesh is the finest sieve readily available, and it still allows through particles up to 20 μm. For many samples that is more than one absorption length.Using a mortar and pestle (or a grinding mill such as a Wig-L-Bug) is sometimes an effective way to reduce the powder to a fine enough grain.If your substance will not grind, or you are concerned it has not been ground finely enough, you may want to try sedimentation, described in Section 3.7.1.2.Another common technique for selecting only the smallest particles is to spread them thinly on adhesive tape. Larger particles tend to brush off the adhesive, but the smallest ones stick.Finally, uniformity can be measured at the beamline, and the most uniform section of the sample chosen. This won't work unless most of the particles are small, but if an occasional "boulder" found its way in, you may be able to avoid shooting through it. So many people mix the sample with a filler. This can also reduce the orientation problem Kitsune mentioned in Section 3.4.4.

Fillers:
If you do decide to dilute your sample with another substance, it is important to avoid contaminating your sample or causing it to react. You also need to choose a filler composed of elements with low atomic numbers (much lower than your sample!), so that the filler doesn't absorb too much. Typically, carbon black, boron nitride, or sugar are used (we've also heard of cellulose or mineral oil being effective). Make sure the material is not contaminated with an element you are trying to measure! Always know how much of the mixture is the target element, so that you can still calculate the appropriate thickness.

Procedure for sedimentation:
1. Grind the material as finely as possible.2. Place the powder into the bottom of a test tube or centrifuge tube.3. Fill with a liquid such as high-purity acetone or isopropyl alcohol.In theory, the liquid could be anything that doesn't react with your sample (which is one reason water is often a bad idea!), but the density and viscosity of the liquid will affect how long the process takes. 4. Assess how long it takes for the bigger particles to settle. This will depend on the density of the sample and liquid, the viscosity of the liquid, and the length of the tube. Fillers are also a method for achieving uniformity, right? It's a lot easier to make something a uniform thickness if you've got more of it.Yes and no. It's true that fillers can help you make the thickness uniform, and that's important. But if the individual grains of sample are large, you've still got a problem. In some places, the beam will pass through a big chunk of sample and not much filler; in others, mostly filler and not much sample. By having a nice smooth surface, you might actually be less likely to catch that kind of nonuniformity before getting to the beamline.Some researchers use white filler (e.g., sugar) with dark samples and black filler (e.g., carbon black) with light samples to make it easier to see the kind of nonuniformity Professor Carvaka just described. Some people even use epoxy; it is clear and relatively easy to visually inspect for uniformity, but it can be difficult to cut to the desired thickness.Contamination can be a particular concern with carbon black, which often contains some iron.may have to recycle the remaining mixture in the tube by letting the liquid evaporate. Start again at step 1 when the powder is dry. With very thin tapes (e.g., 0.3 mil Kapton) at energies above 6 keV or so, even 20 layers can work. Off-the-shelf Scotch Magic Tape is a bit thicker and limiting to half that number is reasonable. But absorption is strongly energy dependentif you're working at 15 keV, you could shoot through several millimeters of tape and barely notice; at 4 keV (the calcium K edge), you won't want to go much above three or four layers.

When you have roughly the total sample mass you're shooting for (or the maximum number of tapes you're comfortable with, whichever comes first), continue to step 6. 6. Sandwich your tape pieces between larger pieces of tape, so that they are sealed together. Yes, this does mean two additional layers of tape that the x-rays have to pass through, but synchrotrons generally frown on exposed powders, even if the material is something completely harmless like table salt. If some of your powder gets on beamline equipment and no one notices until later, the synchrotron staff will have no way of knowing the substance is not very dangerous and will have to regard it as such.If you're at a low enough energy so that an additional layer makes a difference, you might be able to fold the top cover tape around the edges of the sample instead of having a full piece on the bottom.If you run out of sample before reaching your target mass, you may have to fold your tape stack a few times as well. This can be tricky, but tweezers and your spatula should help. 7. (Optional) When you have your sample as you need it, it's helpful to find something to use as a frame, as this will aid alignment. Ordinary razor blades work well because they have a little window in the middle, although it's a bit small for some beams. (Just cut off the sharp edge with shears before using it, making sure to wear safety goggles as you do so.) Be particularly careful for fluorescence measurements that you are measuring your sample and not the frame! It's a good idea to take a scan with just a blank frame in place so you can understand how it interacts with the beam.76 Chapter 3 -Sample Preparation Some researchers like to turn their sample into a paint and do away with adhesive tape altogether.To do this, one must dissolve the sample in Duco cement thinned with acetone. It can then be painted directly on to a thin surface like a sheet of Kapton or Mylar. This does not, however, relieve one of the necessities of making the particles small first, and one still has to be careful that the procedure does not cause any undesired chemical reactions.Another way to deal with powder samples is to use a frame of a known depth. A typical razor blade, for instance, is about 300 μm thick away from the cutting edge. Using about one absorption length's worth of sample diluted with enough filler to fill the window is pretty easy, as neither brushing nor pressing is involved. The downside is that the total thickness is set by the frame depth, and so the filler may absorb a lot more than you'd like, particularly at low energies.It is also possible to put powder samples, diluted if appropriate, into capillaries. For transmission, it is important that the front and back of the capillary be flat, as ordinary round capillaries will lead to the kind of distortion Mandelbrant discussed in Box 3.2. You can try using "square capillaries" or "rectangle capillaries" in Web searches to find suppliers. Be sure to calculate how much absorption you'll get, including from the capillary walls, before deciding what dimensions of capillary to purchase.Finally, do your best to put the most even part of the sample in the window and tape the sample to the frame along the edges, being careful not to partially overlap the window with the additional tape. Now you are ready to collect XAFS data.

1. In most cases, pressing into a pellet will involve mixing with a filler; pellets a few microns thick don't tend to hold together very well! Decide on how many absorption lengths of filler and of sample you want; 0.5 of each might be typical. Use the method of EquationPour that amount of the mixture into the pellet chamber. 3. Press the pellet with force for 10 seconds or so. Be careful while removing it from the chamber, as it may easily crumble when handled. Sandwich the pellet in tape to make sure it does not break apart.3.7.1.5 Air-sensitive powders Materials that are even mildly oxygen or water vapor sensitive become more so when ground into a fine powder, as the surface to volume ratio can be dramatically increased relative to a bulk sample. In cases where this is a concern, samples can be prepared in a glove box, glove bag, or anaerobic chamber using one of the methods described earlier. We've talked about five methods for getting particles small enough to make a uniform sample: grinding, sieving, sedimentation, spreading thinly on tape, and choosing the most uniform part of the sample at the beamline. At energies from 5-15 keV, choosing three of these techniques is usually good enough, unless you're doing ultraprecise work.For example, you could choose to grind the sample finely, spread it thinly on tape, and then measure the most uniform part of the sample.Or one could grind finely, sieve, and then measure the most uniform part.Below 5 keV, you should use more than three of the techniques, because one absorption length is very small at those energies. For energies much above 15 keV, particle size is less of an issue, so using just two of these techniques is probably OK.condensation of ice on the sample would be a major issue. The cryostat can therefore be used to maintain a nonreactive atmosphere, even if you plan to measure at room temperature. The quality of the vacuum or gas environment may not be good by the standards of ultrahigh vacuum or a glove box, but is often sufficient to extend the lifetime of air-sensitive samples sealed in Kapton or Mylar for long enough to collect good data. And for many samples, measuring at the lower temperatures available with a cryostat can also dramatically slow down any reactions if they do begin to occur.One size does not fit all-the energy at which you are planning to measure and the sensitivity of your particular material will determine the optimum strategy. So the bottom line with air-sensitive samples is to avoid guesswork! Test out your protection systems in your home laboratory well before you leave for the synchrotron, and calculate the absorption length of the materials you are using. If you do, you will very likely get the data you want. If you don't, then you are rolling the dice with your valuable beam time.

thin Films:
This sample type usually comes mounted on a substrate. Unless the substrate is fairly thin, this means the sample will have to be measured using fluorescence, as is. Depending on the sample, you may have to collect data for quite a bit longer to overcome the unfavorable signal to noise ratio. An energy-discriminating detector might also be useful.

Solid metals:
Electron yield is often ideal for this kind of sample, although you must keep in mind that electron yield will only probe to a depth of a hundred nanometers or so. If there is a thin oxide layer, or if synthesis or preparation results in a different microstructure near the surface, an electron-yield XAFS spectrum may differ significantly from what would be measured in transmission. Electropolishing or ion milling are the preferred methods for removing surface layers if desired, as mechanical abrasion can modify the microstructure of the near-surface regionTransmission is also an option for metals and alloys, but the thickness should be reduced to below three absorption lengths. It is sometimes possible to cut or shave the sample to this thickness. Alternatively, many metals are sufficiently plastic that they can be rolled or pounded into thin sheets. Because this is very likely to introduce defects into the microstructure, it is a good idea to anneal your sample after it has been made into a foil. Regardless of the method you use to produce a suitably thin transmission sample, if you will be measuring edges with markedly different atomic numbers, a separate foil of appropriate thickness can be made for each element that is to be measured.

Preparing Samples 79:
Finally, you could measure in fluorescence, but may then have to guard against self-absorption or try to correct for it after the measurement.

Solutions and liquids:
Solutions and liquids make excellent XAFS samples, because they are generally quite uniform.

Gases:
Gases work great in transmission, because their thickness will only be dependent on the container, and they are by nature uniform mixtures.The only thing you have to be concerned with is choosing a proper container and windows. Make sure you know how your gas interacts with the materials of the holder; even when you are sure it is properly contained, leave it in its container for an extended period of time before bringing it to the synchrotron. You may be surprised by what happens, and it will be a dilemma best dealt with before your scheduled beam time. If your gas starts to deposit solids on your windows, you will soon be suffering from serious inhomogeneity; see the advice in Section 3.7.7.

environmental:
If your sample is soil or some other heterogeneous mixture, fluorescence is usually the method of choice, provided it is dilute and there are no large grains rich in the element being measured. An XRF tub makes a good sample holder, but almost anything will serve. While preparing your sample, be sure to pull out any pebbles, twigs, and other solid objects that might significantly interfere with the beam. Sieving may even be appropriate, depending on the sample. Though uniformity is not the issue that would be in transmission, you still need to get rid of any concentrated regions of the element of interest on a scale comparable to or larger than an absorption length. Keep in mind while arranging your sample that you can mask off the area of the sample you are most interested in to make sure it is hit by the beam. Some samples can be prepared almost as if they were powder samples; in that case, both fluorescence and transmission can be collected at the same time to see which gives you better quality spectra.

In situ and Operando:
For this category of experiment, you will probably find that it is a challenge to achieve ideal thickness and uniformity. These experiments are therefore often measured in fluorescence.Make sure you test as much of your system as possible before your trip to the synchrotron. This will ensure that you can prepare for whatever otherwise unexpected circumstances you will encounter on your run. If you are trying to observe a chemical reaction in action, or an unstable sample, there are a variety of uniformity issues you may run into. For instance, if gases are evolving in solution, bubbles can form. Even in fluorescence, bubbles are disruptive-for one thing, they change the path length from the sample to the detector. In transmission, of course, the distortion they introduce is severe. Similarly, solutions or gases can deposit solids on your windows. Even if a precipitate is what you are interested in measuring, if the solid remains stuck on the window, you will lose the ability to observe the subsequent progress of the reaction.If either bubbles or deposition is a severe problem, it may be necessary to consider an ex situ version of the experiment, in which aliquots are 3.7 Preparing Samples 81 periodically removed from an ongoing reaction and measured after the reaction has been quenched.Finally, for operando measurements don't become so concerned with optimizing the XAFS spectra that you lose sight of the process you are trying to study. If you are investigating the interaction of a powdered catalyst with a flowing gas, for instance, the gas must be able to flow freely through the powder, perhaps necessitating the intentional introduction of air gaps by sieving the sample and only using particles larger than some size. Such choices will necessarily introduce nonuniformity into the sample, but that is preferable to collecting perfect spectra on a material that never undergoes the reaction you wish to study.82 Chapter 3 -Sample Preparation boX 3.7 All iS not loSt I don't think I paid enough attention to all of the things in this chapter when I made these samples, but I'm already at the beam line! I guess I'll just give up and go home… Hold on! A bad sample doesn't have to make or break you. After all, sometimes there is nothing you can do to perfect a sample! Optimizing the beam by proper detuning and by minimizing slit size can make all the difference. So can the quality of your detectors (what combination of gases you have in them, how well they are lined up, and so on). If you read Chapter 5 to learn how to hone your beams and detectors, you can probably salvage your experiment. As long as two out of three are good (beam, detectors, and sample), your results are likely to be useable. Also, the question you are trying to answer will partially determine how much a bad sample will hinder your data. For instance, phase identification is more likely to remain possible in the event of a thick or uneven sample than figuring out coordination numbers. For more detail on this, see Chapter 10.And hey, the best way to handle bad samples is as a learning experience. Have you ever noticed how XAFS experts can take one glance at your spectrum and tell you that your sample is too thick or too thin or too uneven or whatever? They can do that because they've run tons of bad samples, either accidentally or intentionally. If you discover you've got an iffy sample and you're unable to correct it on the fly, you can at least see what the spectrum looks like, and even, if you have time, play around with how that interacts with changes you make to the beam. Yay! All is not lost. Thanks, guys! References 83

WHAt i'Ve leArneD in CHAPter 3, bY SimPliCio:
• For transmission, it is very important that samples be of uniform thickness on the scale of an absorption length. This means that for powders, the grain size must be very small.• To make the particles small, it is usually OK to choose three of the following five methods when working at energies of 5-15 keV: grinding, sieving, sedimentation, spreading thinly on tape, and finding the most uniform part of the sample at the beamline. At lower energies, more care must be taken.• For transmission, samples should be roughly one absorption length thick above the edge we're going to measure.• For transmission, an edge jump of roughly one is ideal, but it can be quite a bit less and still be OK.• Transmission samples that are too thin suffer from poor signal to noise, those that are too thick from harmonics, and those that are uneven from multiple issues.• Fluorescence samples work best when they are thin (compared to an absorption length) and concentrated, or thick and dilute. Thin, dilute samples exhibit poor signal to noise; thick, concentrated samples suffer from self-absorption.• Grazing entrance can help with thin dilute samples and grazing exit with thick concentrated ones.• Many sample problems can be anticipated and corrected for in advance, if we're willing to do the calculations.

Synchrotron Driver's Training:
But we haven't done data collection yet. How can we talk about data reduction when we don't know how to collect it yet?Well, which did you learn to do first: how to drive a car, or how to buy one? Even though you've got to buy a car before you drive it, you really should know how to drive before you buy. It's the same thing with data. If you know something about how it's going to be processed before you get to the beamline, you'll make better decisions when it comes time to collect it.There's plenty of raw data that you can practice with online before you ever step foot in a synchrotron. For instance, check out the links to databases at xafs.org.Chapter 4

PREPROCESSING:
Depending on the data source, there are a few specialized tasks you might do right at the start.

Rebinning:
Some data collection modes produce EXAFS data that are very finely spaced in energy, but contain relatively few counts per energy point. For example, quick-XAFS is collected by slewing the monochromator continuously through the energy range of the scan. As we will see later in this chapter, it is not necessary to have very fine resolution in energy in the EXAFS region (the XANES region can be another story). Therefore, it is sometimes a good idea to rebin the data on to a coarser grid. This improves signal to noise, and thus makes the subsequent steps of data reduction more accurate.

Selecting Channels and Scans:
Often, not all the data you collect is usable. Consider, for example, FigureThe vertical translation between scans is not a concern; we'd expect some channels to record more counts than others, just from geometrical considerations. As long as they're all recording fluorescence from the sample, they should all have the same shape.But there are two channels that do not have the same shape as the other four, and, in fact, do not appear to be proper XAFS spectra. There's the one at the bottom, which looks normal enough in the pre-edge and shows the beginning of an edge, but then doesn't show the rest of the edge and looks extremely noisy. There's also the channel at the top, which looks OK, if a bit noisier than the others, until about 17,400 eV. From there until about 17,630 eV, it deviates sharply from the slow variations shown by the other scans, only to return to a more typical pattern at the end.

Chapter 4 - Data Reduction:
Smoothing is a fancier alternative to rebinning.A simple smoothing routine, for instance, could replace each y-value with a three-point moving average, and then repeat enough times to get the balance between signal to noise and resolution you want. But a moving average degrades peaks, so a better smoothing method is a Savitsky-Golay filterThe effect of smoothing is not as clear-cut as rebinning! Many in the field recommend against it!  In this chapter, we'll go through data reduction in prettymuch the order you'll do it in. But that order's not set in stone. For example, should you merge individual scans before or after normalization? With good data, either order is OK, but in other cases, one might work better than the other.Something clearly went wrong with those two aberrant channels during data collection, so those data should be rejected from the analysis. While we've given an example in terms of a multielement detector, similar screening may apply in other cases. For instance, a time-resolved in situ experiment may be plagued by occasional air bubbles which disrupt individual scans. Even a conventional ex situ transmission measurement may suffer from occasional bad scans due, for example, to a failure of the synchrotron beam.

Calculating Unnormalized Absorption:
This is a "specialized task" only in that it depends on how the measurement was performed. For transmission, the absorption coefficient at each energy is proportional to the natural log of the ratio of the I o measurement channel to the I t measurement channel, while for an ideal fluorescence sample (or an electron yield sample) it's proportional to the ratio of the I f measurement channel to the I o measurement channel. Because other factors, such as the thickness of the sample and detector characteristics, are buried in the proportionality constant, it's technically incorrect to claim that these quantities are the absorption. Oddly, there is no standard nomenclature for what quantities such as the natural log of the ratio of the I t measurement channel to the I o measurement channel should be called-although "raw data" is an informal term you will probably hear on the synchrotron floor. In this book, we will call these quantities unnormalized absorption, and symbolize them by M(E).If you measure a reference simultaneously, you'll want to calculate the unnormalized absorption due to the reference at this time as well.

Truncation:
Data may be unusable above or below some energy. This might occur because another edge appears in the data, for instance, or because of a severe set of glitches. In such cases, it's reasonable to truncate the data just inside of the problem region so that it will not interfere with subsequent steps.

Aligning Reference Scans:
Some monochromators don't retain perfect energy calibration over the course of multiple measurements. In some cases, the drift in energy is steady, for example, 0.5 eV per scan. In other cases, the monochromator maintains calibration for sustained stretches, but occasionally jumps by an electron volt or two.

Calibration and Alignment 87:
In casual usage, almost no one distinguishes between the intensity of x-rays at some point and the measurement of that intensity. It's quite possible for the I 1 channel to read higher than the I o channel, if, for instance, the gain is set higher. Novices are often surprised: "How can I 1 be 3.78 volts when I o is 2.34 volts? Shouldn't the sample absorb photons?" Their confusion is understandable, as it comes from our playing fast and loose with the difference between the x-ray intensity and our measurement of it! Personally, my preference is to avoid throwing away data early in the process. One can always perform subsequent steps, such as normalization and background subtraction, in such a way as to exclude problem regions.If you measured the reference data simultaneously with your sample, then you should align the reference scans with each other. Unless the reference data are very noisy, alignment is usually most precise if you look at the derivative spectrum near the edge. It is better to look at more than one peak in the derivative spectrum when you align.Align the scans of data by using the same shifts that were necessary to bring the references into alignment.Occasionally, you may not have a simultaneous or near-simultaneous reference measurement. This may occur, for instance, if a fluorescence sample is too thick to allow any photons to be transmitted to a reference channel, and if a long series of uninterrupted measurements need to be performed on the sample-perhaps it's a battery being cycled or an in situ chemical reaction. In such circumstances, alignment is more difficult to assure. At a minimum, a reference should be measured before and after the sample, so that the total shift can be ascertained. It's also a good idea to take several scans of references and standards before beginning your main experiment, so that you can ascertain the behavior of the monochromator. If there is a steady energy drift, then the measurements before and after the data on the sample are collected and can be used to estimate shifts in between. If the energy is prone to jumps, then, frankly, you have a problem. Discuss the issue with beamline personnel to see if it can be resolved.

merging:
After alignment, scans on a sample measured under identical conditions should be merged. Usually this is simply a matter of averaging the scans together.Occasionally, however, the amount of noise present may differ significantly between the scans. At times, the amount of signal may also differ. For instance, you might measure a fluorescence scan with a Lytle detector, and then decide that the gain on the detector should be increased by a factor of 10, after which you collect two more scans. The scans can still be averaged together, but how should they be weighted for that average? It turns out there is not a simple answer to that question. Suppose, for instance, that one scan has a noise level of a and a signal of c, while a second scan has a noise level of b and a signal of d. Let's consider what happens when we average the two scans together, but with the second scan having a weight of w relative to the first.Noise adds in quadrature, so the noise level of the first scan added to w times the second scan would be 88 Chapter 4 - Data ReductionYou may wonder why we suggest you use the reference to align, rather than the data from the sample. That's because materials used for references tend to be very stable and not susceptible to beam damage. Your sample, on the other hand, may change gradually under the beam, confusing attempts at alignment! This section is written as if it is talking about a series of scans taken on a sample under static conditions. But it also applies to the different channels of data collected by a multielement detector. We want to find the value of the weight w that maximizes that ratio.

Signal just adds, so it becomes:
A little calculus leads to the resultThis result can be illuminated by considering some special cases. First, suppose that all noise is strictly shot noise from measuring the signal. In This weighting implies that the scans should be normalized first (see the following text), and then merged.Finally, consider the case where the noise level is independent of the signal. This might be the case if the signal is dominated by background due to other processes, such as scattering and fluorescence from elements other than the target. In that case, a = b andWhile in theory the proper weighting could be chosen by using Equation 4.4, there is a practical difficulty: how does one know signal and noise for an individual scan?

Calibration and Alignment 89:
Wow, this is tedious! Depending on the kind of noise we've got, we're either supposed to weight scans by their edge jumps, or by one over their edge jumps, or by something in between? In any real situation, "something in between" will probably be the right answer.And you know what's in between one over the edge jump and the edge jump? Weighting 'em all equally! So here's what I do. I just average all the scans with equal weighting. If a scan looks extra noisy, I see if including it makes the average look noisier or not. If it makes it noisier, I leave it out. That's much easier than all this fooling around trying to find the perfect weighting. Maybe I end up with a signal to noise that's not quite the best it could be, but it saves me a lot of time that I could be spending on more important things.Mandlebrant, using suggestions from the IXS, covered noise in Box 4.1. The relative amount of signal present in different scans can be estimated by determining the edge jump for each scan.

Calibrating:
If you have a reference scan, or, better yet, a merged set of reference scans, you can now calibrate. The most common method is to find the first peak of the first derivative, and assign it a tabulated value for that material.There is a problem with this procedure, however. The same material may be assigned slightly different values by different tables. That's because, while monochromators are pretty good at maintaining energy calibration over the range of an EXAFS scan, they do not generally measure larger energy intervals to high accuracy, such as the 30 keV difference from the sulfur K edge to the iodine K edge. In fact, those edges would not generally even be measured at the same beamline! Thus, we should expect tabulated values to be a bit inaccurateexperience shows they sometimes differ from each other by an electron volt or two at any given edge. Of course, the scientific community gradually learns how to do that kind of measurement better, with the result that accuracy should improve over time. This would argue that we should use the most recent table that has been published, perhapsThe problem with using the most recent table, however, is that it makes comparison between an older and a newer paper confusing. If the old paper placed the K edge of cadmium metal at 26,711 eV for calibration purposes and the new one places it at 26,713.3 eV, then we have to mentally shift all the figures from the old paper by 2.3 eV when comparing side by side. 1. Compare the data to a smoothed version. Glitches and sharp features, such as the edge and the white line, should be avoided when making the comparison. The best region to choose would probably be a smooth region well past the edge. 2. Perform a rough data reduction on the scan, and look at the amplitude of the Fourier transform well above the value of R where features are expected to be seen. 3. Estimate the noise on the basis of theory and beamline characteristics; for instance, if the number of counts is small, shot noise is likely to be dominant. 4. Compare an individual scan to the average of all scans.So the majority of XAFS researchers stick with the tables that were current at the time modern XAFS research began, namely those found in the work by

Deadtime Correction:
As described in Chapters 2 and 5, energy discriminating detectors have a limited dynamic range, and as they approach the top of that range, they become nonlinear. This nonlinearity can be corrected for, although the details depend on the detector in use. If using an energy-discriminating detector, ask your beamline scientist about deadtime correction.

Deglitching:
As discussed in Chapter 5, glitches are sharp deviations in the measured absorption that are consistent from scan to scan. Small glitches can be removed in a later step by Fourier filtering, but large glitches can distort procedures such as normalization and background subtraction and, if present, should be removed before those steps.Usually, when we talk about glitches being "small" or "large," we are talking about the maximum amount of deviation they cause from the signal. But more important is the width of the glitch. Even a "small" glitch can be extremely disruptive if it extends over a range of, say, 0.25 Å -1 .If you follow the procedures we describe in Section 5.8.1 of Chapter 5, you will have somewhat oversampled your data in energy space, and thus the loss of one point will not significantly affect your analysis. One method for dealing with a narrow glitch, therefore, is to simply remove that point (or points) from the data. Another method would be to interpolate a value from the surrounding data. Since the data are oversampled, the exact method used is not important.If a glitch is broad, however, there is nothing you can do: the data under the glitch are irrevocably damaged. If the glitch is both broad and also sufficiently deep to dominate your signal, then the game is up: you cannot use that part of the signal. Fortunately, beamline scientists generally know the energies of broad, deep glitches on their beamlines, and avoid scheduling users whose experiments would be severely curtailed.

Choosing E o:
Once you have merged and calibrated your data, you can choose a point to represent the zero energy for the photoelectron.

Finding Normalized Absorption 91:
Wait-are you telling me to intentionally use older, less accurate tables to calibrate my data? That sounds crazy! Absolute energy calibration, my dear Simplicio, is not important in XAFS; what is important is the shifts between spectra. If we all agree to use the same values for our elemental metals, it will facilitate observing those shifts, even with papers by different authors or from different eras.In informal conversation, the width of a glitch is often given in terms of the number of separate points in energy space that it affects; to wit, "a three-point glitch."While the choice of E o is somewhat arbitrary, it is helpful to choose it in a consistent and easily described way. That way, equivalent choices can be made on different spectra, facilitating their comparison.Following are some common ways in which E o is chosen:• At the first peak in the first derivative; in other words, at the first inflection point in the spectrum.• At the largest peak of the first derivative. If you are going to be comparing spectra which are quite different from each other, such as a metal and its oxides, this is not a good choice, as which peak is largest may differ from sample to sample.• At some other well-defined peak of the first derivative.• At the top of the white line.• Halfway up the edge. This choice generally requires some kind of curve to be fit either through the edge or through the pre-and post-edge, so that "halfway up" has a reproducible definition.• At the first peak in the first derivative of the reference spectrum.• At the same energy that was chosen for another related spectrum.• Wherever the algorithm embedded in your analysis software decides to put it. While this is OK for a quick look at the data, we recommend against blindly trusting the analysis software when you are performing a careful analysis.• The "onset of the edge," that is, the energy at which the rising absorption attributable to the edge can first be discerned. We recommend against this choice. Such a definition is strongly dependent on the size of the edge relative to the background, as well as the amount of noise in your measurement.

Normalization:
The unnormalized absorption M(E) has the same shape as μ(E), but can also be dependent on a bewildering variety of other factors, including sample thickness, the solid angle subtended by a fluorescence detector, gains on amplifiers, gases used in the detectors, the use of filters and collimators, and more.92 Chapter 4 - Data Reduction bOx 4.2 "CHOOSE" OR "FIND"? "Choose a point?" That's a strange way of putting it. Don't you mean "find the point"? I mean, the zero energy for the photoelectron is a concrete idea, and I should be able to find the energy that corresponds to it, right? You might think so, Simplicio, but as we'll see in Section 10.1.5 in Chapter 10, the zero energy for the photoelectron is not all that well defined. That makes the definition somewhat arbitrary, and thus leaves it up to us to choose. This is one case where smoothing your data is justified! Since the primary goal in choosing E o is consistency, smoothing out noise is very helpful, even if it distorts the data in some way, because the distortion will at least be consistent! So it is typical, for instance, to use a smoothed derivative spectrum when choosing E o .If one has noisy XANES data, using the same E o for all spectra in a series is a good procedure, as opposed to trying to identify inflection points that are being pulled to and fro by noise.The notation we are using in this section is, as far as we know, unique to this book. Unfortunately, the common usage tends to blur the distinctions between measured quantities like M(E) and the underlying quantities associated with the sample, such as μ(E). For instance, many sources will refer to "measuring" the absorption coefficient μ(E) and determining a "smooth background" μ o (E). But in fact, it is rare to determine the absorption coefficient μ(E) during an EXAFS experiment, especially when measuring thick dilute samples in fluorescence. What is actually measured is μ(E) modified by some scaling factor; that is, what we have here called the unnormalized absorption M(E).As we saw in Section 1.6 in Chapter 1, all forms of XAFS analysis depend on comparing spectra, whether they are the spectra of different samples, the spectra of a sample and empirical standards, or the spectra of a sample and theoretical standards. So we have to somehow divide out all the other factors-that is, we have to normalize the spectra.But how do we know the proper normalization to use? After all, we are likely comparing spectra that are not identical, and in some cases, such as linear combination fitting, we may be comparing spectra that are quite dissimilar in fine structure.To wrap our minds around this issue, we'll consider M(E) to be a convolution of the following effects:• A coarse structure due to absorption by the element of interest, consisting of a featureless pre-edge curve, followed by a sharp edge, and by a slowly varying post-edge curve. We'll refer to this as the intrinsic background and use the symbol B i (E).• A slowly varying background function due to the response of the detectors and the presence in the beam path of elements other than the element of interest. We'll refer to this as the extrinsic background and use the symbol B e (E).• A scaling factor that incorporates the concentration of the element of interest, the thickness of the sample, and some detector effects such as the gain on a Lytle detector. For reasons which will become clear shortly, we'll call this ΔM o (E o ).• The fine structure we are interested in, expressed relative to B i (E), which we will call χ(E).From the above definitions, we can write down the following equation:We'd like to have the normalized version of that equation, though, so that we can quantitatively compare different spectra:looks like when a given element is present in a sample-there should be an edge. Since all the information about the atomic environment is in the fine structure, Figure

Finding Normalized Absorption 93:
Equation 4.8 is fine for transmission. But for fluorescence or electron yield measurements, detector response, which is part of B e (E), is a multiplicative factor, even though Equationis the same for all spectra at that edge, so it provides the key to making a consistent comparison. But there's still a bit of ambiguity in this procedure; whilethe same for all spectra at that edge, we're allowed to pick how it should be scaled. So we make the simplest choice, and decide that the edge jump (the height of the vertical line in the graph) will be 1.Thus, we have a plan for normalizing our data. First, remove the fine structure from our data, leaving us a plot that looks something like Since we have plotted FigureWe used the term edge jump in Section 3.3.5 in Chapter 3 for Δμx, but here we are using it for ΔM o (E o ). In the case of an ideal transmission experiment with identical I o and I t detectors, the two quantities would be the same, but, in general, they are not. If you want to be fastidious, you could refer to Δμx as the absorption edge jump and ΔM o (E o ) as the measured edge jump. But, alas, no one in the field is that meticulous, not even I.If we imagine FigureTo complete the process of putting this spectrum into a standard form, we should place it in a standard position vertically; conventionally, it is shifted so that the bottom of the edge lies at 0. We have to do this because B e (E) will be different for different spectra. In this case, we'll shift the spectrum up by 1.2 units, giving us FigureWe have now successfully normalized our spectrum. Some researchers (and some software) like to also take a stab at subtracting out the slow variation before and after the edge is present in M o (E), as shown in FigureIn this particular case, the normalization procedure was done in an ad hoc way: an edge jump was chosen that seemed about right, followed by a shift that seemed to bring the data up to around zero just below the edge, followed by subtraction of best fit lines above and below the edge. The result is just as good a normalization as any analysis software will give you.There are two problems, though, with this seat-of-the-pants procedure: it is time consuming and subjective.Normalization routines used in analysis software automate the process, and in a perfect world they alleviate both problems. Alas, the world is not perfect. FigureThe result is obviously poor; the edge jump of the "normalized" data is around 0.5, not 1.0. (The flattening routine also failed badly, but that's a less serious concern.)96 Chapter 4 - Data Reduction In this case, this happened because there was another edge in the data, as can be seen in FigureWhile in this case the culprit was an additional edge, many different anomalies can cause automated normalization routines to perform poorly, including, for instance, a large white line in conjunction with a relatively short XANES scan.Allowing software to normalize data in a completely automated fashion does remove subjectivity, but it also reduces accuracy-sometimes, as in this example, to an absurd degree. There is no avoiding some subjective evaluation at some stage of the process; you need to decide for yourself how to strike a balance between accuracy, objectivity, and ease of processing.  You may have noticed that we aren't giving details on things like the algorithm used by ATHENA or how it was tweaked to yield FigureIn any case, you should use the normalization process to estimate the uncertainty in your edge jump. Consider, for instance, FigureIt's not entirely clear what the edge jump should be. Does M o trend up after the white line, like the dashed line shows, or slightly down, like the dotted line? Likewise, there isn't quite enough of the pre-edge to decide how M o behaves there. The result is that we could reasonably argue for an edge jump of anywhere from about 0.15 to about 0.19. If we split the difference and selected an edge jump of 0.17, that would still imply an uncertainty of ± 0.02/0.17, or ± 12%. The uncertainty you determine in this way should be carried through subsequent analysis, a point we'll reinforce in the following chapters.

Self-Absorption Correction:
In Section 3.4 of Chapter 3, we discussed self-absorption, a source of distortion in fluorescence measurements. An examination of EquationAs long as the other elements do not have edges near the energy range you have measured, μ(E)μ*(E) will vary fairly slowly over the range and will not be sensitive to structural information. Thus, if you know the elemental composition of your sample, you can estimate μ(E)μ*(E), and, therefore, correct for self-absorption in your spectrum. Variations on this method, first published by

FINDING χ(k ):
Normalization is enough to allow XANES to be compared, but for EXAFS we want to go further and find χ(k), so that we can think in terms of the description introduced in Section 1.2 of Chapter 1.

background Subtraction:
Equation 4.9 guides us in extracting the EXAFS signal χ(E). Rearranging and using the definition given by EquationThe first thing to realize about background subtraction is that it will necessarily involve fitting some kind of smooth curve to the data. While an approximation to the intrinsic background B i (E) is available in tables, the extrinsic

Finding χ(k) 99:
For XANES analysis, data reduction is complete once we've normalized the spectrum. The rest of this chapter will be devoted to procedures related to EXAFS analysis.background B e (E) certainly is not-it's due to too many disparate experimental effects. So we can't calculate M o (E); we have to extract it from our data.Next, you should realize that there are lots of good software packages that perform background subtraction for you, using a variety of schemes that people have found to be effective. While it's a good idea to read about how the particular package you are using performs its background subtraction in its documentation, the process of background subtraction will almost certainly be semiautomated for you.But semiautomated is not the same thing as automated. Your analysis program will still provide you with methods to influence the background, and, just as we saw with normalization, it will sometimes be necessary to override a program's default choice.For example, consider Figurefor the oxidized gallium arsenide spectrum we normalized earlier in this chapter.It is evident that something has gone terribly awry-the background function doesn't look much like the data at all. This time we've got the opposite problem. The background follows the wiggles in the data too faithfully; it's supposed to only be the "slowly varying" part. But Goldilocks only had three beds to choose from, and we have many. background is only used for EXAFS. Are they different in the EXAFS region? One way to tell is to zoom in on that part of the graph.Figure

So let's jump ahead and use:
At this point we have to review our expectations. Our sample is oxidized gallium arsenide. We do not know the exact structure of our sample-"oxidized" is a bit vague. But we do expect to see gallium-oxygen bonds. Those typically have bond lengths of about 1.7 Å

χ(E):
Now that we have one (or more!) acceptable background functions, we can use EquationFrom there, in theory, we can do a pretty good job of getting χ(E), since B i (E) is supposed to be nearly independent of chemical environment and can thus be tabulated (seeIn practice, however, that is not usually done at this stage. For linear combination analysis or principal component analysis, a factor that affects all spectra in identical ways is irrelevant to the analysis, and can be left out. I wouldn't put it that way, Kitsune. Intrinsic and extrinsic backgrounds are both part of M o , which we have tried to subtract from our data. Any remaining "structure," except for the truncation effects, represents a failure of our background subtraction procedure, and while we might be able to attribute a feature which is hard to remove as having some physical cause, it is risky to discuss physical causes for features we have tried to remove! Whatever, guys. We're not going to analyze that part of the Fourier transform anyway, so I don't particularly care why it looks the way it does.χ(E), except that the oscillations ofthan those in χ(E) when well above the edge.For these reasons, EXAFS users occasionally call"experimentally determined χ(E)" as opposed to the "theoretical χ(E)" implicitly defined in Section 4.3.4. Both are generally shortened to just χ(E) in practice, with context hopefully making it clear which is meant.

Converting from E to k:
This part is easy. Basic physics tells us thatwhere m e is the mass of the electron.

A Second Chance at Self-Absorption Correction:
In Section 4.3.5, we discussed the possibility of correcting for selfabsorption. Because EXAFS is a small variation in total absorption, it is somewhat easier to derive an accurate self-absorption correction for χ(k) than it is for the normalized absorption. One well-known correction of this type is given in Booth and Bridges 2005. A good brief review of selfabsorption correction methods is given by

Weighting χ(k):
χ(k) typically decreases in amplitude at high k. For that reason, the data are usually weighted by k, k 2 , or k 3 for purposes of both presentation and analysis. In other words, rather than χ(k) being used directly, kχ(k), k 2 χ(k), or k 3 χ(k) is used. This is colloquially referred to as using a k-weight of 1, 2, or 3, respectively.There is no single guideline for which k-weight to use in a given circumstance. For a discussion of how the members of our panel decide which k-weight to use, see Box 12.2 in Chapter 12. For a way of using k-weight to help distinguish between scattering species, see Section 10.3 in Chapter 10.

About Fourier Transforms:
As discussed in Section 1.4.2 in Chapter 1, a Fourier transform is a way of picking out periodic frequencies in data. Mathematically, let us preliminarily define the EXAFS Fourier transform with k-weight w as104 Chapter 4 - Data Reduction While your data analysis software will convert from a function of E to a function of k at the click of a button, it's often useful to be able to make a rough estimate in your head. For that purpose, it's good to know that k = 10 Å -1 is roughly 400 eV above the edge. Since energy above the edge is proportional to k 2 , that means 20 Å -1 is about 1600 eV above the edge, 5 Å -1 is about 100 eV above the edge, and so on.The "~" on top of χ ~(R) is there as a reminder that χ ~(R) is a complex function, with a real and an imaginary part. We will often drop this reminder, and use χ(R) and χ ~(R) interchangeably.Readers familiar with Fourier transforms in other contexts will notice the usual definition has been tweaked for ease of use with EXAFS. In particular, the kernel is e i kR 2 rather than e ikR , since 2kD is the argument of the sine function in the motivational derivation we used in Section 1.2 in Chapter 1. Equation

Data Ranges Are Finite:
While we can imagine χ(k) extending over all real k (or at least all positive k), in practice we only have a finite amount of usable data. So we would like to replace EquationCan we make the χ( ) R defined by EquationSo we're going to use a different method for visualizing the effect of evaluating a Fourier transform over a finite interval.Consider, first, what you would think if you saw FigureSince we have a quasi-periodic function over a finite range, a very crude way to extrapolate it out to lower and higher k is to just have it repeat (this is called periodic extension). Figure 4.19, for example, shows the same crude extrapolation applied to χ(k) data for an iron foil.Superficially, it looks like a reasonable extrapolation. It's almost certainly not what actually does in the extrapolated region, but on the other hand, it surely wouldn't suddenly drop to zero, either.How does the expression in Equation106 Chapter 4 - Data Reduction where Δk = k max -k min . The integrals aren't identical, because while we can construct the function so that the k k w χ( ) part repeats, the phase of the kernel,, will, in general, be different for different integrals in the series. In fact, over an infinite sum, that means the integrals will usually cancel out.But there are some exceptions. If 2RΔk is a multiple of 2π, then the phase term will be the same for all integrals, and they will not cancel. In fact, if there were N repeats, the result would be just N times the integral in EquationLet's take the condition that 2RΔk is a multiple of 2π and solve it for R:The fact that the shape of the Fourier transform at those values of R is the same whether we assume χ(k) repeats or goes to zero, suggests that we just don't have enough information to know what happens on a scale finer than that in R. For instance, if we Fourier transform a χ(k) from 3 to 10 Å -1 , it is unreasonable to think we could resolve peaks which were much closer than π -≈ (10 3) 0.4 Å apart. Please note that the y-axis tells us that the data are plotted using a k-weight of 2.What's "much closer than 0.4 Å apart"? There's a degree of arbitrariness in all peak resolution criteria, but many describe the ability of EXAFS to resolve peaks to be half the spacing found by stepping Equationplots of peaks and shoulders we usually see are a great aid to the eye (for instance, it's hard for us to take in the significance of the point just below 5 Å being a bit higher than the point just below 4 Å), but they don't actually contain more physically meaningful information! You may have noticed that for pedagogical reasons we chose the k-range in Figures 4.18 and 4.19 so that the repeating function didn't have a sharp discontinuity. But is that actually useful to analysis as well? As a first attempt at understanding that issue, FigureIt is important to realize that this is not an artifact of using a repeating function, because we already know we would get the same result by assuming the function was zero outside of the selected interval. Rather, it's an artifact of the interval we chose-it doesn't contain a whole number of oscillations of our periodic function.Let's change the interval we use for our iron foil, and see if something similar happens.The mismatch in FigureThis time, it's hard to summarize what happened. The peak around 4.4 Å actually appears to have gotten sharper, since it falls further at 4 and 5 Å. But, there seems to be a small peak around 5.6 Å that wasn't evident before, and the peak around 2.2 Å seems to be roughly the same width. Of course, inspection of the graph is made more difficult by the fact that EquationTo understand the answer to these questions, go back to the EXAFS equation (on the inside front cover) or to the initial discussion in Section 1.2 in Chapter 1. χ(k) is a sum over paths. While each path is quasi-periodic, the periods of the paths in k-space are different. When we take the Fourier transform, we are doing it in part so that we can separate the contributions of different paths.Each path, then, does behave a bit like the sine wave did, giving a narrower Fourier transform when the interval chosen contains a whole number of  its periods. But, if we satisfy this condition for one path, we are unlikely to have satisfied it for another, as different paths have different periods. That's why our eye doesn't particularly object to FigureSo that may be why the peak near 4.4 Å got narrower-we may actually be giving that particular frequency a smoother repeat. But in doing so, other peaks could get broader, and in general it's very difficult to judge whether we came out ahead.The considerable differences between the magnitudes of the two Fourier transforms shown in Figure110 Chapter 4 - Data Reduction  Makers of keyboard instruments such as harpsichords and pianos faced a similar problem hundreds of years ago. The periods of different notes can't be made to line up, just as the periods of different paths can't. Their eventual solution was to distort all notes a bit, so that no musical key is perfect for a given melody, but none is awful-a solution known as equal temperament.The answer is that we should never draw detailed conclusions by looking at a single EXAFS Fourier transform. We are always comparing Fourier transforms, whether it is a sample to a standard, or two samples to each other, or a sample to a theoretical calculation. Since we will process each of those spectra in the same way, we will be all right. It is not so important whether there is "really" a feature at 5.6 Å, so long as spectra corresponding to the same structure all produce it.

Windows:
Although we will process all spectra we wish to compare in the same way, we are still facing a somewhat unsatisfactory state of affairs. What if, for instance, we are comparing the Fourier transform of data from a sample to the Fourier transform of a theoretical calculation, but we have not quite aligned the energies of the two spectra properly. (In Section 10.1.5 of Chapter 10, we'll learn that this is, in fact, a nontrivial problem.) That relatively modest misalignment might cause us to inadvertently select modestly different intervals in χ(k), which in turn could lead to substantial differences in the Fourier transforms! In addition, broadening is at odds with one of the reasons we wanted to take a Fourier transform in the first place, causing paths to "leak" into each other.The solution to this problem is to smoothly reduce all paths to zero, or at least to a small value, at the endpoints of the interval chosen. That will distort all paths a bit, but will avoid sharp discontinuities. It will also avoid having a sharp dependence on the interval chosen.To accomplish this, we multiply (k) by a window function that has the desired properties of being small at the endpoints and being smooth throughout. Two popular window functions for EXAFS are shown in Both functions have adjustable parameters so that they can be made more or less sharply peaked; the versions shown are typical.Figure

zero Padding:
Some computer algorithms for computing fast Fourier transforms require data sets with a number of equally spaced points that is an integral power of 2: 256 points, perhaps, or 512. The "equally spaced" part is no problem for EXAFS data, as we can just rebin and/or interpolate data to fall on an equally spaced grid in k-space (0.05 Å -1 is the spacing typically used). In fact, most EXAFS analysis software does this rebinning automatically when converting from E to k.Getting an integral power of 2 for the number of points is a bit trickier, though. One option would be to use a grid spacing that depends on the size of the range chosen. For instance, the unwindowed data from 3 to 10 Å -1 could be placed on a grid -≈ -10 3 127 0.0551 Å 1 apart, so that there were 128 points in the data set and the data spacing was about 0.05 Å -1 . But that's a bit unwieldy, as then the data would have to be rebinned and interpolated every time the range or the window was changed.Instead, most software packages opt to zero pad the data; that is, to add enough zeroes to the end of the χ(k) data to bring the total number of points to some convenient power of 2.In addition to the computational advantages, zero padding also results in a finer grid in R, in accord with Equation

Choice of Windows:
Windowing is supposed to make us less sensitive to small changes in the range of data we select. Let's see if it works on our zero-padded data. First, FigureThat's an interesting approach, Robert. But it could confuse people looking at your data, say in an article or presentation. If you change the window you're using from data set to data set, then the Fourier transforms of those sets will be more difficult to compare visually. In fact, that's probably the best justification for Simplicio's software having so many choices. If you're trying to compare your data to a figure published in the literature, it's best to use the same window they did, as well as the same k-weight and endpoints.

Fourier Transforms Are Complex:
An examination of EquationNote that the derivative of the phase of the Fourier transform is independent of this initial arbitrary choice, which is another good reason for looking at it! Magnitude and phase do not have the same units (see Section 15.3.2 in Chapter 15 for a discussion of the units of magnitude). An arbitrary scaling factor has, therefore, been applied to the magnitude in FigureThere is another way to represent Fourier transforms, however, rather than plotting the phase and the magnitude, and that is to plot the real part and the imaginary part. The real part is the magnitude multiplied by the cosine of the phase; the imaginary part is the magnitude multiplied by the sine of the phase. Plotting either will provide us with information that is due to both phase and amplitude.  There is clearly some qualitative information that can be extracted from the magnitude (even though it's not a radial distribution function, many of the peaks do correlate to scattering shells, and in later chapters we'll show you some tricks for learning about your sample by manipulating them). Can the phase plot also be used to directly extract qualitative clues about the sample's structure? To my knowledge, little investigation has been done in this area.With a little practice, you can imagine the magnitude as a kind of envelope defining the amplitude of the oscillations in FigureAs for the phase, the relatively flat sections of the derivative are responsible for the sinusoidal behavior of the real part within each magnitude peak. The discontinuities in phase, however, are clearly visible between magnitude peaks; the oscillations in the real part appear to "stutter" there.The imaginary part is qualitatively similar to the real part, but with a phase shift, as seen in FigureThe real part and the imaginary part still provide independent information; one cannot be used to predict the other precisely. But, since the phase only changes rapidly when the magnitude is small, it is possible to know roughly what one will look like given the other. This makes the real part (or the imaginary part) a very useful way of presenting the Fourier 4.5 Finding the Fourier Transform 117 For the rest of this chapter, we're going to continue to use both this NiF 2 example and the iron foil example. The NiF 2 spectrum is a nice example, because it is so complicated, while the iron foil is a nice example because it is relatively simple. Pay attention to the captions so you know which one you're looking at! transform, because it encodes most of the magnitude information and most of the phase information in one plot.By the way, as long as we're talking about interesting ways to present Fourier transform data, here's one borrowed from infrared spectroscopy.Compute the second derivative of the magnitude of the Fourier transform, and then multiply by a negative scaling factor. This process makes peaks appear sharper. It is important to realize that it doesn't actually improve resolution or add information; it's a visual trick.But the same can be said of most of the processing we've explored in this chapter. In this case, it clearly demonstrates that the peak between 3 and 4 Å is not symmetric, but rather has some characteristics of a poorly resolved double peak. This property is very difficult to see by inspection of the original transform!118 Chapter 4 - Data Reduction

"Corrected" Fourier Transforms:
Since the absorbing atom is known, it's possible to mathematically remove the part of δ(k) that is due to the central atom (see Section 10.3 of Chapter 10). If there is only a single scattering species, and it is known, then all of the effect of δ(k) can be removed. Then let us agree. We will not use corrected Fourier transforms to present our results.

back-Transforms:
By definition, the inverse Fourier transform of the Fourier transform of a function results in the original function, if both are performed from -∞ to ∞. If we use EquationAn inverse transform per se, though, is not very useful in EXAFS analysis-since we get χ(R) from χ(k), there is little value in perfectly reversing the process.It turns out, however, that reversals that are modified in some way, so that they do not reproduce the original χ(k) with complete fidelity, can be useful. The resulting modified χ(k) is called a back-transform of the data, and is symbolized in this text as ′ χ ( ) k .One way to motivate the idea of a back-transform is to go back to the way we developed the EXAFS equation in Section 1.2 of Chapter 1. On the basis of the way we developed the equation, χ(k) can be thought of aswhere the sum is over scattering paths (both single and multiple), A(k, R) incorporates all the amplitude effects, and Φ(k, R) incorporates all the phase effects. But phase and amplitude are difficult to pin down for the real-valued function shown in Equationχ(k) is still real-valued, but we're now thinking of it as a complex function that happens to have zero for its imaginary part; a conceptual shift laying the groundwork for what is to follow. (This is a common convention, but not the only one; see Robert's comment at the start of Section 4.5.6.)Next, take a Fourier transform of the function in the usual way. The first term will yield nonzero values for positive R, while the second will yield nonzero values for negative R. In fact, sinceNow for the key step: throw away the values for negative R before taking the back-transform. Neglecting the practical effects discussed in 120 Chapter 4 - Data ReductionThe argument in this section is loosely based on Sec. 6.8 in the work byOne will sometimes encounter χ(q) as the symbol for the backtransform. While fairly common, this notation does not emphasize that ′ χ ( ) k and χ(k) can and should be compared on the same graph.Sections 4.5.2 through 4.5.4, we'll recover only the first term, and thus we have a function that is now complex, with a magnitude of∑ and a phase of -+ ∑ π 2 Φ( , ) k R . Since it's easy to extract the magnitude and phase of a complex function, we now have a way of getting ΣA(k, R) and ΣΦ(k, R) from χ(k). By using the EXAFS equation, we even have an approach that can be used to estimate f(k) and δ(k) from experimental data. Before the development of theoretical methods such as FEFF, this approach was used to extract f(k) and δ(k) from known, "model" compounds for analysis of unknowns. Many clever variations on this approach were developed-seeThe real part of the back-transform (again, ignoring the effects discussed in Sections 4.5.2 through 4.5.4 for the moment) is given by1 2To cancel out the factor of 1/2, ′ χ ( ) k is usually defined with an extra factor of 2 out front:Ignoring the effects discussed in Sections 4.5.2 through 4.5.4, if R min = 0, and , thenχ χ , which is thought to be a pleasing way of arranging things.More important for our purposes is what happens if we choose R min and R max so as to select just part of the Fourier transform. For instance, slowly varying parts of χ(k) aren't EXAFS signal; they're part of the background M o (E). In fact, many background subtraction algorithms essentially use this strategy to determine M o (E). And it's not just low-frequency oscillations that you might want to filter out. Because of effects discussed in Chapters 1 and 10, the EXAFS signal decreases at large R, so rapid oscillations are probably noise rather than signal. Finally, you might want to focus on scattering paths of only a certain length; for instance, you might want to know what the signal due to just the near-neighbors looks like. The 1.2-4.0 Å plot is perhaps the most interesting. It clearly corresponds to two magnitude peaks in the forward transform, but the effect of the second frequency on ′ χ ( ) k is somewhat subtle. While the information content is the same, it is easier for our brains to take in the information in FigureSo why use ′ χ ( ) k in this way at all? Because it can be quite helpful for understanding how features in the raw data relate to the forward transform χ(R). In FigureSimilarly, if you are comparing two spectra (for example, data you have collected with a calculated model spectrum), ′ χ ( ) k allows differences to be studied as a function of k. Are the differences only at very low k? Perhaps comparable backgrounds were not used in the same cases. Only at very high k? Perhaps signal to noise at high k is an issue. Somewhat throughout the spectrum, but more severely above ′ χ ( ) k ? Perhaps the nearest-neighbor light elements match, but more distant shells of metal atoms are different (see Section 10.3 in Chapter 10). And so on … backtransforms can be quite useful as diagnostic tools! 122 Chapter 4 - Data Reduction Now, at last, we can see why collecting or binning χ(k) data at 0.05 Å -1 intervals is oversampling. If we back-transform χ(R) data from 0 to 10 Å, the logic of Section 4.5.2 tells us that we cannot resolve ′ χ ( ) k peaks closer than 1 2 (10-0) 0.16 π ≈ Å -1 apart.Since all the physical information about the system is likely contained within the 0-10 Å range, it must not be necessary to sample χ(k) on a finer grid than that. Therefore, 0.05 Å -1 represents a mild degree of oversampling, and the claim in Section 4.3.2 that a one-point glitch can be removed without disrupting the analysis is verified. References 123

WHAT I'VE lEARNED IN CHAPTER 4, bY SImPlICIO:
• Scans should be aligned in energy before being merged, preferably using a reference.• There are many valid ways to pick an E o for a spectrum. The key is to be consistent and to be able to easily describe how the choice was made.• Normalizing a spectrum means dividing by its edge jump and shifting it so that the edge rises from zero to one (ignoring the fine structure). It is important to estimate the uncertainty in normalization, and to use a consistent method on spectra that are going to be compared.• It may be possible to partially correct for self-absorption mathematically, either before finding χ(k)or after, but it is better to collect data that doesn't show much self-absorption in the first place.• To find χ(k), an appropriate background function must first be found. The background function should not vary so slowly that it can't follow the general trend of the spectrum, nor so rapidly that it follows the EXAFS wiggles. More than one background function may appear acceptable; the effect of different choices of background functions can be compared at later stages in the process.• 100 eV is roughly 5 Å -1 , 400 eV is roughly 10 Å -1 , 1600 eV is roughly 20 Å -1 , and so on.• Taking the Fourier transform of a finite function introduces truncation effects. Windows can be used to reduce the impact of those effects on analysis. The precise choice of window is not all that important, as long as the choice is consistent for data being compared.• Fourier transforms can be broken into a magnitude and a phase, or a real part and an imaginary part. The real part and the imaginary part contain similar information.• While "correcting" a Fourier transform for phase shifts is scientifically legitimate, it probably creates as many problems as it solves, and all of my friends recommend against it.• Back-transforms turn the real-valued χ(k) into a complex ′ χ ( ) k . They are also useful for seeing how different ranges of R show up as features in χ(k).

Finding Truth and Beauty by Closing Time:
There's nothing I'd like better than coming home from my beamtime with a beautiful spectrum! When one places beauty over all else, Simplicio, that is what one gets.If you prioritize beauty over truth, you might end up getting lied to.And if you spend all your time chasing after beauty, you may wind up with very little to show for it! Um, we are talking about spectra, right?Whether or not our friends were referring to the trade-offs inherent in collecting XAFS data, those trade-offs are crucial to understand. Let us keep them in mind while we read this chapter.

NOISE, DISTORTION, AND TIME:
Researchers often describe a spectrum as "beautiful" when it is free of evident noise and the features appear sharp. But as our panel just discussed, a beautiful spectrum is not necessarily an accurate one. In fact, conditions that minimize noise often introduce systematic error, distorting the spectrum. As an example, consider the "thin" and "thick" spectra from Box 3.4, shown together in FigureThe thin-sample spectrum looks like it has somewhat more noise than the thick-sample one. For example, it shows a couple of little bumps around 8 Å that look as if they might represent random variation, and the peak near 10 Å is particularly irregular.But, as was discussed in Box 3.4, the thick-sample spectrum is significantly compromised by harmonics. Although it is less noisy than the thin-sample spectrum, it is much more distorted. When faced with a choice between the two, the thin-sample spectrum is preferable.When collecting data, there is a natural tendency to continually tweak the collection conditions so as to maximize signal-to-noise ratio, since it is easy to see the improvement. Systematic distortion, on the other hand, is less evident. It is, therefore, crucial to keep sources of systematic error in mind, so that you do not inadvertently distort your data in the pursuit of noise reduction.The third aspect that must be balanced against noise and distortion is time. It is always possible to improve the signal-to-noise ratio by collecting more data; that is inherent in the definition of noise! But is it better to collect beautiful data on two samples or usable data on ten? That depends on the particular study being conducted and the scientific questions being asked.126 Chapter 5 - Data Collection Sure, Robert. But an experienced eye can do pretty well recognizing noise-or at any rate things that are not EXAFS-even in a merged spectrum (see Section 5.9 for examples of features that are neither noise nor signal). And anyway, the point of this section is to warn against optimizing data collection to produce the best looking spectrum.One spectrum "looks" noisier than the other? That is a vague statement. It would be better to examine multiple scans of each sample and look for variation between scans. It would be better yet to quantify that variation, yielding a standard deviation as a function of k.

DETECTOR ChOICE:
There are many special detectors that have been designed or adapted for XAFS, some of which were discussed in Section 2.5 of Chapter 2. If you're using, say, an energy-dispersive geometry or an electron-yield detector, this is likely to have been part of your experimental design, decided on long before arriving at the light source. But if detector choice is not an integral part of your experimental design, and you are using a typical beamline, then you are most likely faced with three choices: transmission using an ion chamber; fluorescence using a Lytle, passivated implanted planar silicon (PIPS), or similar total-yield detector; or fluorescence using an energy-discriminating detector. Even on many specialized lines, such as those dedicated to microprobe or quick-XAFS, you may be faced with a choice of this nature.As a rule of thumb, signal-to-noise ratio generally improves as you go from transmission to total-yield fluorescence to energy-discriminating fluorescence, but the possibilities for systematic error also increase.Transmission measurements using an ion chamber have the advantage that, with a proper choice of gas (see Section 5.5.3), there is often no practical limit on the intensity of x-rays that can be measured. Self-absorption is also not an issue, although inhomogeneous samples and harmonics can be. Of course, some samples are unsuitable for transmission because they are inherently thick or are mounted on a thick substrate. Transmission is a particularly simple experiment.Transmission, however, exhibits poor signal-to-noise ratio when the concentration of the absorbing element is low or when the sample is particularly thin or thick.When using a total-yield fluorescence detector, on the other hand, selfabsorption must be considered (Section 3.4 of Chapter 3) when the concentration of the absorbing element is high. Scattering and fluorescence from elements other than the absorbing element contribute to the background, and this can degrade the signal-to-noise ratio, particularly when the concentration of the absorbing element is very low. Energy-discriminating fluorescence detectors reduce the background from scattering and other elements, yielding the best signal-to-noise ratio for low-concentration samples. But linearity of these detectors can be an issue, particularly when the number of counts is high.

Predicting Signal-to-Noise Ratio in Transmission:
Many researchers treat signal-to-noise ratio as a matter of trial and error, to be done only after arrival at the beamline. Although a certain amount

Detector Choice 127:
of trial and error is often part of the data collection experience, it is helpful to be able to compute an estimated signal-to-noise ratio in advance.The first step in this kind of calculation is to estimate the flux of photons incident on your sample. This depends on many factors that are out of your control once you select a beamline, such as characteristics of the light source, the bending magnet or insertion device used by the beamline, the resolution of the monochromator, and gases and windows in the beam path before it enters the hutch. Other factors may be under your control, such as the positions of slits used to control the size of the beam and, of course, the duration of time that data are collected at each energy (the integration time). Beamline scientists are usually happy to provide you an order-of-magnitude estimate of the flux you can expect for your experiment. For modern light sources with the slits positioned so as to provide the maximum flux consistent with good energy resolution, flux may range from about 10 10 photons per second for some bending magnet lines to perhaps 10 14 photons per second for some undulators.As an example, suppose you are using a wiggler that produces a flux of 10 12 photons per second on the sample. If your I o detector collects 10% of that flux, it will measure about 10 11 photons per second. Poisson statistics (Section 3.3.3 of Chapter 3) tells us that the noise in the number of I o counts should be about the square root of the number of photons measured or 3 × 10 5 for a 1-second measurement. The signal-to-noise ratio in I o is impressive: about 300,000 to 1.Now suppose the sample is a pure copper reference foil, 2 absorption lengths thick above the edge. As shown in Table(Under these circumstances, it is doubtful that signal-to-noise ratio will be quite as good as these calculations indicate, as electronic noise will probably dominate over the shot noise we have calculated).What we've done so far, however, is estimate the signal-to-noise ratio in the total signal measured in the detectors. If all we were interested in was an edge jump, that would be good enough. But we're interested in fine structure, so the variations in signal we are looking at are much less than the total signal. For XANES, we're often looking at features about 10% the size of the total signal; for EXAFS, the features may be nearly that large at the start of the EXAFS range, but drop rapidly in amplitude with increasing energy. Although the rate of this drop varies greatly from material to material and also depends on conditions such as temperature, for EXAFS dominated by low-Z scatterers (e.g., oxides), the signal may drop by an order of magnitude every couple of inverse angstroms 128 Chapter 5 - Data CollectionLight sources and beamlines are always trying to upgrade to provide more flux. By the time this book has been out for a few years, expect the typical fluxes to be higher. The maximum flux of around 10 14 photons per second is unlikely to increase, however, since it is limited by considerations such as the heat load on beamline components, including the monochromator.in k-space (see Section 10.3 of Chapter 10). For reasons discussed in Section 10.3 of Chapter 10, our pure copper foil will not see the sharp drop begin until perhaps 7 Å -1 above the edge, but from that point on suffers a similar rate of decline. Although these rules of thumb are very rough, let's use it to try to make a back-of-the-envelope calculation of the value of k at which the signal drops to the level of the noise in the I t detector for a 1-second measurement in our example. Since the total signal is about 10 11 photons, the size of the EXAFS features in the low-k part of the range might amount to differences of about 10 10 photons, giving a signal-to-noise ratio of less than 30,000 to 1. If the size of these features was to drop by an order of magnitude for every 2 Å -1 above 7 Å -1 , it would take about 8 Å -1 more for the signal to drop to a level comparable to the noise; that is, we could still make out the signal up to about 15 Å -1 if we took a single scan with an integration time of 1 second per point. This is comparable to what is actually observed, validating our rough calculation.

Detector Choice 129 BOx 5.1 ThICKNESS EFFECTS ON SIGNAL-TO-NOISE RATIO:
Friends, why are we treating I t as if it were the signal? The measured signal is ln(I o /I t ), and from there we normalize the data before analyzing it. For EXAFS, we also subtract a background and convert to χ(k). Do these steps change this analysis?The first-order Taylor expansion of ln[a/(b - x)] is x + a, a constant, so small changes in I t translate proportionally into small changes in the signal. Normalization and transformation to the unweighted χ(k) leave that proportionality in place. If, say, χ(k) at 13 Å -1 is 1% of its value at 9 Å -1 , then the change in I t relative to the background at 13 Å -1 is also 1% of its value at 9 Å -1 . But, friend Dysnomia, we are not looking only at small changes relative to the background. In the text above, we have calculated the noise on the entire measurement, including the background. Then we compare with the signal, which has been normalized by the edge jump. The edge jump is not small; for an ideal sample, it is likely around 1, as we discussed in Section 3.3.5 of Chapter 3. Let us attempt to work this out in detail.Suppose there is a feature that is of amplitude x in the unweighted, experimentally determined χ(k). I agree that it is also of amplitude x in the experimentally determined χ(E). It corresponds to a difference relative to the background in the normalized spectrum:xNext, suppose that instead of a metal foil, we have a finely ground soil sample with a concentration of copper such that 1% of the absorption just above the copper edge is due to copper and 99% to other elements. (As Section 3.4.6 of Chapter 3 explains, this suggests the sample is considerably less than 1% copper by mass). If the total absorption is still 2 absorption lengths, it means that the size of our signal is reduced by a factor of hundred relative to the foil, without affecting the noise at all. For 1-second scans in the low-k part of the EXAFS region, the signal-to-noise ratio is now perhaps 300 to 1. In addition, unless the copper is present as tiny nodules in the soil, it is likely that the copper EXAFS is dominated by low-Z scatterers, meaning that the signal will start dropping sooner than it did for the metal. If we suppose the signal begins to drop rapidly after about 4 Å -1 (typical for an oxide), then the signal will be comparable to the noise roughly at 8 Å -1 above the edge. At this point, we could say that we have reasonably good XANES data, but any EXAFS analysis would be significantly compromised.If we imagine the concentration of copper being such that it is responsible for only 0.1% of the absorption, even XANES features yield signal-tonoise values of only around 30 to 1, and we can see features only in the very beginning of the EXAFS region.130 Chapter 5 - Data Collection

BOx 5.1 ThICKNESS EFFECTS ON SIGNAL-TO-NOISE RATIO (Continued):
To estimate the signal-to-noise ratio more accurately, we want to know what fraction y of I t (E) the feature represents. To do this, we introduce a smoothly varying background functionAssuming y is small compared to 1, as should be the case for an EXAFS feature, we can use the rules of logarithms to writeRight. For a reasonably normal transmission sample, with an edge jump around one, the fractional effect of a feature on I t is more or less the same as the fractional effect on χ(k).We have been doing very rough, back-of-the-envelope calculations to estimate noise, which could easily be off by a couple of orders of magnitude for other reasons. I don't think we have to worry about your correction much.Not for samples close to the ideal thickness, no. But EquationOne could imagine addressing this problem with longer integration times and maximum scans. But the nature of Poisson statistics means that we need to increase the duration by a factor of 100 for every factor of 10 by which we'd like to improve the signal-to-noise ratio. To measure a transmission EXAFS spectrum of our 0.1% soil sample with a signal-to-noise ratio comparable to the 1-second integration time, copper foil spectrum would require an integration time of about 10 9 seconds per point-that's roughly 30 years! Although increased integration times, or equivalently, multiple scans (see Section 5.8.2) can improve signal-to-noise ratio, the improvement will only be modest and cannot make up for problems such as very low concentration.

Signal-to-Noise Ratio in Fluorescence:
Signal-to-noise ratios are poor in transmission when the absorbing element is dilute, the sample is much thinner than ideal, or the sample is much thicker than ideal. Let us consider those cases one at a time.

Very thin samples:
For very thin samples, EquationFor a thin, concentrated sample, however, the total signal in the fluorescence detector is primarily due to fluorescence from the absorbing element, and thus the signal-to-noise ratio is proportional to the square root of the edge jump, as would be expected from Poisson statistics. While the geometry and physics of fluorescence contributes to significantly lower total counts than transmission (for one thing, the fluorescence detector is usually positioned so as to capture only a modest fraction of the fluorescent photons), the weaker dependence on the edge jump makes fluorescence better for thin samples.For example, suppose a fluorescence detector is capturing 1% as many photons as an I t detector when the edge jump is 1. The signal-to-noise ratio would then be three times worse for fluorescence than transmission. (In addition, this sample would suffer from substantial self-absorption!) But now suppose the concentrated sample was so thin that the edge jump was only 0.01. The signal-to-noise ratio in transmission would have gotten 100 times worse than for the thicker sample, but in fluorescence it would only be 10 times worse than before. Since the fluorescence started three times worse, the fluorescence would now have a signal-to-noise ratio that is about three times better than the transmission. The difference would become even greater for thinner samples.

Very thick samples:
This case is straightforward. In transmission, signal-to-noise ratio degrades because the total number of photons detected in the I t detector decreases. For example, at 9 absorption lengths, transmission has dropped to about 0.01%, or roughly 10,000 times less than for a sample of ideal thickness for transmission. This causes the signal-to-noise ratio to degrade by a factor of 100, in accord with Poisson statistics.In fluorescence, on the other hand, the fluorescence signal plateaus for thick samples and does not drop.

Dilute samples:
At first consideration, it might seem that there should be no special advantage for total-yield fluorescence with dilute samples. After all, if elements other than the one for which XAFS is being sought are doing most of the absorbing, then they should be doing most of the fluorescing as well, right?Not necessarily. It turns out that low-Z atoms are much more likely to fill a core-hole by an Auger process than by fluorescence. Oxygen, for example, fills a K-shell vacancy through fluorescence only 0.8% of the time and silicon 5% of the time, while copper does so 44% of the timeMore importantly, most of the fluorescence from low-Z elements never makes it to the detector. The most prominent fluorescence lines from silicon, for instance, are at about 1700 eV. At that energy, the absorption length of air is less than a centimeter. The combination of gases and windows in the path from the sample to fluorescence detector will filter out almost all of the fluorescence from these low-Z elements.The fact that most absorption by low-Z elements does not result in counts in the fluorescence detector can result in a dramatic improvement in the signal-to-noise ratio. To see how large this effect can be, let's return to the example from the end of Section 5.2.1, in which copper is responsible for only 0.1% of the absorption of a sample. Let us assume the rest of the absorption is from low-Z elements and that no fluorescence from those low-Z elements reaches the fluorescence detector. Let's also assume that geometric and physical factors combine so that only 1% of the absorption by copper results in fluorescence is counted in the detector.With approximately 10 12 photons per second getting absorbed by the sample, about 10 9 of those are due to copper, leading to about 10 7 photons per second being counted in the fluorescence detector. Shot noise in 1 second will be the square root of that value or about 3 × 10 3 photons. If at the start of the EXAFS range the features have an amplitude around 10% of the total signal, the signal-to-noise ratio for those features will be on the order of 10 6 /(3 × 10 3 ) ~ 300 to 1. This is a factor of ten improvement over our estimate for transmission. The relative improvement would be greater for even more dilute samples.132 Chapter 5 - Data CollectionIn practice, signal-tonoise ratio is the least of your worries for a very thick, concentrated sample. In transmission, harmonics will be a problem. In fluorescence, self-absorption is an issue. One possible solution, if you can't make the sample thinner, might be to measure in electron yield.We've referred to fluorescence detectors such as Lytle detectors as "total-yield." But that's only true for photons that penetrate the detector! Materials in the beam path act as a highpass filter; this effect can be intentionally enhanced, as we will discuss in Section 5.6.2. In the same section, we learn that it is also possible to use low-pass filters. By choosing an appropriate combination, we can cause our experimental set-up to have some degree of energy discrimination, even though the detector per se does not.

Energy-Discriminating Fluorescence Detectors:
In Section 5.2.2, we neglected the effect of photons scattered into our fluorescent detector. We also focused on the case where the element of interest is much higher-Z than the bulk of the sample. This is because, in part, these effects can be mitigated by shielding, filters, and geometry (Section 5.6). The calculations in Section 5.2.2 compute best-case scenarios, in which the mitigation is entirely successful.However, as Section 5.6 shows, there is a limit to how well such procedures work with total-yield fluorescence detectors. If a sample is very dilute, or if it includes a substantial amount of elements with atomic number similar to or higher than the absorbing element, then the fluorescent background from scattering and fluorescence from nontarget elements may be the dominant source of noise.This problem can often be alleviated by the use of an energy-discriminating detector (see Section 2.5.4 of Chapter 2), which will only count photons near the energy of the desired fluorescence. This can improve signal-to-noise ratios to levels closer to the results of the kinds of calculations done in Section 5.2.2.This does not mean, however, that the signal-to-noise ratio achieved by energy-discriminating detectors is always better than total-yield fluorescence detectors. That is because energy-discriminating detectors saturate at high flux, and so in some circumstances, the intensity of the x-rays must be lowered (e.g., by narrowing the pre-I o slits).For example, suppose an energy-discriminating multielement detector becomes nonlinear at count rates above ~3 × 10 6 total photons per second (a typical value for a good detector as of this writing). Although the detector can discriminate photons within about 200 eV of the target fluorescence from the background, it does so after detection, so that it is the total number of counts, including the background, that must be kept below 3 × 10 6 per second. If only 1% of the photons counted by the detector are within the chosen energy range, which is not atypical for circumstances in which energy-discriminating detectors are used, we would thus be limited to perhaps 3 × 10 4 photons per second of signal. Poisson statistics tells us that the shot noise is about 200 photons per second. For XANES or low-k EXAFS features with amplitudes about 10% of the total signal, which in this case would be about 3000 photons per second, this implies a signal-to-noise ratio of only about 15 to 1! Even if there was no fluorescent background at all, the signal-to-noise ratio for such a feature would be only about 150 to 1.Notice that these calculations, unlike those for transmission or total-yield fluorescence, do not depend on the flux of the incoming beam-once your detector has reached its maximum counting rate, you wouldn't do any better by upgrading from a bending magnet to a wiggler, for example-you'd just have to reduce the flux of the beam back to the same level anyway!

Detector Choice 133:
Also notice that for the dilute sample described in Section 5.2.2 (0.1% absorption by copper), the flux of fluorescence photons from the absorbing element alone is higher than the maximum count rate of most energydiscriminating detectors.In short, energy-discriminating detectors are useful primarily for extremely dilute samples or ones where the fluorescent background is very high for other reasons, such as the presence of substantial amounts of elements with atomic number comparable to or greater than the absorber.

Plan Your Beamtime:
Before you begin to collect data, take stock of what you hope to accomplish. Decide which of your samples are of high priority and which are lower. Know which edges have to be measured on each. Have an estimate of which ones will take longer, perhaps because they are particularly low concentration. Also keep in mind any that will be particularly challenging, perhaps because they will be measured operando, are at an edge that is rarely used on the beamline, or have a tricky mix of elements or substrates.At that point, you have some choices to make. If you plan to measure multiple edges on some of your samples, will you measure all the edges on one sample before switching to the next or all the samples at one edge before changing energies? That will depend on how difficult it is to align your samples, how crucial it is to know that you're hitting the same spot, and how much of a hassle it is to change between the edges you'd like to measure.Beamtime is often precious, and many experimental runs are performed with little sleep, and/or with some team members who have less experience-everyone has to have a first time to the synchrotron and it's a lot easier if it's with someone who knows the ropes! Try to plan the challenging stuff for when the experienced people are up and alert-and for when beamline staff are around. Three in the morning on the third night of a run is a good time to have your new undergrad measuring powder standards, but it is not such a good time to make your first attempt at setting up your in situ chemical reaction quick-XAFS experiment.On the other hand, you would also like to measure samples you know quite a bit about before you measure the ones that are more unfamiliar, so that you can make sure everything is working properly. Therefore, whatever edge you start on, you will probably start by measuring a reference foil-these are often available to borrow from the beamline or perhaps from a central repository in the synchrotron. (If this is your first time coming to a particular synchrotron, however, ask before you come!) After the reference foil, you might want to measure a standard or two, so that you can again work out any kinks that might be present. You can even do some quick fitting at the beamline, further confirming that everything is going well. At that 134 Chapter 5 - Data Collection point, you can get into the trickier stuff. Of course, on a very short run, you may have to get to your high priority samples more quickly than that.The best laid plans of mice and men, of course, often go off track. Leaving high priority samples to the last few hours of the run is a bad idea: the synchrotron beam may go down, a beamline amplifier might burn out, your sample might turn out to have some surprises in store for you. In addition, information you learn from earlier samples may influence your priorities for later ones-perhaps the sample you thought you were going to use as a baseline has an interesting story of its own to tell, and you decide the other baseline samples you brought along, which you had thought of as low priority, may now be the subject of your next paper.

Get to Know Your Beamline:
Every XAFS beamline is different. Some are optimized for microprobe work, others for quick-XAFS. They may have mirrors to reduce the spot size of the beam, or mirrors to eliminate harmonics, or both. Monochromators come in a variety of designs and use a variety of feedback systems to maintain alignment. Special sample environments may be available and motors may be enabled that allow the sample to be moved and rotated in various ways. Parts of the beam path may be through vacuum, or helium, or just open to the air (inside a hutch from which people are excluded when beam is present, of course!).Figure

Before You Begin 135:
Airplane pilots take a walk around the outside of their plane before taking off. You should take a walk along your beamline before taking data! It's better to get an idea of the layout when you are getting started, rather than having to search for some component at two in the morning when something has gone wrong!

OPTIMIzING ThE BEAM:
Although it is the responsibility of light source and beamline staff to provide you a stable and well-characterized beam, there may be a few aspects of the beam that are under your control. What is the responsibility of users and what is the responsibility of staff varies from facility to facility, beamline to beamline, and by level of experience of the user.Beamline scientists are usually happy to discuss these matters with you, train you for the aspects that you can control, and provide you information about the aspects that you cannot. Because every beamline is different, their advice should supersede anything we say in this section.

Aligning the Beam:
The beam leaving the monochromator is not uniform. At angles above and below its center, it will drop off in intensity at angles above and below its center, the energy of the fundamental will change as dictated by Bragg's Law, and the energy content (e.g., the relative amount of harmonics) will change as well.In addition, depending on the design of the beamline optics, the position of the beam may change gradually with energy to the extent that its center may be a few millimeters higher or lower at, for example, the silver K edge (25.5 keV) than at the iron K edge (7.1 keV).For these reasons, it is a good idea to align the beam at the beginning of any experimental run and also when moving from one edge to another at a significantly different energy. On some beamlines, the beamline scientist may also recommend that you realign after every refill of the storage ring.Typically, beam alignment will involve opening up any pre-I o slits very wide. On some beamlines, you will have control over a pre-mono slit as well; if so, it could be adjusted up or down to maximize the signal in I o . On the other hand, moving the pre-mono slit will change the heat load on the monochromator, which may take a while to re-equilibrate in response. Whether or not adjusting the pre-mono slit is a good idea is another of those things the beamline scientist will know!The alignment itself often involves moving the table on which the detectors are located up and down, although it's also possible it will be done by moving the monochromator crystals. In either case, effect on I o is monitored, and the beam position is chosen to be in the middle of the broad maximum for I o .

Choosing Pre-I o Vertical Slit Width:
The beam emerging from the monochromator will have an energy that varies vertically. Therefore, narrowing that beam will allow through a 136 Chapter 5 - Data CollectionYou may be wondering what constitutes a "significantly different" energy. That will depend on the beamline and is something that your beamline scientist will know. Some lines may need to be realigned even for a move of a couple of keV, while some may be stable across their whole energy range.smaller range of energies. It will also, of course, cut down the flux incident on the sample.It is worth presenting a rough argument as to how the energy resolution of the beam should depend on the vertical width of the slit. Although any actual case will be more complicated, depending on the design of a particular beamline's optics, it will allow us to get a sense of the dependence.Monochromators can be thought of as taking advantage of the first-order peak given by Bragg's Law:For typical beamline optics, geometries, and energies, the fractional broadening δE E is on the order of 10 -4 when the pre-I o slit is narrowed vertically to 1 mm or so. EquationVertical slit width is not the only factor affecting energy resolution, however.The XAFS process involves the formation of a core-hole with a finite lifetime; eventually, an electron in a higher energy level will fill the core-hole and the atom will either fluoresce or emit an Auger electron.According to the Heisenberg uncertainty principle, this finite lifetime corresponds to an uncertainty in energy, thus limiting the energy resolution.Core-hole lifetime broadening has been tabulatedOver most of the range commonly used for XAFS, the lifetime broadening is, therefore, on the order of a few electron volts. It is also worth noting that for the K and L 3 edges, the fractional energy resolution owing to the core-hole lifetime is only weakly dependent on energy.

Optimizing the Beam 137:
FigureFinally, diffraction from a monochromator has an inherent angular spread known as the Darwin width, after the pioneering work of Charles GaltonIf it weren't for lifetime broadening, and, to a lesser extent, monochromator broadening, we would face a difficult trade-off: the more we narrow vertically the pre-I o slits, the better the energy resolution. This would also result in a lower flux, however, and thus worse signal-to-noise ratio.But the lifetime and monochromator broadenings establish a limit on how good an energy resolution we can achieve, and it usually turns out that we can get close to that limit with only a modest reduction in flux.The vertical width of the pre-I o slit should, therefore, be chosen by beginning with the slit fairly wide open and collecting the XANES spectrum of a standard, such as a metal foil. The slit should then be narrowed and the standard measured again; presumably, the resolution will visibly improve, as seen in FigureThe effects of the lower resolution of the 2.00 mm spectrum in FigureThe most straightforward difference of energy resolution on the 2.00 mm slit spectrum is the suppression of the small shoulder around 6545 eV. The peak near 6557 eV is also suppressed, as can be seen most easily in the derivative plot.138 Chapter 5 - Data CollectionOn some beamlines, the pre-I o slit has little effect on resolution, because upstream optical components already limit the beam to a resolution comparable to that dictated by the monochromator and lifetime broadenings. If that's the case, the pre-I o slit is used to provide sharp definition to the beam, but not to adjust resolution.Since fractional broadening due to vertical slit width increases with energy (for a given width), while fractional broadening due to the core-hole lifetime is roughly energy-independent (at least for K and L 3 edges), the vertical slit width needs to be narrowed for higher energy edges to maintain maximum resolution. Therefore, this procedure should be repeated whenever energy is changed by more than a few keV. Unfortunately, the flux of most beamlines falls off at high energy! This means that for high-energy edges, the trade-off between flux and resolution may be a more difficult choice to make, particularly for dilute samples. Remember that resolution is more important for XANES analyses than for EXAFS, because EXAFS features are broader in energy.Alternatively, if your experiment is not highly flux-sensitive (e.g., your sample is concentrated, smaller than the vertical slit width with no postslit focusing, or being used with an energy-discriminating detector that is saturating), this procedure can be done once at the highest energy you will be using for your experiments. This will assure the best possible energy resolution at all edges at the cost of some flux at the lower energies.

Reducing harmonics:
As described in Chapter 3, harmonics in the beam will distort transmission spectra, especially for thick samples. For spectra measured in fluorescence, harmonics do not cause distortions, but they still degrade signal-to-noise ratio by increasing the fluorescent background. Since Bragg's Law allows harmonics to pass through the monochromator at the same angle as the fundamental, it is necessary to find a way to suppress them.There are two common methods of harmonic suppression: harmonic rejection mirrors and detuning.

Harmonic rejection mirror:
Mirrors designed for x-rays work only at very shallow angles; at steeper angles, nearly all the x-rays are absorbed or transmitted rather than reflected. This effect is energydependent; the higher the energy of the x-rays, the smaller the angle with the surface necessary for efficient reflectionIt must be noted, however, that while the reflection efficiency of x-ray mirrors decreases rapidly with energy above the cut-off, it does not go to zeroIn addition, if the coating of a harmonic rejection mirror includes an element with an edge in the energy range being measured, the XAFS spectrum of the coating will modulate the reflectivity of the mirror, with the result that its spectrum will show up in I o . If the detector chain is not completely linear, then this will prevent accurate measurements of this edge in a sample. In such cases, the harmonic rejection mirror may sometimes be moved out of the beam path and detuning (see below) used instead.140 Chapter 5 - Data Collection Some crystal orientations, such as Si(111) and Si(311), do not pass the second harmonic, making the job of harmonic rejection somewhat easier.

Detuning:
A typical monochromator employs two crystals, although sometimes either one crystal in two sections (a channel cut monochromator) or four crystals are used instead. FigureThe first crystal diffracts x-rays of the desired energy, along with harmonics, on to the second crystal, which then diffracts them back along the original direction, albeit displaced vertically.What if the two crystals are slightly misaligned? Diffraction peaks are not infinitely sharp, so some photons of the desired energy will still travel down the beam pipe, through the slits, and into the detectors and sample. (The intensity of diffracted x-rays as a function of angle around a diffraction peak is called the rocking curve).A detailed calculation of Bragg diffraction using the dynamical theory of x-ray diffraction• The peaks are shifted slightly relative to the angle predicted by the Bragg equation.• The shift is smaller for harmonics than for the fundamental.• The peaks for harmonics are significantly narrower than the peak for the fundamental. This, then, suggests a method of reducing the harmonic content of the beam: intentionally misalign one crystal relative to the other. This is called detuning the monochromator.For the geometry shown in FigureThe reflectivity curves for the fundamental of the two crystals have significant overlap, resulting in a flux roughly half that the crystals would have if perfectly aligned. But the curves for the second harmonics hardly overlap at all. Thus, the harmonics have been suppressed, as desired.The implementation of detuning is highly idiosyncratic, varying from beamline to beamline. It is usually not as simple as just using a small motor to deflect one crystal relative to the other, because the alignment must be maintained and the beam kept at the same vertical position (or the experimental table moved to match changes in the vertical position of the beam) as the energy selected by the monochromator is changed. Frequently some kind of electronic lock-in system is employed, which uses small adjustments to keep the desired detuning as the energy is scanned. It is the job of the beamline scientist to let you know how to maintain detuning on their line. They can also let you know how much detuning is normal for their line-for example, they might advise you to "detune 30%" (meaning misalign the crystals so that the reading in I o drops by 30%). Using the principles you have learned in this book, you can adjust their advice up or down, as appropriate. For example, it mightIt is also important to realize that there is often a limit to how far you can suppress harmonics. Depending on the arrangement of the optics, surface imperfections on the monochromator crystals, and the structure of the beam incident on the monochromator, there may be some reflection of harmonics even far from their diffraction peaks. (See

Testing for harmonics:
144 It is possible to make a rough correction for harmonics in your data using this method-this could be useful if you have a sample that is concentrated and intrinsically thick and if you have detuned to the point that you can't suppress the harmonics any further. But keep in mind that this correction is very rough, as it ignores the energydependent absorption of the harmonics by the aluminum, as well as the presence of harmonics in the I o signal! A somewhat more careful analysis and correction are discussed byDark current from the I t amplifier will produce an effect similar (but not identical-see

ION ChAMBERS:
For transmission, the most common detectors are ion chambers (also known as ionization chambers). The Lytle detectors

Physics of Ion Chambers:
In an ion chamber, some fraction of x-ray photons entering the chamber get absorbed by atoms in the fill gas, ionizing them and forming electronion pairs. Each electron is likely to have nearly the same kinetic energy (less whatever energy it took to ionize it and the recoil energy of the ion) as the original x-ray photon, but a much shorter mean free path, meaning it is likely to undergo a collision with another molecule of gas, ionizing it as well. These collisions will continue in a cascade. In the mean time, the excited ions will relax to their ground states, emitting fluorescent photons and Auger electrons in the process, many of which will cause further ionizations, enhancing the cascade. Eventually, most of the energy of the original x-ray will go into ionizing the gas, although some will show up as heat in the detector or fluorescence out of it. The resulting ions and electrons are pulled out of the detector by an electric field placed across the chamber, typically on the order of 10 4 V/m for a bending magnet beamline. This creates a current, which is then sent to a current amplifier, which converts the current to a voltage. The voltage is then typically passed to a voltage to frequency converter and from there to a scaler, which at last passes the value to a computer. The read-out on the computer is typically scaled to match the voltage output of the current amplifier. In this case, the best linear fit was achieved by assuming that 0.18% of the incident counts were due to harmonics.It's possible to estimate the number of photons corresponding to a given reading on a current amplifier. If we ignore energy lost to heat and fluorescence, then the current I coming out of the detector is given bywhere e is the charge on an electron, N/t is the number of photons absorbed per unit time, E photon is the energy of a photon, and E ionization is the energy of an average ionization event. (The factor of 2 appears because each ionization even results in an ion of charge +1 and an electron of charge -1, both of which contribute to the current.)E ionization is 41 eV for helium, 36 eV for nitrogen, 26 eV for argon, and 24 eV for kryptonWe can, therefore, estimate the typical current output from a detector. Suppose, for instance, that 10 11 photons per second are absorbed in an ion chamber, as in the first example in Section 5.2.1. Suppose the gas is nitrogen and the energy is 6000 eV, which is just above the chromium edge. Equation

Limitations:
Consideration of the physical process described in the previous section tells us that ion chambers cannot respond instantaneously to changes in photon flux-unless they are specially designed for rapid response, they are likely to have response times measured in milliseconds. Although fast enough for traditional XAFS measurements, this may be an issue in some quick-XAFS experiments.In addition, while ion chambers have the ability to handle higher flux than many other kinds of detectors, the ability is not unlimited. Above ~10 11 photons/s/cm 3

Choosing Fill Gasses:
Ordinary air is not generally used for ion chambers, in part because the moist air can itself support a current between the plates comparable to the current we would like to measure. In addition, this current is strongly dependent on small changes in conditions and can thus show large fluctuations and even hysteresis-like effectsIdeally, a gas should be chosen for I o that absorbs enough photons to provide good statistics for the incident beam, but not so many as to substantially impact the statistics in the transmitted (or fluorescent or 146 Chapter 5 - Data CollectionThe different forms of signal in the detector chain can cause some muddiness of nomenclature. It is not unusual to hear someone say "Don't let the current in I o exceed 10 volts," for instance. The final readout on the computer is sometimes referred to as " current," sometimes "voltage," and sometimes "counts," as the signal has been expressed in all three forms during its journey from detector to monitor.Auger) beam-a target of 0.1 absorption lengths is a good rule of thumb. The absorption can be calculated using the same methods as are used for samples in Section 3.3.1 of Chapter 3.For example, consider a measurement at the arsenic K edge (11,867 eV) using a 15-cm long ion chamber to measure I o . If we are measuring EXAFS, we might be most concerned about signal-to-noise ratio well above the edge, so let's calculate the absorption at 12,500 eV. Helium (15 cm) at that energy is only 0.0003 absorption lengths, which is much too little. Nitrogen gives 0.04 absorption lengths, which is in the right ballpark. And argon gives 0.90 absorption lengths, which is a bit more than ideal. Nitrogen, therefore, would be the most appropriate gas for I o in this case.For I t or I f , we would prefer to absorb most of the photons-we will improve our counting statistics, and we don't need them further down the line.Continuing our arsenic K edge example, suppose the I t chamber were 30 cm long (it is not unusual to have the I t chamber longer than the I o chamber, since we want more absorption there). Nitrogen would give 0.08 absorption lengths, which is a bit low for I t -we would be throwing a lot of photons away. Argon, on the other hand, gives 1.8 absorption lengths, which is ideal. So we use nitrogen in I o and argon in I t .

Ion Chambers 147 BOx 5.2 DON'T GET CARRIED AWAY!:
But I do need photons after I t -my beamline has a reference there and then the reference detector! Sure, but the reference is primarily there just so we can check the energy calibration. We don't need a lot of photons for that.There's a more important reason we don't want to absorb too many photons in I t , though. In most detector designs, the collecting plates don't extend all the way to the entrance window-there are guard electrodes there insteadSome beamlines have the ability to mix gasses, so that you could use mostly nitrogen and a little bit of argon for this example. But really, just picking the best of the standard gases usually works just fine; I o is not usually the dominant source of your noise!

Amplifiers:
Current amplifiers generally give users the ability to control their gain. A gain of 1 means that 1 V is output for every 1 A of current input (the gain may also be described by specifying the units; e.g., 200 μA/V). They also have a maximum voltage they can output without distortion, typically 5 or 10 V, depending on the model of amplifier. If the gain is set too high, the amplifiers will saturate, distorting the readings received by the computer.However, overly low gains also present a problem. In the typical detector chain, the output of the amplifier is sent to a voltage to frequency converter, which turns it into a series of pulses with a frequency dependent on the input voltage. The pulses are then counted by a scaler, which sends the result to a computer. Each of those components, along with the cabling connecting them, is subject to various kinds of error: dropped counts, rounding errors, electronic noise, and so on. In a well-functioning system, an EXAFS feature represented by the difference between the amplifier outputting 4.720 and 4.755 V will be much larger than these sources of noise. But the same signal could be given by the difference between 0.004720 and 0.004755 V if the amplifier is set at a lower gain. We are now asking the components in the signal chain to distinguish differences of a few parts in a million of their full dynamic range. This is a much more challenging task, and our spectrum may appear noisy or distorted.Therefore, we should choose the gain on each amplifier to be as high as it can be without risking saturation. While I o is likely to vary only modestly across the course of a spectrum, I t (and I ref , if it is also measured in transmission) are most likely to saturate the amplifiers either just below the edge or at the highest energy of the scan and should be checked there. I f , on the other hand, is most likely to saturate an amplifier just above the edge, particularly if there is a strong white line feature.On some beamlines, you also need to be concerned with dark current, the output of a current amplifier when there is no input signal. Why is there dark current? Some detector chains are not as linear at very low voltages. Also if zero signal was to correspond to zero output voltage, random variations could sometimes result in negative voltages-an eventuality that some detector chains are not designed to handle. So some current amplifiers provide the ability to choose a level of dark current (also known as an offset). Your beamline scientists will know what setting works best for the electronics on your beamline.If there is dark current, however, it is imperative that it be subtracted back off before calculating a spectrum! This is easily done by software on the beamline computers, but to do it, the computer has to know how big the offsets are. Beamline software will typically have a feature that allows the output of the detector chains to be counted for a few seconds when the hutch shutter is closed. The measured offsets are then automatically subtracted from the detector readings displayed by the software.148 Chapter 5 - Data Collection Note that saturating an amplifier is different from saturating a detector, a particular concern with energy-discriminating detectors.There is no point in getting too fussy about gains, as long as you don't saturate the amplifier. With decent electronics, an I t that ranges from 0.1 to 0.7 V over the course of a scan won't be a lot noisier than one that ranges from 1 to 7 V. If it were 0.01-0.07 V, though, I might start to worry.Keep an eye on the detector readouts on the beamline computer each time you close the hutch shutter. If the values differ significantly from zero, you need to measure the offsets again! It can be quite useful to use Equation

SuPPRESSING FLuORESCENT BACKGROuND:
If you are using a Lytle, PIPS, or other total-yield fluorescent detector, the fluorescent background should be suppressed because it contributes significantly-often dominantly-to noise. If you are using an energydiscriminating detector, the fluorescent background contributes significantly to the total counts and can thus force you to reduce the flux to avoid dead-time nonlinearity and saturation; in other words, it may force you to reduce your signal. Either way, suppressing fluorescent background is key to optimizing your signal.FigureNotice that the peaks appear about 500 eV wide at their base. This may surprise you if you've heard that such detectors can achieve energy resolutions of better than 200 eV. There are several reasons for this:• Energy resolutions for detectors are usually given as full width at half maximum (FWHM). That's a useful measure if you are interested in knowing how precisely you can pin down the energy of a peak. But that's not generally the question when using an energy-discriminating detector for XAFS-you know where the fluorescent lines are, but want to be able to choose energies so that one is not "contaminated" by counts belonging to another. For that purpose, it's more important to know the broadening of the peak at the point it rises above the background at its base. This can easily be two or more times the FWHM.• Some energy-discriminating detectors have the ability to tradeoff dead time with energy resolution-the better the energy resolution, the greater the dead time, and the lower the rate at which counts can be collected without distortion. Thus, There are a couple of kinds of artifacts that can appear in fluorescence spectra that you should be aware of! One is pile up peaks, caused by two photons arriving very closely together in time and being counted as one. They show up as small peaks at an energy corresponding to twice the energy of a large peak or the sum of the energies of two large peaks. Another possible artifact is escape peaks, in which a fluorescent photon from the detector substrate (usually silicon or germanium) escapes the detection region, lowering the detected energy by an amount equal to its energy. It is important to realize that in both cases, the recorded events do not correspond to the energy of actual photons! For example, if there is a very large amount of calcium in your sample, there may be a pile up peak at twice the energy of the Kα line of calcium, that is, at 7.4 keV, which is close to the Kα line of nickel. But the pile up calcium peak is actually made of two Kα calcium photons, each with energy 3.7 keV, which can be suppressed accordingly (e.g., by the technique described in Section 5.6.2).energy-discriminating detectors are often not operated at their best energy resolution.• Fluorescent lines usually consist of several closely related transitions that are too close in energy to be resolved, but far enough apart in energy to broaden the peaks. The copper Kα peak, for example, comprises two prominent transitions at 8027 and 8046 eV.The fluorescence spectrum in FigureIt is useful to know the energy at which the Compton peak will appear, relative to the incident x-ray energy. Assuming the scattering is at 90° (see Section 3.4.4 of Chapter 3), the formula isAt 11,967 eV, Equation

Suppressing Scatter Peaks:
To illustrate most clearly the effect of filters and, in the next section, shielding, suppose we are interested in measuring the gold L 3 edge of a sample using fluorescence and that the fluorescence spectrum looks like the one in FigureThe spectrum clearly shows the gold Lα line we're interested in measuring, but there are also a substantial number of counts coming from calcium, iron, and scatter peaks. (There's also a gold Lβ line hidden in the scatter peaks.)To suppress the scatter peaks relative to the gold Lα, we can insert a filter between the sample and the detector. The filter needs to be something that absorbs more at the energy of the scatter peaks than at the energy of the gold Lα line. Since that requires more absorption at higher energy, it means we need something with an edge between the 9.7 keV energy of the gold Lα fluorescence line and the 11.9 keV energy of the gold L 3 absorption edge. Gallium, with an edge at 10.4 keV, is a good choice.150 Chapter 5 - Data Collection Make sure to understand the part written in italics, my friends! The difference between the absorption edge of an element and the fluorescent line resulting from it is fundamental to the idea of a filter.Filters do not need to be pure; they are often partially or entirely oxidized and may include low-Z binders or matrices to provide structural integrity. The thickness of filters is usually specified by giving the number of absorption lengths just above their edge.FigureThree absorption lengths of gallium at its edge is about 23 μm. Twentythree μm of gallium at 12 keV, where the scatter peak is located, is roughly 2 absorption lengths, meaning the scatter peak should be decreased by nearly 90%. In contrast, it is only about half of an absorption length at 9.7 keV, where the gold Lα line is located, meaning that the intensity of that line is only decreased by about 40%. In addition, the usual rise of Many people learn the "Z-1" rule for choosing filters. Under that rule, they just choose a filter with an atomic number one less than the edge that they are measuring. That works pretty well for K edges, but not so well for L edges, so you do need to understand the principles involved.Often, beamlines will have filters available for use with your experiment. If you are not sure, ask! absorption with lower energies means that 23 μm of gallium is about 7 absorption lengths at the calcium Kα line, meaning that 99.9% of that fluorescence is suppressed! While that sounds good-we've gotten rid of most of the scatter and the low-Z fluorescence at the price of less than half of the signal-it may not always represent an improvement. For example, consider a total yield fluorescent detector (Lytle, PIPS, etc.). Suppose that, without a filter, there are 10,000 counts per second in the line we want to measure and 2,000 counts per second in the scatter peaks and low-Z fluorescence. (Such a spectrum would look very different from FigureIn addition, filters have another, undesired, effect. Look at FigureWith energy-discriminating detectors, fluorescence by the filter can be particularly annoying, as it often ends up overlapping the fluorescent line you are trying to measure! One solution to suppressing fluorescence by the filter is to place some kind of collimator between the filter and the detector, so that only photons traveling near a line of sight back to the sample can reach the detector. An example of such a collimating system is Soller slitsWhile filters are a reasonably effective way of suppressing scatter peaks, it can also help to move the detector further away from the sample. As described in Section 3.4.4 of Chapter 3, scattering is at a minimum near 90° from the original beam. Moving the detector back reduces the solid The history of the term "Soller slits" in this context is worth mentioning. Walter Soller (1924) invented a system of parallel slits for collimating x-rays.152 Chapter 5 - Data Collection angle it intercepts and thus limits it to angles closer to 90°. Notice that if a filter is used in addition, the filter should be left near the sample, not moved back with the detector, as this will help reduce the fraction of the filter's fluorescence that is directed at the detector.Moving the detector back is not usually a net improvement for a total-yield detector, since the loss of signal will outweigh the advantage of reducing scatter. But if the number of counts for an energy-discriminating detector needs to be reduced anyway because of dead-time, then backing up the detector is preferable to, for example, reducing the amount of beam incident on the sample.Notice that for a total-yield detector, you don't have a fluorescence spectrum as in FigureSoller slits have to be made of something! Make sure your Soller slits don't themselves add fluorescent background. Frequently Soller slits are made from, or coated with, silver, since the K lines of silver are higher than the most common range for EXAFS measurements, and the L lines are lower. But if you were to try to measure a silver edge using silver Soller slits, you'd have a serious problem: not only would the slits contribute substantially to the fluorescent background, but they would absorb the scatter peaks in a way that was strongly energy-dependent, contaminating your signal! Measurements at the cadmium K edge would also be contaminated; at the cadmium K edge of 26.7 keV, the Compton peak lags 1.3 keV behind the elastic scatter peak and thus would scan across the silver K edge (25.5 keV). Even measuring indium with silver Soller slits is probably not a good idea. In these cases, if the Soller slits are silver, either leave them off or replace them with Soller slits made from another material.One particularly annoying problem with dilute fluorescence samples it that, if care is not taken, your reference foil may fluoresce into the fluorescence detector! Some people choose to remove the reference foil in such cases, but it's usually pretty easy to solve the problem by putting some shielding in the line of sight from the reference foil to the fluorescence detector.limitations on the number of counts per second that they can record that are low compared to the maximum number of counts that can come off of a sample, it's often useful to be able to reduce unwanted sources of counts.

Suppressing Low-Energy Peaks:
Scatter peaks aren't the only source of unwanted counts in FigureUnlike filters, there is very little fluorescence from aluminum foil used in this way, as most of the energy goes into Auger electrons, and any fluorescent photons that do get emitted are absorbed within a short distance (a few centimeters) in the air. A rule of thumb: with a total-yield fluorescence detector, always place the detector as close to your sample as you conveniently can, given the geometries of stages, filters, slits, and so on. With those kinds of detectors, the top priority is to maximize the number of counts for the signal. With energydiscriminating detectors, though, move them away from the rest of the components until the total counts are in a range where the detector is known to behave linearly. If you need to reduce counts anyway, there are added benefits to doing it by increasing the distance of the detector from the sample, filters, slits, and shielding! As with scatter peaks, the suppression of low-Z peaks can also be aided by moving the detector back from the sample, this time because the air will act as a low-Z absorber, much as the aluminum foil does.

Making the Choice:
It is very important to remember that the goal is to increase signal-tonoise ratio, rather than to increase signal to background. The difference is subtle, but important: Poisson statistics tell us that even if there were no fluorescent background at all, there would still be noise given by the square root of the number of signal counts.With that in mind, the number of counts measured in the fluorescence spectra shown in Figures 5.12 through 5.15 are given in TableOften, particularly on beamlines with insertion devices, the flux incident on the sample must be reduced to avoid nonlinearity in an unfiltered energy-discriminating detector. For purposes of this example, we assume that, in the unfiltered case, the incident flux had been reduced by a factor of two. Therefore, in the cases shown in Figures 5.13 through 5.15, we calculate the signal-to-noise ratio for the energy-discriminating detector by scaling up the counts by a factor of two or to the level of the unfiltered case, whichever is less. We also assume that, were a total-yield detector to be used instead, we would also scale the incident flux up by a factor of two.  • Total yield detectors often subtend a greater effective angle than energy-discriminating detectors (Section 2.5 of Chapter 2). For this example, we assume the total yield detector has four times the effective collecting area as the energy-discriminating detector.

Suppressing Fluorescent:
• For the energy-discriminating window, we use 9.5-10 keV. We neglect the contribution of the gallium Kα peak to this region, as in these cases it is small compared to the gold Lα peak.• Using these assumptions and Poisson statistics, the signal-to-noise ratio can be computed for the case of a total yield detector ("Total Yield S/N") and an energy-discriminating detector ("Partial Yield S/N").In the particular case described, the Ga-3 filter would be best for the total-yield case and the aluminum foil would be best for the case of the energy-discriminating detector. In this case, the signal-to-noise ratio is actually a bit better for the total-yield detector than the energy-discriminating one.These results are strongly dependent, however, on the relative numbers of counts in the various peaks of the fluorescent spectrum, on the energies of those peaks, and on the amount by which the incident flux was cut down in the unfiltered energy-discriminating case. It also depends on the thickness of the filter or aluminum foil used. There are no simple "rules of thumb" on the circumstances when filters are helpful, which kinds to use, and to which thicknesses. In practice, many experimenters simply use trial and error at the beamline to see which works best in a given case. As this section shows, however, it is also possible to compute what combination should be best, either from a fluorescence spectrum or, at least roughly, from estimating the composition of your sample (see

ALIGNING ThE SAMPLE:
Of course, it is very important that your beam hit the part of your sample you want it to hit, rather than being partially on a frame or an air gap! On a microprobe beamline, there is an imaging system to help guide you, but on other beamlines, the alignment system might involve any of a number of combinations of video images, motorized stages, phosphorescent cards, registration marks, and "burn paper," which turns color when exposed to x-rays.

Chapter 5 - Data Collection:
This section addresses alignment of a sample for which spatially resolved information is not desired. Microprobe beamlines usually have their own imaging systems, which allow you to choose interesting areas of the sample for measurement. It is often helpful to have your sample masked or framed, so that the wellprepared part of your sample is surrounded by a material that absorbs more x-rays than the sample does. This makes it relatively easy to tell whether your beam is fully on the sample, just by positioning it so as to maximize I t . If a motorized stage is available, as is often the case, you can scan across the sample, producing a plot like FigureA remarkable amount of information can be deduced from a plot of this type. First, you need to orient yourself to what you are seeing. Positions where there is no sample, frame, or sample holder will have the lowest absorption. Frames usually have higher absorption than the sample, but not always, particularly if the sample is rich in high-Z elements. In FigureIn FigureNext, notice that the transitions from air to frame, frame to sample, and sample to frame are not sudden. Each seems to take about 2 mm, and in fact the transition from air to frame is not quite complete before the transition from frame to sample begins. This implies that the beam is about 2 mm wide, and thus, the transitions on the graph correspond to circumstances where the beam is partially on one region and partially on another. We can also conclude that the beam is entirely on the sample from about 7 to 11 mm, which means the sample is 6 mm wide: the 4 mm of travel when the beam is entirely on the sample, plus the 2-mm width of the beam. We also know that the section of frame around the 5-mm mark is not quite 2 mm wide, as the beam cannot quite fit on to it. These figures can all be checked with the dimensions of the actual sample and frame, providing confirmation that the sample is aligned as intended.Finally, we can see that the sample is quite even, with deviations of no more than 0.2 absorption lengths. Consider, however, FigureWe see the typical profile of a sample mounted in a frame. There's a little hitch in the rise up to the frame at around 9 mm; this might indicate a mounting hole or something like that, but it's outside the range we'd think about using for data. There are also thinner spots near both ends of the sample (2 and 8 mm). This sample was prepared by the method of thinly brushing powder on to tape, a method that often leaves the edges of the tape with thin spots or pinholes. Again, we would not usually be inclined to collect data from the very edges of the sample anyway. But there is also a distinct thin spot around 5.5 mm. The drop is more than 0.5 compared to the thicker, even region around 2.5-4.5 mm. Since uneveness is the worst enemy of a transmission sample (see Section 3.3 of Chapter 3), it would be much better to place the sample at a position around 3 or 4 mm on the plot than to risk whatever is causing the nonuniformity around 5.5 mm.Of course, if the sample is irregular on a scale much smaller than the beam, we would not detect that by this method. If irregularities such as thin spots are suspected and you want to test for them, you should narrow the beam to be smaller than the expected scale of irregularities before scanning the position.So far we have discussed aligning a sample measured via transmission. Even if you plan your primary data to come from fluorescence or electron yield measurements, it is often a good idea to align using transmission, 158 Chapter 5 - Data Collection assuming you can get any signal through the sample at all. If your sample is too thick to measure any transmission, you can align using fluorescent measurements, but that's harder to do. Frequently frames and sample holders will fluoresce strongly, so it can be difficult (but crucial!) to tell what is sample and what is something else. It often helps to take a motorized scan without the sample present, to try to understand how the other components look. Or you could fall back on a visual method of alignment, using phosphorescent cards, burn paper, or registration marks to assure that the beam is falling on the sample and only the sample. It is also probably not a good idea to hunt around for a high-fluorescence spot if the sample is supposed to be uniform. If you do find such a spot, it means you have found a region that is not characteristic for your sample and, thus, is not what you want to measure. Perhaps there are even microscale concentrations of your element at that spot that could lead to self-absorption, even if the overall concentration of the element is low. Searching for the highest fluorescence by moving the beam around on the sample also tends to be subject to geometrical considerations; for example, the total fluorescence entering the detector will usually be higher if the beam is on the part of the sample nearest the detector. Moving off-center in that way, however, may have unintended consequences, such as creating an uneven distribution of counts across the elements of a multielement detector or interfering with the function of Soller slits. Most of the time, if you cannot get any transmission data for your sample, it is best to align the sample so that the beam hits it in the center.

SCAN PARAMETERS:
Although the details are different from beamline to beamline, you will be able to choose aspects of how the monochromator steps through energy.There are two main parameters under your control: how big the steps

Scan Parameters 159:
BOx 5.3 WhICh ORDER?Sections 5.4 through 5.7 sort of look like they're written in the order I'd do them at the beamline, but we didn't actually say that. Am I right?As a first approximation, friend Simplicio, yes, the order given here is a reasonable one. But the reality is that when we change one aspect we will often have to readjust others. For example, if, after aligning the sample, we discover that the sample is particularly thin, we may need to adjust the gains on our amplifiers to avoid saturation, which then means we will also need to measure offsets again. Or if the sample were particularly thick, we might decide to detune a little more. Or if your energy-discriminating detector was recording too many counts, you might decide to detune more, or maybe change the slit size. And it's frequently the case that you have to adjust filters, slits, shielding, and detector position after you find the perfect spot on the sample via alignment.True story: our author once spent several hours measuring the iron fluorescence from a screw in his custom-designed flow cell, thinking he was measuring the sample. Oops. For the next run, he switched to nylon screws.Quick-XAFS doesn't work the same way as traditional XAFS. In quick-XAFS, the monochromator is continuously slewed through the energy range, and the data are collected continuously and binned in a subsequent step. Section 5.8, therefore, does not apply directly to quick-XAFS experiments.in energy are between points (step size) and how long data are collected at each point (integration time). Most beamline software will allow you to specify different step sizes and integration times for different regions of the scan and some allow one or both of those to vary smoothly with energy within the defined regions as well.

Scan Regions:
If you're collecting EXAFS data, you'll usually divide your scan into three or four regions (for XANES-only scans, you'll skip No. 3 in the below list):1. Pre-edge. The pre-edge region is crucial for normalizing the spectrum. You need a long energy range to establish the trend, but there's no detail in this region, so you don't need a finely spaced grid or to spend much time per point. Starting 200 eV below the edge is typical. A step size of 5 eV is reasonable, and unless the signal is unusually noisy, an integration time of 1 second is usually sufficient. (Times below 1 second per point are rarely efficient. The monochromator needs a little time to move and react to the lock-in system between each point, and during that time, the beamline software will instruct the computer not to collect data. This time to move and settle is likely to be several hundred milliseconds. Reducing integration time below 1 second, therefore, doesn't usually result in much time savings.) 2. XANES. If you're planning to analyze XANES, this region is more important than if you're just planning to do EXAFS, but even for EXAFS measurements, it is helpful to get good data in this region to aid with alignment. Signal-to-noise ratio will be better in the XANES region than the EXAFS region, but features will also generally be sharper. Although energy resolution may be a few eV (see Section 5.3.2), it's a good idea to oversample a bit (i.e., make the step size a few times smaller than the resolution). Glitches, for instance, are often quite narrow in energy, and oversampling can help reduce their impact.Oversampling also acts in a similar way to collecting multiple scans: measuring four points, spaced a half eV apart, for 1 second each may provide a similar contribution to the data as measuring a single point for 4 seconds in the middle of that range, but observing the variation between the four points can give a much better sense of the noise in the data. Since fractional broadening owing to core-hole lifetime is roughly independent of energy, the broadening as measured in eV increases with energy. For that reason, while 0.5 eV is a typical step size for the K edges of the first row of transition metals, 1.0 eV is reasonable for the K edges of, for example, arsenic and bromine, and 0.2 eV is appropriate for lower energy edges such as sulfur.Integration times depend on the signal-to-noise ratio and on how important XANES is to your analysis. For concentrated samples, 1 second is often fine, while dilute samples may work well with 10 seconds or even more. For purposes of defining scan parameters, you should begin the XANES region a bit below the nominal edge (perhaps 20 eV) and continue it well past the white line. 3. EXAFS. The EXAFS region is most naturally thought of as a function of k. Because E is proportional to the square of k, features will tend to broaden and reduce in amplitude as you get further above the edge. In addition, the signal falls off with increasing energy, further reducing the amplitude of features high above the edge. Ideally, therefore, step size in energy should increase as the scan moves further above the edge, and integration time should increase as well. Many beamlines allow step size in the EXAFS region to be defined in terms of k; if so, a step size of 0.05 Å -1 provides an appropriate amount of oversampling. If not, it is often worthwhile to divide the EXAFS region up into smaller regions and adjust the step size so as to approximate 0.05 Å -1 in k. Since E ≈ 4k 2 when E is in eV and k is in Å -1 (see Dysnomia's comment in Section 4.4 of Chapter 4), a bit of calculus tells us that. For a spacing of 0.05 Å -1 in k, that means a step size of about 0 2 . E . At 100 eV, this corresponds to a step size of about 2 eV, while at 1000 eV, it corresponds to about 6 eV. Integration times can be considered in terms of noise and k-weighting: to keep constant noise in a plot of kχ(k), Poisson statistics require the integration time to be weighted by k 2 ; to keep constant noise in a plot of k 2 χ(k) would require integration times weighted by k 4 ! As with step size, some beamline software allows weighting integration times by a function of k; for those that do not, the EXAFS region can be divided up into smaller regions and the integration times chosen so as to roughly mimic the desired weighting. 4. Post-edge. The region above the XANES region is important not just for EXAFS, but also to help determine normalization. Even if you are only interested in XANES, this means you need to collect data over a broad energy range above the edge-perhaps 400 eV. But if you are not interested in EXAFS, this can be treated as the pre-edge was, perhaps with a step size of 5 eV and integration times of 1 second. And if EXAFS is collected, it is often good to collect a bit beyond the end of your analyzable EXAFS data, to help establish the background function at the top of the range. For this reason, some people add a region about 2 Å -1 in width following the EXAFS region, using a step size of perhaps 5 eV and an integration time of 1 second. This adds very little to the duration of a full EXAFS scan, but can pay off in better determination of the background.

Number of Scans:
To get a good sense of what is noise and what is feature, it is important to conduct more than one scan of each sample under each set of conditions.To a first approximation, doubling the number of scans has the same statistical effect as doubling the integration time, but there are some subtle differences. In terms of total duration, increasing integration time is more efficient, as it does not increase the number of times the monochromator has to move-including the long move from the highest energy to the lowest energy between each scan. On the other hand, there are disadvantages to long integration times:162 Chapter 5 - Data Collection

BOx 5.4 xANES RESOLuTION:
I really must object to the implication that there is no point to making the step size in XANES much finer than the energy resolution! Resolution is not the same as accuracy. Suppose that one is fingerprinting a pre-edge peak. If the resolution of a scan is 1.0 eV, then two peaks 0.1 eV apart could not be resolved, but a peak shift of 0.1 eV might very well still be visible.That peak shift would be implicit in data taken at a wider spacing, Robert. You don't really need to collect the energy on such a fine grid.Perhaps, but as a practical matter, it is much easier to fit a peak that is explicit in the data than to try to reconstruct it from small differences on a broader grid. Doing so puts one at the mercy of interpolation algorithms, which is a dangerous thing.Yeah, and if you like to plot the second derivative, you've got similar algorithmic issues with a grid that's too coarse.I concede the point. But those are only an issue for fingerprinting and visual techniques. Linear combination analysis, PCA, and modeling shouldn't need the finer grid."Visual techniques"? If you include any graphs in your papers, Carvaka, the people reading them will be using "visual techniques" when they look at them! It's easier to understand what's going on in XANES if the features are oversampled. But yeah, it's partly a judgment call and a matter of personal preference. Personally, I'd rather spend a few more minutes per scan to have small shifts in features leap out at me during analysis, but hey, with your data it's your call.• Typical beamline scalers can only count at full voltage for perhaps 10 or 20 seconds before they reach their limit.• XAFS measurements can sometimes have a subtle time dependence-perhaps the gases in the detectors are slowly changing composition, temperature, or pressure, or perhaps the decay of the current in the synchrotron itself has nonlinear effects on the measurement, such as by changing the heat load on the monochromator. These changes are generally slow enough that they show up as subtle differences in the background of different scans or as a shift to higher or lower absorption from scan to scan. Both those effects tend to be removed in the process of data reduction. But if integration times are excessive, these changes can begin to appear on the time scale it takes to measure a single EXAFS oscillation, meaning that they would not be removed by the process of background subtraction.• The sample itself may change over time, either because of damage caused by the beam or because the sample is air sensitive. Shorter integration times allow changes of this kind to be discovered. If beam damage is occurring, the sample can be moved slightly between each scan, so that the beam is always hitting a fresh spot.• If, for some reason, you lose beam during a scan (light sources are not 100% reliable!), the scan you are in the process of collecting will need to be restarted when the beam returns. If you're running long scans, that's more of a loss than if the scans are short.In general, aim for a minimum of 3 scans and a maximum of 15 seconds of integration time. Within those parameters, whether you prefer a greater number of scans with lower integration times or vice versa is largely a matter of personal preference.

Time-Resolved Studies:
There are many situations that call for time-resolved studies but do not necessitate quick-XAFS. Cyclic voltametry, for instance, or operando catalysis studies, often have time scales measured in tens of minutes or even hours. If desired, scan parameters can be adjusted to reduce total scan time and allow for finer time resolution, without sacrificing much energy resolution or signal-to-noise ratio. In such cases, here are some recommendations:• DO trade-off integration time for number of scans.• CONSIDER using a moving average of scans to improve signalto-noise ratio while still maintaining some time resolution. (For example, average each scan with the one before and after it for analysis purposes).• CONSIDER increasing the step size in pre-edge and post-edge regions.• CONSIDER increasing the step size over the XANES region if you are primarily interested in EXAFS.• CONSIDER reducing how high above the edge you collect EXAFS data.

Scan Parameters 163:
Friend Dysnomia, we should demonstrate your assertion by computation. Suppose that the noise is comparable to the signal in some region of χ(k), providing a noisy looking, but visible signal. Increasing the product of integration time and number of scans by a factor of 9 will improve the signal-to-noise ratio by a factor of 3, which is quite helpful. But if there is no sign of a signal, it is likely that the signal-to-noise ratio is on the order of 1:10, at best (remember that we are oversampling, so a single oscillation has many points!). It would take an improvement in the product of integration time with number of scans by a factor of 100 just to bring the signal-to-noise ratio to even.If we are willing to spend a very long time, we can therefore sometimes bring a usable signal out of what appears to be only noise, but even this is unlikely to add more than about 2 Å -1 to the range of data we can use.Poisson statistics tells us that while you can use integration time and multiple scans to clean up a section of χ(k) where a noisy signal is evident, it usually isn't worth it to try to make a signal appear where you can't discern one at all.• CONSIDER increasing the step size over the EXAFS region to approximately 0.10 Å -1 .• DO NOT reduce the size of the pre-edge or post-edge regions.Poorly normalized data are not of much use, even if the timeresolution is good! With these techniques, you can reduce the time for an EXAFS scan on a conventional line to 10 minutes or less and you can manage a XANES scan in as little as 2 minutes.

Making the Most of Your Beamtime:
One of the most important keys to optimizing your use of beamtime is to be willing to change your scan parameters after the first scan or two on a sample. Until you measure your sample, you don't know what the signalto-noise profile looks like. It may be that your first scan extends to 16 Å -1 , but shows no discernible signal above 10 Å -1 -an enormous amount of time could then be saved by truncating future EXAFS scans at 10 Å -1 , plus an additional 2 Å -1 to establish a post-edge.You also might discover that you are spending more than 1 second of integration time on a part of the scan that shows no evident noise-on subsequent scans, that integration time can be reduced in favor of increasing the integration time on noisier portions of the scan.

"WhAT'S ThAT?":
Many features show up in scans that aren't part of the spectrum we're trying to measure. This section will detail some of them.

Noise:
5.9.1.1 Cause Noise may be due to counting statistics in the detectors, electronic noise, or other nonreproducible sources.5.9.1.2 Identification Noise does not appear consistent from scan to scan (FigureIf scan 2 had been the only scan, it would be difficult to judge if the excursions around 5.5 Å -1 were due to noise or some other effect, such as one of the ones described in the remainder of this section. But with four scans, it's clear that they don't recur in the same way from scan to scan-they're due to noise.The small upward spike around 4.2 Å -1 is interesting in that, by chance, it happens to show up in two scans. Once again, though, that is coincidence, and it is due to noise.

Effect on analysis:
Noise increases the uncertainties resulting from analysis.164 Chapter 5 - Data Collection No! The only thing that taking more scans or increasing the integration time does is improve signal-to-noise ratio, and that's not something that needs to be matched for standard and sample! When people say that you should measure your samples the same way as your standards, they mean aspects like energy resolution, including the related concept of step size in the XANES region. But feel free to spend, for example, half an hour measuring a standard and 2 hours measuring a sample, if the sample is dilute! 5.9.1.4 Prevention Noise can't be prevented, but it can be minimized, as has been discussed in the previous sections of this chapter. 5.9.1.5 Mitigation Additional scans and increased integration time will reduce the effect of noise. 5.9.1.6 Silver lining Noise provides one way of estimating ε, the measurement uncertainty.

Monochromator Glitches:
5.9.2.1 Cause At certain orientations, the diffraction peak being utilized by the monochromator can interfere with multiple reflections associated with another set of crystal planes (Van Der Laan and Thole 1988), resulting in a glitch in I o . If the harmonic content of the glitch differs from that at the orientations around the glitch, it may show up in M. More commonly, even if the harmonic content is the same, the glitch, which will travel vertically across the slit as the energy changes, can interact with vertical inhomogeneities in the sample to produce variations in M

Identification:
Figure5.9.2.3 Effect on analysis Sharp, isolated, narrow glitches have little effect on EXAFS analysis, although for large glitches it may be necessary to remove them to avoid distorting normalization and background.Glitches in the XANES region can be more inconvenient, but oversampling can often allow for their benign removal as well. Glitches sometimes occur in rapid succession; such cases can be more detrimental to analysis.

Prevention:
Glitches should divide out of M when samples are uniform and detectors are linear; a well-prepared sample and a wellconditioned beam are your best defense.

Mitigation:
In some cases, changing the amount of detuning may help reduce the effect of individual glitches. Since sample nonuniformity is a complicating factor, moving the beam to a different part of the sample (or repreparing the sample, if feasible) may help. Because glitches travel vertically across the slit, narrowing the vertical size of the pre-I o slit can also help reduce the impact of glitches5.9.2.6 Silver lining Since glitches are sensitive to sample nonuniformity, if glitches in I o are not appearing in M, you can be fairly confident that the sample is uniform on the scale of the beam height. (The sample may still be nonuniform on a smaller scale, however). Glitches can also be used to check energy calibration, particularly in cases where the simultaneous use of a reference spectrum is inconvenient.

Other Edges:
5.9.3.1 Cause An impurity within your sample, or something else in the beam path, including windows, tape, and so on. to show a sharp rise relative to the trend of the background followed by a gradual drop. For small edges, they are often easier to see in χ(k) than in M(E).

Effect on analysis:
Unless you know the chemical structure of the substance causing the edge, you will probably be unable to rely on any data beyond it, even (especially?) for purposes of establishing background and normalization.

Prevention:
For dilute samples, it's a good idea to measure a "blank"-a spectrum with everything in place, including tape (if the sample is to be mounted on tape). This can help you to understand where other edges are coming from and in some cases arrange to reduce their effect.

Mitigation:
If an unwanted edge cannot be removed just by changing beamline geometry (e.g., it is due to a substance in the sample), then the only mitigation is to shorten the scans so that they stop before the edge. If you can't collect data out further than that, at least you can spend more time collecting data at lower energies.

Silver lining:
If the edge appears to be from an element in your sample (i.e., it doesn't show up in the spectrum of a corresponding blank), then you have learned additional information about your sample composition.

Other ExAFS:
5.9.4.1 Cause A large edge below the energy of the scan can be associated with EXAFS oscillations that extend into the scan region.

Identification:
Usually when this is the case you know to expect it as a possibility, because you are aware of the large edge responsible. The What if we're using an energy-discriminating detector? Then we wouldn't be measuring the fluorescence of the new edge.But the element responsible for the new edge would still be absorbing, thus reducing the number of photons available to be absorbed by the element we are trying to measure. In this way, the new edge would affect the intensity of the line we were interested in.degree to which the EXAFS oscillations are present can be determined by looking at the pre-edge region-see FigureThe pre-edge background in FigureModeling later confirmed this

Effect on analysis:
In many cases, only the near-neighbor peak from the lower edge has significant amplitude in the scan region. Because E is proportional to k 2 , these oscillations will be very broad in energy, and when Fourier transformed relative to the higher, measured edge, they will usually be shifted down below the region of the data and thus removed with the background. There are exceptions, however. For example, if the lower edge is an elemental metal or an alloy, the nearest-neighbor peak may have a half-path length of as much as 3 Å. Even if that was shifted down, it might still appear in the range associated with near-neighbor oxygens for the measured edge.

Prevention:
This problem is usually better mitigated than prevented. If prevention were truly desired, the sample could be measured at an elevated temperature. This would increase the MSRD for the oscillations from the lower edge, thus suppressing them more quickly. Of course, it would also affect the measured edge, but if only XANES or the lower part of EXAFS was desired, this might be acceptable.

Mitigation:
In many cases, mitigation is not necessary. But if oscillations from the lower edge are appearing in the range of the Fourier transform of the higher edge that will be used for fitting, it is possible to include the structure of the lower edge in the model168 Chapter 5 - Data Collection 5.9.4.6 Silver lining At least you have the comfort of knowing that the lower edge is providing good EXAFS data! In some cases, it is possible to simultaneously fit both edges, including the effect of the lower one on the higher one.

Multielectron Excitations:
5.9.5.1 Cause An additional electron of definite binding energy on the order of tens or hundreds of electron volts is excited.

Identification:
In some ways, this is similar to another edge, as it will be seen as a sharp jump in the absorption at some energy. Multielectron excitations are generally quite small and are thus best seen in χ(k), as FigureMultielectron excitations can be distinguished from noise (Section 5.9.1), in that they are reproducible from scan to scan; from monochromator glitches (Section 5.9.2), in that there is no corresponding peak in I o ; from Bragg peaks (Section 5.9.6), in that rotating the sample has no effect on them; and from edges (Section 5.9.3), in that there is no plausible edge at that energy.5.9 "What's That?" 169 5.9.5.3 Effect on analysis Once energies are sufficient to excite the second electron to the continuum, there is no longer a single solution for allocating the energy between the two electrons. In effect, amplitude is removed from the single-electron channel from that point in k forward. Thus, including χ(k) values above that point in a fit could be particularly disruptive to the determination of amplitude parameters such as coordination numbers and MSRDs5.9.5.4 Prevention It is not possible to prevent multielectron excitations.5.9.5.5 Mitigation A variety of heuristic approaches to compensate for multielectron excitations have been used. The simplest would be to treat the initial rise as a glitch and allow a background spline to handle the slope change. A slowly varying spline, however, is poorly suited to the sharp slope change associated with multielectron excitations. Thus, a variety of phenomenological functions have been used to model (and subsequently subtract) the shape of multielectron excitations, including those based on arctangentThe software package ATHENA

Bragg Peaks:
5.9.6.1 Cause Crystalline samples can themselves cause diffraction. This diffraction will occur at specific energies.5.9.6.2 Identification A narrow spike will appear in M, but not in I o (see Figure170 Chapter 5 - Data Collection Bragg peaks can be seen in the pre-edge region around 6390 and 6490 eV. Note that they do not appear in I o . The identification was confirmed by rotating the sample slightly, which caused the peaks to shift in energy.5.9.6.3 Effect on analysis As with monochromator glitches, the effect of a single Bragg peak on analysis is modest, and it can be removed by deglitching. Where there's one Bragg peak, however, there are often many, causing the data to be obscured.5.9.6.4 Prevention If crystalline samples are spun with a frequency fast compared to the integration time during measurement, most Bragg peaks will be suppressed. One simple way to make a sample spinner is to remove the blades from a miniature electric fan and mount the sample on its hub.5.9.6.5 Mitigation Changing the orientation of the sample relative to the beam slightly will cause Bragg peaks to move. If there are only a few, you may be able to find a position in which they are at energies that are less troubling.

Monotonic Time-Dependent Effects:
5.9.7.1 Cause Sometimes, the measured absorption M at a given energy will change gradually with time. This may occur, for example, when the gas in an ion chamber has been changed, and it has not yet completely flushed through the detector. Gradual temperature changes can have a similar effect.

Identification:
The trend is often visible as a kink in the background when the step size or integration time changes (i.e., when the scan moves from one region to another). Because there is now more (or less) time spent per electron volt, a constant trend now appears as a differing slope. Figure

Effect on analysis:
If sharp, kinks of this kind can make normalization and background subtraction difficult.5.9.7.4 Prevention Gas changes in detectors should be done well in advance of important measurements. For example, it's usually best to make changes in detector or sample chamber gases before aligning the sample, so that the time it takes to align can allow the gas to finish purging.5.9.7.5 Mitigation Often, the effect is temporary. Simply monitor M at a fixed energy, waiting until it is no longer changing to resume measurements.5.9.7.6 Silver lining While you're waiting, you can get something to eat. That may sound like a joke, but it's easy to forget to eat often enough at the beamline and that can reduce your efficiency as a data collector! 5.9.8 Oscillatory Time-Dependent Effects 5.9.8.1 Cause A variety of problems can create changes in M that oscillate with time. Gas flow rates through a detector that are too high, for example, can cause the pressure to vary in this way. Monochromator feedback can be underdamped. Temperatures, either for the monochromator, the sample, or the detector could also suffer from underdamped control.5.9.8.2 Identification Figure 5.25 shows an example of a large timedependent oscillation, in this case due to variations in pressure of a detector gas

Effect on analysis:
If the oscillation frequency is such that it is close to the rate at which your scan records EXAFS oscillations, even a small oscillatory component can compromise your data. If it is slower than that, however, it can be removed with the background. Faster oscillations can appear as jitter, which looks and acts much like noise but shows correlation from one data point to the next. Jitter can interfere with some methods of estimating the measurement uncertainty of the data (see Section 4.2.2 of Chapter 4).

Chapter 5 - Data Collection:
A similar kind of phenomenon is an oscillation caused by monochromator moves. Likely attributable to either feedback or temperature control of the monochromator, that kind of oscillation can be especially frustrating. Source effects from an undulator can also have this kind of impact. If something like that appears to be happening (e.g., the oscillation does not show up on a time scan, does not depend on integration time, but does show up with a blank), talk to your beamline scientist! 5.9.8.4 Mitigation Time-dependent oscillations are often among the most challenging effects to track down and correct. Try turning down the gas flow through detectors. If that doesn't work, ask your beamline scientist; they'll usually be very interested in this kind of problem. If you still can't make the oscillations go away, choose your scan parameters so that the oscillation is either slower or faster than the rate at which your scan records EXAFS oscillations. 5.9.8.5 Silver lining You're likely to learn quite a bit about the beamline trying to track this kind of thing down! 5.9.9 Electronics Out of Range 5.9.9.1 Cause An amplifier becomes saturated or goes negative; maximum count of a scaler is exceeded; and so on. 5.9.9.2 Identification Sometimes, the beamline software will alert you to problems such as this, perhaps only in the sense that it refuses to collect the data. But sometimes you won't get an alert, and the problem can be seen in the form of a sharp corner in the data, such as in FigureWhile the spectrum in Figure5.9.9.5 Mitigation Once discovered, these problems are easily corrected by changing gains, changing dark currents, measuring offsets, or reducing integration times, as appropriate.

Sample Motion:
5.9.10.1 Cause Sometimes, a sample physically shifts during measurement. A simple example of this is when the sample is not securely attached to the sample holder-it may even fall off altogether! Powder samples sometimes settle during measurement, creating temporary pinholes and then filling them. If the sample is measured at cryogenic temperature and is not completely dry, ice crystals may form and change the density of the sample or trigger settling. Samples measured operando, whether batteries or catalysts, may undergo volume changes. Liquid samples, particularly those that flow, may be subject to the formation and motion of gas or air bubbles. Gas samples may change volume or density.

Identification:
In most cases, M will show sharp discontinuities that are not reproducible in energy. In other cases, when the motion is ongoing, the effect will be more like a non-Gaussian form of noise. FigureThe spectrum in FigureThe explanation was that a bubble had formed in the flow cell, and the motion of the flowing solution was jiggling it in and out of the beam.

Effect on analysis:
Scans with these kinds of discontinuities are not generally usable, unless the scan can be truncated before the discontinuity.5.9.10.4 Prevention Think about possible changes in the sample before mounting it, and if possible, try to allow for them in a way that won't change the part of the sample exposed to the beam. If bubbles have a place to go, for example, they are less likely to overlap with the beam. For powder samples in a well (Section 3.7.1 of Chapter 3), it is often helpful to tap them on a table a few times to encourage the powder to settle before measurement.5.9.10.5 Mitigation Visual inspection of the sample will usually reveal the problem and suggest a method of fixing it: perhaps the sample position can be changed or it can be gently shaken to restore uniformity.

Loss of Beam:
5.9.11.1 Cause The synchrotron may unexpectedly lose beam, the feedback system on the monochromator may lose lock, a safety interlock may be tripped, and so on.5.9.11.2 Identification Beamline software may note the absence of a reading on I o or the spectrum may abruptly flatline. Figure

Effect on analysis:
The part of the spectrum after loss of beam is, of course, not usable. • Signal-to-noise ratio isn't everything. It has to be balanced against distortion and speed.• Signal-to-noise ratio generally improves as you go from transmission to total-yield fluorescence to energy-discriminating fluorescence, but the possibilities for systematic error also increase.• Harmonics can be suppressed by harmonic rejection mirrors or by detuning the monochromator, but can never be completely eliminated.• Fluorescent background can be reduced by a combination of filters, shielding, and slits.• Careful choice of scan parameters can improve signal-to-noise ratio without increasing the total scanning time. PART II

Identification without Physics:
No physics? But how can I understand XAFS without understanding physics?You can't. But you don't have to understand something to use it. Do you understand what causes one person to have a whorl on their finger and another an arch? But you can still use the pattern to identify whether a particular person was at a crime scene.Ugh. This is not going to be my favorite chapter.I don't like "pattern matching" without knowing where the patterns come from.There will be plenty of physics later, friend Carvaka. Fingerprinting of XAFS has been used for nearly a century and is still used today-let us give it its due before moving on to more recently developed techniques.

Matching EMpirical StandardS:
The simplest kind of fingerprinting is comparing the spectrum of a sample to that of a known substance-an empirical standard. Figure182 Chapter 6 -Fingerprinting While the spectra of the sample are clearly not identical to those of the nickel foil, the difference is largely in amplitude. This difference might, in part, be due to issues in sample preparation, as it is difficult to prepare a powder to be as uniform in thickness as a metal foil (see Section 3.3.2 of Chapter 3). In addition, it may suggest a lower average coordination number for the sample, perhaps due to vacancies or a nanoscale structure.Since the sample was prepared by annealing nanoparticles, such differences from bulk nickel are not surprising.Nevertheless, the resemblance to the spectra of the nickel foil is so striking that we can state with confidence that the nickel in the sample is in a form "similar to" ordinary nickel metal. If much of it were oxidized, for example, we would see significant changes in the shape and position of the edge. Significantly different local structures, such as body-centered cubic (ordinary nickel metal is face-centered cubic), can also be ruled out. (Face-centered cubic structures have 12 near neighbors, while bodycentered cubic have 8 near neighbors and 6 a bit further out.) But just glancing at these spectra might not be enough to eliminate other nickel spectra with 12 near neighbors, such as hexagonal close-packed or icosahedral nickel, or close-packed alloys of nickel with iron or zinc. Other evidence would have to be used to distinguish between those choices, either deduced from the spectra using the methods of the remainder of this book or based on other knowledge about the system.

FingErprinting SpEctral FEaturES:
Sometimes, a particular feature within the normalized XANES, χ(k), or Fourier transform can be correlated with particular structural information.6.2 Fingerprinting Spectral Features 183 As an example, we will consider the spinel crystal structure. This structure, commonly adopted by a class of materials called ferrites with molecular formula AB 2 O 4 , is interesting in that the cations A and B may sit in sites that are either tetrahedrally or octahedrally coordinated with oxygen atoms. The spinel structure includes twice as many octahedral sites as tetrahedral ones, so one might be tempted to guess that the A cations sit in tetrahedral sites, while the B cations sit in octahedral sites, but that is often not the case. Depending on the identity of A and B and how the sample was synthesized and processed, some or all of the A atoms may sit in octahedral sites and some of the B atoms may sit in tetrahedral sites.The distribution of cations between tetrahedral and octahedral sites has important ramifications for the magnetic properties of these materialsFigureIt is probably not surprising that the first peak in the Fourier transform, around 1.5 Å, becomes somewhat larger when a greater percentage of the manganese is in octahedral sites. That first peak is largely due to scattering from the oxygen near neighbors, and there are more of them 184 Chapter 6 -Fingerprinting ) Magnitude of the Fourier transform for the manganese K edge of theoretical spectra of manganese ferrite, computed assuming a variety of site occupancies for the manganese atoms. The transforms were taken on data with a k-weight of 1 from 3 to 10 Å -1 , using Hanning windows with sills of 1.0 Å -1 on each side.in octahedral sites than in tetrahedral. But we wouldn't be able to judge the occupancy of a spinel cation just by looking at the Fourier transform of its spectrum-there are too many other factors, such as disorder or Fourier truncation effects, that could affect the size of that first peak. We could try to use modeling (see Chapter 9) to decide based on the first peak, but just a glance wouldn't tell us much.The region between 2.0 and 3.5 Å, on the other hand, shows a difference between tetrahedral and octahedral occupancy that is much more stark: for octahedral occupancy, there is a large peak around 2.6 Å, while for tetrahedral, the peak is around 3.1 Å. Calculations of theoretical standards show that this is true for all first-row transition metal spinels, regardless of the particular cations involved.We thus have a feature that fingerprints site occupancy in a spinel. No modeling is necessary; we can just glance at a Fourier transform (given the right k-weighting and k-range) and know whether the cation is present mainly in tetrahedral sites, mainly in octahedral sites, or in a mixture of both.For example, see FigureIt is evident from comparison with FigureIt is crucial, when fingerprinting using features in the Fourier transform, that the same k-range and weighting are used! This is particularly a risk if you are comparing your data to a figure in a publication!

SEMiquantitativE FingErprinting:
The spinel example suggests that fingerprinting can sometimes be extended to yield semiquantitative information-for example, the distribution of a cation between tetrahedral and octahedral states could be estimated by finding the position of the 2.0-3.5 Å peak when using the k-weight and region used in Figure186 Chapter 6 -Fingerprinting Box 6.1 nothing out oF SoMEthing So I guess the peak in the spinel Fourier transforms around 1.5 Å is due to the first-shell oxygen atoms. That's shorter than the bond length, but I learned in Section 1.4.2 of Chapter 1 that a Fourier transform is not a radial distribution function-the distances are shifted down some. But the difference in where the second peak is for tetrahedral and octahedral must correspond to a difference in cation-cation distance between the two cases, right? You haven't thought that through, Simplicio. Some of the cation-cation scattering for absorbing atoms in tetrahedral sites is from scattering atoms in octahedral sites and vice versa. The distance from a tetrahedral site to an octahedral site must be identical to that from an octahedral site to a tetrahedral site. What peak represents that distance?Why not find out, my friends? Let us use the theoretical standard to show us the contribution of each single-scattering path. FigureWait-the tetrahedral scatterers on their own yield a good size peak around 3.1 Å, but the total doesn't show a peak there! How is that possible? Recall, friend Simplicio, that Fourier transforms are complex valued, and both the real and imaginary parts can be positive or negative. In this case, the contribution from the tetrahedral scatterers must be out of phase with some of the contributions from the other single scatterers, and possibly the multiple-scattering paths that are not explicitly shown in FigureSo it turns out that the Fourier transform of the octahedral absorber doesn't show a peak at 3.1 Å because there are a bunch of paths contributing peaks that end up canceling out? Weird! Particularly in the 1980s and 1990s, many works were published detailing the XANES signatures of common compounds of particular elements, with the aim of facilitating these kinds of analyses.

Example: vanadium xanES:
As an example, we will consider a work byCoordination charge is a concept defined byTo see how this works, consider FigureThe first inflection point in Figure

Fitting Features:
The example in Section 6.3.1 raises an issue: the data are a little bit noisy, particularly when we look at the derivative spectrum. XANES fingerprinting is often used in cases when the data are noisy, since in those cases EXAFS, which is often easier to interpret, might not be available. But if the data are noisy, how can we be sure we identified a particular inflection point, or the top of a peak? In addition, consider the case where we want to analyze changes over a series of spectra, perhaps because we are studying time-resolved data or a temperature series or a series of samples that differ in a synthetic parameter such as doping fraction. In those cases we would like to be able to detect small changes from spectrum to spectrum. One way of addressing both issues is to fit a peak with a simple mathematical line shape, rather than just find its highest point.Individual, isolated peaks, such as the pre-edge feature in vanadium, are often fit by one of the following functions:• Gaussian:A is related to the amplitude of the peak and σ to its width, with E c being the location of the center of the peak.• Lorentzian:A is related to the amplitude of the peak and Γ to its width, with E c again being the location of the center of the peak.• Voigt: The convolution of a Gaussian with a Lorentzian.• Pseudo-Voigt: The sum of a Gaussian and a Lorentzian. E c is the same for both functions, but the amplitudes can be different. This fit yields a peak position of 4.95 ± 0.03 Å, a precision which could be useful for working with a series of spectra. But notice that it does not help us determine the coordination charge using Figure

thEorEtical xanES StandardS:
In recent years, the ability to calculate XANES spectra from structural information has improved significantly. Fingerprinting XANES, therefore, can now be based on theoretical standards as well as empirical ones.

Theoretical XANES Standards 191:
The take-home message here is that using functions to fit peaks makes the process less subjective, allowing you to compare series of spectra, even when the signal-to-noise ratio is not great. But fingerprinting is only a semiquantitative technique anyway, so precision in determining a peak position doesn't correspond to accuracy in determining structural parameters. For example, suppose we think we have a compound that has a vanadium atom coordinated to six oxygen atoms, but that the vanadium may be offcenter. We can use software (in this case FDMNES) to create theoretical standards corresponding to placing the vanadium atoms at various points within the oxygen octahedron. The result is shown in FigureThe forbidden transition 1s → 3d, discussed in Box 6.2, can be clearly seen as a pre-edge peak around -1 eV in the off-center standards. It is evident that the amplitude of the peak correlates to the distance the vanadium atom is displaced from the center and that the effect is nonlinear, with 0.432 Å displacement producing a pre-edge peak more than twice as large as a 0.216 Å displacement.While the direction of displacement doesn't have a significant effect on the size of the pre-edge peak, moving toward a vertex causes a greater suppression of the white line than moving the same distance toward a face and also shifts most features, including the pre-edge peak, to higher energies.These kinds of calculations can be very helpful in understanding the structural factors contributing to differences between spectra. It is still difficult, however, to use theoretical standards in the XANES regions for fully quantitative analyses. Doing so would constitute modeling, not just fingerprinting, and will be discussed further in Chapter 9.192 Chapter 6 -Fingerprinting Box 6.2 thE phYSical BaSiS I'm not a big fan of "fits" like FigureIn papers, I've often seen peaks like that identified with particular transitions, splittings, and the like. For example, the pre-edge in vanadium is the forbidden transition 1s → 3d. The fact that we get a forbidden transition tells us that there is no center of inversionHrumph. I suppose. But I can't shake the feeling that this kind of science is done by first fitting peaks and then justifying them with arguments from quantum chemistry. I suppose you're right, though; if you know about forbidden transitions, you know that the pre-edge in vanadium means there is no inversion symmetry relative to the absorber in this compound.You don't have to be able to identify the physical basis for each peak and shoulder to be able to use them to fingerprint. As long as you analyze standards and samples the same way, you're OK.Consult Chapter 9, friends, for more about the methods and software used to compute theoretical standards.One nice thing about using theoretical standards in this way is that we can isolate structural changes for which it is very difficult to find empirical standards. Figure

Box 6.3 thEorEtical StandardS For ExaFS FingErprinting?:
This section was called "Theoretical XANES Standards." What about fingerprinting using theoretical EXAFS standards? EXAFS theoretical standards are easier to compute and are more easily compared with data. They are, therefore, usually used for modeling, not just for fingerprinting (although we did do a bit of fingerprinting with theoretical EXAFS standards in Section 6.2). We will discuss them in detail in Chapter 9 and Part III.EXAFS is easier to use for modeling not just because the theoretical standards are easier to compute. It turns out that most of the measurement-dependent stuff can be segregated from the EXAFS structural information. For example, we normalize and subtract a background as part of data reduction and that gets rid of a lot of measurement-dependent effects. In analysis, we then use a fudge factor of S o 2 to scoop up a lot of the amplitudedependent effects and another called E o to handle phase-dependent ones. Finally, EXAFS is not strongly dependent on energy resolution … the features are inherently broad. The overall result is that we can do pretty well in comparing experiment to theory without having  194 Chapter 6 -Fingerprinting to worry too much about the details of the experiment. For XANES, though, each of those things is missing: we don't perform a robust background subtraction (although we do still normalize), there aren't simple fudge factors in the theory, and energy resolution is very important. • Entire spectra can be compared, or just individual features.• By identifying correlations between feature position, area, or width and structural parameters, semiquantitative characterizations can be made.• Theoretical XANES standards can help us understand differences between spectra. I always try to be careful, dear Dysnomia.

Linear Combination Analysis:
Well, Robert, you usually work with stuff like 400-year-old priceless paintings, right? You can afford to take your time. Me, I'll cut corners if it lets me get more done, as long as it doesn't louse up my results too much. But linear combination analysis is especially sensitive to corner-cutting, so I'll try to learn from the master of carefulness: lead on, Robert!

A Simple Example:
Imagine a thin film sample that is made up of two layers: the front layer is gypsum (hydrated calcium sulfate) and the back layer cinnabar (mercury sulfide). Let's further suppose that the gypsum layer is half the thickness of the cinnabar layer. For measurements made at the sulfur K edge in the transmission geometry, what would the resulting spectrum look like?According to Bouguer's Law (Equationwhere the g in the subscripts refers to the properties of the gypsum layer.After passing through the cinnabar layer, the transmitted intensity would be given by Here is how we can do the calculation correctly, friends. Gypsum is CaSO 4 •2H 2 O, so it is 18.6% sulfur by weight. The density of gypsum is about 2.32 g/cm 3 , so there is about 2.32 times 0.186 = 0.432 g of sulfur per cubic centimeter of gypsum. Dividing by the

Intimate Mixtures in Transmission:
Next, consider an intimate mixture of two phases-perhaps gypsum and cinnabar again, but now mixed on a scale much smaller than the beam or the thickness of the sample. For instance, nanoscale gypsum inclusions might be present in a cinnabar matrix. In that case, the sample is still uniform in the sense discussed in Section 3.3.2 of Chapter 3, and each section of beam will traverse alternating layers of cinnabar and gypsum. Once again, the total absorption will be the sum of the absorption due to the gypsum and the absorption due to the cinnabar, weighted by the percentage of sulfur atoms present in each phase.

Intimate Mixtures in Fluorescence:
A review of the derivations in Section 3.4 of Chapter 3 reveals that the result from Section 7.1.2 also holds in either the thin or dilute limits for fluorescence, as long as the mixing scale is also much smaller than the penetration depth of the x-ray beam into the sample.

WHEN LCA DOESN'T WORK:
For LCA to be feasible, the total absorption of the sample must be the sum of the contributions of each of its constituents, weighted by the fraction of the absorbing atoms found in each constituent. This was the case in the circumstances listed in Section 7.1, but that doesn't mean it is always true. There are many ways to measure percent composition, including percent by weight, percent by volume, parts per million of a mineral, and so on. X-ray absorption from a given phase is proportional to the percentage of the absorbing atom present in that phase, which does not correspond to any of the measures commonly used in other fields. It is, therefore, usually necessary when comparing XAFS to the results of other techniques either to convert the XAFS percentages to a more familiar measure or to convert the familiar measure to the one that XAFS uses.Just because a sample has a uniform thickness (i.e., has the same thickness in micrometers across its width) does not mean it is uniform in the sense discussed in Section 3.3.2 of Chapter 3! If a 30-μm-thick film is pure copper on the left side of the beam, and pure Cu 2 O on the right side of the beam, the absorption of the left side will differ significantly from the right side, leading to distortions!

Nonuniform Samples in Transmission:
In Section 3.3.2 of Chapter 3, we discussed the problems with measuring nonuniform samples in transmission. The distortions introduced by nonuniform samples will affect LCAs.This kind of distortion only applies to nonuniformity perpendicular to the beam. As Section 7.1.1 shows, it is OK in transmission for the sample to be nonuniform front to back.198 Chapter 7 -Linear Combination Analysis BOx 7.2 FLUORESCENCE FOR "SURFACE" ANALYSIS I'm not sure that I'm comfortable with saying that the mixing scale has to be much smaller than the penetration depth. Some people choose fluorescence for its surface sensitivity. Saying they want surface-sensitive data means their sample is inhomogeneous on a scale larger than the penetration depth, doesn't it?That's a good point, Kitsune. It would be better to say that we don't want the mixing scale to be similar to the penetration depth. If the penetration depth is 4 μm, and you're interested in the composition of a 100 μm thick "surface" layer, there will be no problem. Or, if the penetration depth is 4 μm, and the material consists of 10 nm (= 0.01 μm) nanocrystals embedded at random in a matrix, that's fine too. The problem appears if the penetration depth is 4 μm and there is a surface layer that is itself a few micrometers thick. It is very difficult to interpret the data quantitatively in that kind of situation.But, if there's a surface layer a few micrometers thick, it wouldn't be thin or dilute anyway, right? I learned from Chapter 3 that if I have, for example, 20 μm iron oxide grains in a silica matrix, that an iron-edge spectrum will show self-absorption no matter what the iron to silicon ratio is.I'm glad you learned that lesson, Simplicio, but inclusions like that aren't the only kind of inhomogeneity. Suppose you had clay that included a small amount of Cr 3+ substituted for Al 3+ . If there were a surface layer that was only 10 μm or so thick, you'd still have no self-absorption at the chromium edge, because the sample really is dilute, even on the nanoscale. But, the composition would be changing on roughly the scale of the x-ray penetration, so it would be difficult to evaluate a LCA quantitatively.

Surface Gradients in Thick Fluorescence Samples:
Fluorescence spectra taken from thick samples are weighted more heavily toward the front of the sample. Therefore, LCA ceases to be an accurate quantitative measure if there is a surface gradient. It may still be qualitatively useful, particularly, when used in conjunction with transmission analyses. If, for instance, the fluorescence measurement shows an enhancement of a highly oxidized species relative to what was found in transmission, it may be inferred that the highly oxidized species tends to lie nearer to the surface. Similarly, K-edge fluorescence data sample a deeper region than that in the L edge of the same element, and thus a comparison of the two can yield some qualitative information regarding surface gradients.

Surface Gradients in Electron Yield Experiments:
Electron yield experiments probe an even shallower region than fluorescence, generally reaching less than 100 nm into the material (see Section 3.5 of Chapter 3). Indeed, this surface sensitivity is one of the primary reasons that electron yield measurements are made. The ramifications of this shallow penetration depth should be considered carefully when interpreting LCAs of electron yield measurements.

An Example of LCA 199:
Surface gradients aren't always a concern.Measuring standards in electron yield, for instance, may be a perfectly sensible method of avoiding selfabsorption effects. Estimating how much of each constituent is present by eye helps develop the sense of XAFS spectra that most experts possess. Before reading further, let us guess how much of each mineral is present in the sample. 5% Monteponite? 30%? What do you think?The spectrum of the sample does look something like that of greenockite, and the deviations look as if they could be caused by the addition of a small amount of monteponite. We'll use a computer program to find the percentage of each standard that will match the data as well as possible. The data are almost indistinguishable from the fit, which ended up combining 24% monteponite with 76% greenockite. We can be fairly confident that the sample actually is a mixture of monteponite and greenockite.

BOx 7.3 CONFIDENCE:
"Fairly confident"? That fit looks perfect! I'd bet anything that we've got the right mixture.You'd bet anything? Are you sure, Simplicio? FigureYeah, the fit's pretty good, but the data are kind of noisy, and some of the features don't seem quite right. For example, the fit has a little shoulder around 26,735 eV, which seems to be missing from the data. What if there's another material that has a spectrum kind of similar to either monteponite or greenockite? Maybe it would give an even better fit, or at least one just as good. "Fairly confident" is about as far as I'd go.As explained in Box 7.1, it's straightforward to calculate that if 24% of the cadmium atoms are present in monteponite and 76% as greenockite, then by mass the sample is 22% monteponite and 78% greenockite. You might want to try that calculation yourself, to make sure you understand how to go back and forth between different measures of percent composition.

An Example of LCA 201:
In fact, greenockite has an allotrope called hawleyite. The near-neighbor structure for both minerals is very similar, and thus the XANES as well. Without a standard for hawleyite, we can't be sure the sample doesn't contain hawleyite and not greenockite, or even a mixture of those, in addition to monteponite.But that means this is hopeless! My own data are never going to match any better than FigureA fit like that in FigureThat is true, Kitsune. But a scientist should never publish a result that indicates a sample "is" 22% monteponite and 78% greenockite by mass.Robert is right. A statement like that doesn't give any indication of precision. If the sample is 23% monteponite, was the fit "wrong"? What if the actual fraction was 25%? 30%? Results have to be reported along with the uncertainties. "22% ± 2% monteponite" says something meaningful about the sample; "22% monteponite" doesn't. (Continued)

Normalization: A Source of Systematic Error:
In Section 4.3.4 of Chapter 4, we looked at a spectrum where we were uncertain of the proper normalization to within ±12%. In Chapter 6, we learned that uncertainty of that type does not have a big impact on fingerprinting techniques, as long as we try to normalize spectra we are comparing in a consistent manner. But, the spectra that contribute to an LCA are not necessarily similar to one another, and thus there might be a question of what constitutes "consistent" normalization. For example, FigureThere's no way of knowing if these standards, and the sample, were all normalized in a consistent manner, even if a single person took great care with the task. The normalization of these standards relative to each other, or the sample relative to some of the standards, could easily be off by 10%.The best linear combination fit to the data using the standards shown is given in Figure202 Chapter 7 -Linear Combination Analysis

BOx 7.3 CONFIDENCE (Continued):
That is true, friends. But saying something meaningful about the sample is the final goal. FigureThe fit in Figure

HOW TO AVOID (SOME) NORMALIZATION PROBLEMS IN THE FIRST PLACE:
In actuality, anyone who "took great care with the task" would never collect data over such a limited energy range. Even if one is only interested in the XANES, it only adds a little scan time to gather enough data to establish the post-edge trend of the background. Points spaced every 5 eV out to 400 eV above the edge should be sufficient. Sure, Robert, if you can, you'd collect points well above the edge. But there are a lot of reasons you might not be able to. Maybe there's another edge not too far above the XANESthat happens a lot with L-edges. Maybe, there are some really, really bad glitches up there. Or the monochromator might not be able to go much higher in energy.And sometimes the time it takes to measure those extra points does matter. In quick-XAFS measurements, the monochromator slews through the full energy range, so a much bigger energy range means much more time, and you don't want that if good time resolution is your goal. Sure, you could still collect careful spectra on the standards, but the spectra of the sample might still not extend very far above the edge.You raise good points, my dear Dysnomia. But the circumstances you describe are less likely to apply to standards than to an unknown sample. I, for one, would prefer to use standards that had been properly measured out well above the edge to ones that are truncated, even if it means collecting the data on a different beamline at a different time. along with the fraction of each standard present. A fit adding that degree of freedom is shown in FigureThe fit in Figure

Degrees of Freedom and Statistically Distinguishable Fits:
A comparison of the fits in Figures 7.5 The fit in FigureA rigorous statistical analysis of XANES fits turns out to be hard to do. For one thing, suppose we made a combination using the exact physical percentage of each constituent. Would the fit then be perfect? No, in part because of noise, but also because of systematic errors such as 204 Chapter 7 -Linear Combination Analysis What if, for some reason, you lack confidence in the normalization of your standards and data? In that case, you won't be able to trust your linear combination results to better than the uncertainty in your normalization. That's OK, as long as you acknowledge that uncertainty when reporting your results! Don't forget that these percentages give the percentage of chromium atoms present in each phase, as explained in Box 7.1! They are neither mole fractions nor percent by mass! Mathematically inclined readers might think that there are 128 possible combinations of the 7 standards-after all, each one might be present or absent, giving 2 7 possibilities. But the case where no standard is used doesn't produce anything, and if only one standard were needed, simple fingerprinting would have provided the answer. That gives us 128 -1 -7 = 120 feasible combinations.normalization and other sources we'll discuss in Section 7.6. In many cases, systematic errors are the more important source of mismatch. Most of the familiar machinery of statistics is not well-adapted to cases where systematic errors dominate. For instance, it is difficult to estimate what the "measurement uncertainty" of a XANES spectrum is.Another difficulty with traditional statistics is that it is not clear how many "independent points" a XANES measurement includes. Compare, for instance, a spectrum collected with an energy spacing of 0.5 eV to one collected with a spacing of 1.0 eV. The former may give us a little more information about sharp features, but it certainly does not provide us twice the information of the latter, despite including twice as much data. In other words, the absorption at any particular energy is correlated to the absorption at neighboring energies, and the points are, therefore, not independent.The combination of these difficulties makes it very hard to use statistics to answer the question "Is this linear combination fit good enough that it might be the right one?" by statistical methods alone.It turns out we can, however, use statistics to address a more limited, but still very useful, question: "Is fit B significantly more likely to be right than fit A?" This question does not depend on our estimates of measurement uncertainty, and only modestly on our estimate of the number of independent points. Walter Hamilton addressed this problem in 1965 for the context of crystallographic modeling

Quantifying Fit Mismatch:
Mismatch between data and fit can be measured in a number of ways. One common method is the XAFS R-factor, which we define as the sum over all N measured points of the squared difference between each data point and the fit, normalized by the sum over all measured points of the data: While the superscripted 2 may appear to make the IXS definition similar to EquationSpeaking of χ 2 , another common method of quantifying fit mismatch is to use the definition of that statistical quantity:where ε i is the measurement uncertainty associated with point i and the points i are taken so as to make data i represent N ind independent measurements. As we discussed in Section 7.4.2, it is difficult to know the measurement uncertainty ε i for XANES data, and even more difficult to know how to pick points so as to make the data i independent. One option that has been adopted is to simply use Equation

Degrees of Freedom:
Statistically, the degrees of freedom for a fit is given by the difference in the number of independent data points and the number of free parameters. Because we do not know how many independent points are in a XANES spectrum, it may appear that we are stuck. Fortunately, we can put lower and upper limits on the number of independent points. 206 Chapter 7 -Linear Combination Analysis Consider FigureEach peak or shoulder can be characterized by an amplitude, a centroid, and a width. If we think there are around 6 features visible, that means there are 18 quantitative parameters extractable from the data. Add in 2 to specify the edge itself (energy and width), and we can think of this spectrum as having at least 20 independent points. The upper bound is easy, since it's the number of distinct energies at which the data are measured; in this case, that turns out to be about 90."Somewhere between 20 and 90" may not sound very precise, but it turns out to be good enough to get information from the Hamilton test.

The Hamilton Test:
The Hamilton test is relatively insensitive to which measure of closeness of fit is usedThe procedure for the Hamilton test for linear combination fits on XANES data is then as follows:1. Compute the ratio of the R-factor for the closer fit to that for the other fit. Call the result r. 2. Take half of the lower bound of the number of degrees of freedom of the closer fit, and call that a. 3. Take half of the number of free parameters that have to be added According to the software used to produce the fits, which uses definition 7.3 to compute the R-factor, the fit shown in Figure

Statistics of Linear Combination Fitting 207:
The method suggested here for placing a lower bound on the number of independent points in a XANES spectrum is, unfortunately, quite subjective.The procedure for EXAFS is, at least, more objective, as we can see in Section 11.1.1 of Chapter 11.b is calculated from half the total number of free parameters added, not half of the net number of free parameters added! Suppose fit alpha uses the amounts of constituents A, B, and C as free parameters, while fit beta uses the amounts of constituents C, D, and E. Even though there are the same number of free parameters (three each), to get from fit alpha to fit beta we need to add two free parameters (the amounts of D and E). Thus b would be 2/2 = 1.an R-factor of 0.00116, while that in FigureThe lower bound on the number of independent points for our data is 20. The closer fit has four quantitative free parameters, so it has at least 16 degrees of freedom. That means the lower bound for a is 16/2 = 8.The closer fit adds two free parameters: the amounts of K 2 Cr 2 O 7 and Na 2 CrO 4 . So, in this case, b = 2/2 = 1.Calculating I 0.147 (8,1) gives 0.0000002, meaning that there is less than one chance in a million that the difference between these fits would arise by random variation. The fit in Figure

Uncertainties:
Numbers corresponding to physical systems should be reported with uncertainties (or at least the implicit uncertainty associated with the number of significant figures shown). That is a basic rule in the practice of science. How do we determine the uncertainties associated with the fraction of each constituent as determined by a linear combination fit?The uncertainty in a fitted parameter (we'll call it δ) can be defined as the amount by which the parameter can be changed, while allowing the other fitted parameters to vary, without causing the fit to be significantly worse. This means that determining δ, like the determination of whether a fit is "good" statistically, requires consideration of both the measurement uncertainty ε and the number of independent points in the data.Software that performs the least-squares minimization used for linear combination fitting is commonly available: you may, for instance, use a stand-alone application, a feature that's part of a graphical analysis suite, a routine in a mathematical analysis language, or a software package designed specifically for XAFS analysis. Any of these should provide an estimate of the δ's for fitted parameters or a way of computing them-if what you're using doesn't, then start using something else! But in almost all cases, the uncertainty estimate provided by software is not an appropriate uncertainty for XAFS linear combination fitting, and needs to be modified.To understand how to modify the reported uncertainty, we first need to understand how we would identify a statistically good fit if we did know the measurement uncertainties ε i and the number of independent points. We would start by computing the reduced χ 2 , represented by the symbol χ ν 2 :208 Chapter 7 -Linear Combination Analysis where ν is the number of degrees of freedom of the fit and N ind is the number of independent points. As Dysnomia explains, for a fit to be statistically good, the reduced χ 2 should be around 1 (for a more careful discussion of this criterion, seeTo help us adjust the δ's reported by a fitting routine, we'll first note that almost all software that would be used for XANES linear combination fits will assume N ind = N, and thus will also have too large a value for ν. We discussed in Section 7.4.2 how to place a lower bound on N ind . For the ε i 's, let's start by using an arbitrary value-typically 1.For the fit shown in FigureBut we believe our best fit to be a "good" fit-we chose a reasonable set of standards to conduct our fitting process, and believe that the result we've arrived at is probably also reasonable. This means we expect χ ν 2 to be around 1; the value of 0.00015 we got is presumably so much smaller because we made up an arbitrary value for ε. Examining the proportionalities in Equation

. Since the δ's should be proportional to ε, that means we should multiply all of the δ's reported by our fitting software for ε = 1 by 0.012.Following that procedure for the fit, from FigureHere is a summary of the method we just used to estimate uncertainties:

Statistics of Linear Combination Fitting 209:
If the mismatch between data and fit were due just to the uncertainty ε, we'd expect the average term in the sum in EquationYou need to make sure you understand what your software is doing before reporting uncertainties! For instance, some XAFS-oriented software performs the trick for you of assuming χ ν 2 should be 1 to compute uncertainties, but still assumes N ind = N.

COMBINATORIC FITTING:
We have established that the fit shown in FigureWe could try a fit without K 2 Cr 2 O 7 , and see what the statistics tell us. But why stop there? Given that we have a computer to do the numbercrunching for us, let's try fitting all 120 possibilities. That's called combinatoric fitting. Since we've already established that the normalization of the sample was likely a bit different than that for the standards, we won't force the percentages to total to 100%.The R-factors of the resulting combinations range from the 0.00017 we already found up to 0.06025 for a combination of Cr and Cr 2 S 3 . The fits corresponding to the lowest values are shown in Table

Now we can use the Hamilton test to understand what the combinatorics results tell us.:
A good principle is to begin by comparing fits with relatively few parameters because they are the easiest to distinguish. In this case, let's compare fit number 5 (lowest R-factor with two free parameters) to fit number 2 (lowest R-factor with three free parameters). The ratio of R-factors is 0.00017/0.00052 = 0.33, fit number 2 has at least 20 -3 = 17 degrees of freedom, and fit number 2 adds one free parameter to fit number 5. So we calculate I 0.33 (8.5,0.5) = 0.00002. That means that fit number 2 represents a statistically significant improvement over fit number 5 at the 99.998% confidence level, and we thus reject fit number 5.What about fit number 3? Comparing it to fit number 2, the ratio of R-factors is 0.00017/0.0032 = 0.53. Fit number 2 still has at least 17 degrees of freedom, and fit number 2 again adds one free parameter to fit number 3. (Again, note that it is the total number of free parameters added that matters; in this case, we need to fit the amount of Cr 2 O 3 . The fact that we no longer need to fit the amount of Cr does not enter into the computation.) I 0.53 (8.5,0.5) = 0.001, indicating a 99.9% confidence level that fit number 2 is better not just by chance, so we reject fit number 3.210 Chapter 7 -Linear Combination Analysis There's no magic order to doing these Hamilton tests. We're just playing the fits off in pairs and rejecting any one that loses at the 95% level.Eventually, we get down to a set that can't knock each other off anymore, and those are our winners.Since fit number 4 has a worse R-factor and the same relative number of parameters, we can reject it as well.Fit numbers 1 and 2 cannot be distinguished, as they have the same R-factor. Both must be considered possible.Of course, we have assumed there is little systematic error in our analysis, beyond the allowance we have already made for the normalization of the sample's spectrum. Systematic error could conceivably change our conclusions. We'll discuss ways in which it could originate next.

SOURCES OF SYSTEMATIC ERROR:
We've already discussed inconsistent normalization as a source of systematic error. This section will discuss additional sources.

Energy Alignment:
Suppose that, because of inconsistent energy calibration, the sample spectrum we've been fitting in the last two sections had been shifted to the right by 1 eV relative to the standards. Rerunning the analysis, it turns out that the R-factor for the best fit rises all the way to 0.0050that's nearly 30 times worse than before! Even worse, the closest fit turns out to be 0.47 CrO 4 , 0.45 K 2 Cr 2 O 7 , and 0.10 K 2 CrO 4 , which is radically at odds with the conclusions in Section 7.5. The danger is clear: even a slight error in relative energy calibration can wreak havoc with LCA of XANES spectra.Fortunately, it is usually easy to prevent this from happening. If reference spectra are collected simultaneously with the sample data, those can be used for alignment as described in Section 4.2.1 of Chapter 4. Even when simultaneous collection of a reference isn't feasible, reference data can be collected periodically during the experiment, and carefully scrutinized for drift.Still, it is possible that at some point you will be working with a data set for which the energy calibration is not reliable. It is not uncommon, for example, to find that a constituent is something you did not anticipate in advance, and, therefore, to have to use a standard collected by someone else, at least until you get more beam time and a chance to measure it yourself. If the person who measured that standard didn't take good care with energy calibration, you may be stuck.In cases like that, it's not difficult to allow the fitting routine to fit an energy shift for any spectrum where calibration is suspect. Continuing with our example, when we allow our computer software to treat the

Sources of Systematic Error 211:
Inspecting encoder values can correct for some kinds of energy drift! energy calibration of the sample as a free parameter, we immediately retrieve something very close to the closest fit, as shown in Section 7.4. So what's the big problem? Can't we just rely on our software to save us?The problem is, of course, that each distinct energy shift allowed in a fit represents one more free parameter, and one fewer degrees of freedom. That means the ability to statistically reject fits will drop, and we'll become less confident in our conclusions.To take it to an extreme, suppose that we had no confidence in any of our energy calibrations, and allowed a separate shift for each of the standards. When the fits from Section 7.4 are rerun, but with every energy allowed to vary independently, we do get a closest fit something a bit like the best fits from before: 0.24 Cr 2 O 3 , 0.13 CrO 4 , 0.02 K 2 Cr 2 O 7 , 0.02 K 2 CrO 4 , and 0.62 Na 2 CrO 4 . The R-factor for this fit is 0.00010, which is better than the old fit number 2. But is it a statistical improvement? We can use the Hamilton test to find out.In this case, we are adding seven free parameters: the old fit number 2 didn't have any K 2 Cr 2 O 7 or K 2 CrO 4 , and we have five new parameters for energy calibration. Our new fit has a total of 10 free parameters, and thus 20 -10 = 10 degrees of freedom. That brings a down to 5, and b up to 3.5. With a ratio of R-factors of 0.00010/0.00017 = 0.59, we compute I 0.59 (5,3.5) = 0.49, meaning the improvement in the R-factor is quite likely to have occurred by chance alone.It is also worth mentioning that, in this case, the fit turns out to give energy shifts for the standards that vary by nearly 5 eV. If we knew that we (or someone else) had collected the standards fairly carefully, and had used a reference foil for calibration, then a 5 eV shift should not have occurred. So, the fit in which all the energy shifts allowed to vary is, in this case, less realistic than our old one.The moral: don't allow parameters to vary just because your software happens to allow it. Careful planning during measurement and data reduction can cut down on the number of free parameters, which in turn should increase the confidence you place in your results.

Background:
For XANES and energy-space EXAFS analyses, if the spectra being used for standards and sample have different slopes in the pre-edge or postedge region, it can introduce systematic error. This can be addressed either during data reduction (by some sort of "flattening" routine) or during fitting (by introducing additional free parameters).If this is a problem in your data set, it can also be advisable to limit the range over which the data are fit (see Section 7.7).

Chapter 7 -Linear Combination Analysis:
Why, if we have no confidence in the energy calibrations of five standards and a sample, does that represent five free parameters rather than six? It is because it is only the relative shift that matters. Varying the shift of each standard relative to the sample gives five parameters.

Attenuation: Self-Absorption, Inhomogeneous Transmission Samples, Harmonics, Dead Time, and So On:
As discussed in Chapters 3 and 5, there are several sample preparation and data collection issues that can cause spectra to be distorted. While they differ in detail, these distortions generally result in attenuation of high-absorption features, most notably white lines. They can also result in exaggerated amplitude for pre-edge features.Distortions of this kind have a substantial effect on LCAs, particularly of XANES. Even in cases where the shapes of the standards are very different, such as the greenockite/monteponite example in Section 7.3, distortions can cause substantial changes in the percentages of each constituent found by a linear combination fit.With the partial exception of self-absorption corrections (Section 4.3.5 of Chapter 4), not much can be done about distorted spectra after the fact. When planning on LCAs of XANES, use the information from Chapters 3 and 5 to minimize the extent of distortion before analysis.

Energy Resolution:
As discussed in Section 5.4.2 of Chapter 5, the shape of an XAFS spectrum depends on the energy resolution of the measurement; this is particularly true for sharp features such as the white line and some pre-edge features. While it is possible to mathematically modify a spectrum after collection to deliberately simulate a coarser energy resolution, it is better to try to have the same energy resolution for all measurements.

Glitches:
Because the minimization routines used in LCA minimize the square of the difference between the data and the fit, even a single-point glitch, if large enough, can introduce systematic error into a XANES or energyspace EXAFS linear combination fit. If possible, deglitch all the standards and the sample before analysis (see Section 4.3.2 of Chapter 4). If large glitches happen to lie near the boundaries of the region you're fitting (e.g., in the linear pre-edge region), simply choose the region so that they're not included.

Noise:
Noise might seem as if it should contribute only to random error, not systematic error. That would be right if individual scans were each fit separately. But since we are usually working with merged data (see Section 4.2.2 of Chapter 4), the noise remaining after the average gets "frozen in." If shot noise causes one of your standards to have a little blip up or down at, say, the top of the white line, then that will affect all7.6 Sources of Systematic Error 213 analyses using that standard. In the sense that it biases all analyses in the same way, the error is systematic, even though it is random in origin.

CHOOSING DATA RANGE AND SPACE FOR LCA:
So far, this chapter has focused primarily on LCAs of XANES data in energy space. We've done that for three reasons: because it's one of the most common forms of LCA, because it's particularly sensitive to several types of systematic error, and because we thought that sticking with one or two examples for most of the chapter was less distracting than looking at all the varieties of LCA together.In this section, we will discuss each of the common forms of LCA of XAFS.214 Chapter 7 -Linear Combination Analysis

BOx 7.5 MEASURING STANDARDS FOR LINEAR COMBINATION ANALYSIS:
I guess the message of Section 7.6 is: "Measure your standards under the same conditions as the sample." If I do that, then problems I have with self-absorption or energy resolution or whatever will apply to the standards and the sample, and I'll be fine.Careful! If, by trying to measure "under the same conditions," you mean that you'd take a standard that is pure gypsum and measure it in the same fluorescence geometry as a sample that is 80 parts per million sulfur, then your standard would suffer severely from self-absorption, but your sample would not! OK. So in that case I could dilute the gypsum standard to have more or less the same sulfur concentration as my sample and then measure it the same way.Then you'll end up with a very poor signal-to-noise ratio for your standard. You may not have a choice with your unknown material, but why intentionally collect lousy data for your standards?Ideally, standards should be measured using the same energy resolution as the sample. If it is convenient to measure them in such a way so that the standards and the sample have a similar background, that is also helpful. Other than that, one should simply try to get good measurements off the standards; to wit, strive for a good signal-to-noise ratio and to minimize distortions such as harmonics, self-absorption, and so on.

xANES in Energy Space:
As discussed in Chapter 6, XANES is sensitive to valence, the symmetry of the local environment, and nearest-neighbor bond length. It is largely insensitive to "disorder," as discussed in Section 1.2.12 of Chapter 1, and is not usually as good a tool as EXAFS for investigating structure beyond the first coordination shell.The signal-to-noise ratio will be better for the XANES region than for the EXAFS region; for low-concentration samples, this may make XANES the only usable option.To avoid systematic error introduced by differences in the slope of the pre-edge, it is best to choose the low end of the fitting region to be about 10 eV below the first pre-edge feature (or the beginning of the main rise, if there are no pre-edge features).It is also possible to perform linear combination fits on particular features within the XANES region; this is closely related to some of the advanced fingerprinting techniques discussed in Chapter 6.

xANES in Derivative Space:
Any XANES fit in energy space is dominated by the overall shape of the edge. By performing the analysis on the derivative of the normalized absorption, the emphasis is shifted to features such as peaks and shoulders.Derivative spectra are often noisy, however. As indicated in Section 7.6.5, noise can act as systematic error in certain cases. Rebinning (Section 4.1.1 of Chapter 4) or smoothing can reduce that noise, but also reduces energy resolution unless the data were oversampled to begin with.

ExAFS in Energy Space:
The EXAFS region of the spectrum, as discussed in Section 1.2 of Chapter 1, is sensitive to the distances, coordination numbers, chemical identity, and disorder of scattering shells from the nearest neighbors out to 5 Å or more. It thus provides somewhat complementary information to the XANES region. If, for example, the sample is disordered in comparison with the standards, a linear combination of standards may fit the XANES region much better than the EXAFS region. If, however, a local distortion changes the symmetry of the nearest-neighbor environment, but has minimal impact on the longer-range crystal structure, the EXAFS may fit better than the XANES.EXAFS is also somewhat less severely affected than XANES by measurement problems such as self-absorption, inhomogeneous transmission samples, harmonics, or instability in energy calibration. In the EXAFS 7.7 Choosing Data Range and Space for LCA 215The insensitivity of XANES to disorder can be a good thing, since the sample can then be more disordered than the standards without affecting the analysis much.The second derivative spectrum is like the derivative spectrum, but more so-peaks become strongly emphasized, at the cost of additional noise.R-factors for different spaces (e.g., energy vs. derivative) or for different regions cannot be directly compared! That's part of the price we pay for not being able to calculate true statistical measures of closeness of fit. When we talk about the XANES fit or the EXAFS fit being better, we are relying on judgment and experience, not quantitative measures.range, modest amounts of attenuation due to those problems tend to mimic normalization errors. While they would thus still contribute to systematic error in the quantitative results (how much of each constituent is present), the qualitative results (which constituents are present) are less affected.The fits in Sections 7.4 and 7.5 utilized both the XANES region and a bit of what many would consider the EXAFS region, extending to 80 eV above the edge. If we had wanted to investigate much farther above the edge than that, however, it's generally best to fit the XANES and EXAFS regions separately. That's because the features in the EXAFS region are usually much smaller than those in the XANES region, and are certainly much smaller than the edge jump itself. If a pre-edge peak, for instance, were 10 times larger in amplitude than a particular EXAFS oscillation, then a 5% mismatch in the pre-edge feature would count as much as a 50% mismatch in the EXAFS feature. Thus, a single fit across the entire spectrum could prioritize reducing small fractional errors in the XANES part of the spectrum at the expense of entire wiggles in the EXAFS. If, as is often the case, systematic error at the 5% level is present in the XANES region, a single fit across the entire XANES and EXAFS regions might discount EXAFS information in favor of trying to improve a fit in a XANES region which is somewhat distorted.In addition, fitting the two regions separately provides a partially independent check of the results. It may even act as a way of distinguishing between fits which are otherwise difficult to distinguish, such as fit numbers 1 and 2 shown in TableApplication of the Hamilton test to an EXAFS fit once again requires estimating the number of independent points present in the data, but the method discussed in Section 7.4.4 seems somewhat less well-suited to the oscillations of EXAFS data. Instead, we'll take inspiration from Robert's argument in Section 4.5.8 of Chapter 4, and assert there can't be more than one independent point per 0.16 Å -1 , and that's only if we think we can extract data out to 10 Å in the Fourier transform. That's very rarely the case, except for the most highly ordered crystals-5 Å might be more typical, in which case we're getting one independent point per 0.32 Å -1 .In the spirit of conservatism, we'll round that up to one independent point per 0.4 Å -1 .

ExAFS in χ(k):
If we want to focus on the EXAFS, linear combination fits can also be performed in χ(k). As described in Chapter 4, this requires background subtraction and a consistent choice of E o . Background subtraction can be beneficial, in that it removes a potential source of nonstructural variation between spectra. However, if background subtraction is done inconsistently, that can introduce a new source of systematic error! 216 Chapter 7 -Linear Combination AnalysisWe will consider the statistics of EXAFS more rigorously in Section 11.1 of Chapter 11.Of course, "fitting in χ(k)" generally actually refers to fits on kχ(k), k 2 χ(k), or k 3 χ(k), in accord with Section 4.4.5 of Chapter 4.TableAfter looking at the R-factors in TableYou have to pick an E o for the spectrum of the sample, right? But the spectrum is a linear combination of the spectrum of a metal and an oxide. So for the sample, you will necessarily be choosing E o incorrectly for at least one of its constituents-there's no way around that. To make LCF work properly, the standards must have the same errors. So for an LCF analysis, you need to assign the same E o to every spectrum involved, even if they have different oxidation states! The easiest way to answer that is to look at one of the fits graphically. Look ahead a bit to FigureThe k-weighted χ(k) data are quite noisy, and that noise appears to be the greatest contributor to the R-factor for these fits. In this case, an R-factor above 0.10 appears to be consistent with reasonably good fits, given the noise in the data. This would certainly not have been the case for the XANES data, for which the signal-to-noise ratio is much better.Let's apply the Hamilton test to fit numbers 5 and 2 in TableNext, let's compare fit numbers 3 and 4 with fit number 2. The ratio of R-factors is 0.138/0.190 = 0.73. Once again, only one parameter needs to be added to go to fit number 2 (the amount of CrO 4 ), so we compute I 0.73 (7.5,0.5) = 0.033. These fits are also tentatively rejected.Tables 7.1 and 7.2 now each include two fits that we have not rejected on statistical grounds. But only one of those is common to both: the one with just Cr 2 O 3 , CrO 4 , and Na 2 CrO 4 , numbered 2 in both tables.Those two fits are not entirely in agreement, however, as the percentage of each constituent they show are different. Are they close enough that we could say they're "consistent"?At this point, we can assume each of those fits is "good," and use the method discussed in Section 7.4.6 to compute the uncertainties in each amount. For fit number 2 in To answer that, we construct an energy-space spectrum using the constituent weights found by the k-space fit, that is, the weights given in fit number 2 of TableOne option at this point would be to consider ourselves done, and just declare that our sample has somewhere between 12% and 22% Cr 2 O 3 , between 10% and 22% CrO 4 , and between 64% and 72% Na 2 CrO 4 , with possible trace amounts of other chromium compounds.But we do have one more weapon in our arsenal-the magnificent ability of the trained human brain to synthesize and analyze complicated information, particularly when presented visually. Let's compare the two fits in both energy space The graph has been split into two regions to facilitate examination of the differences between the two fits and the data. examination of fine detail, the energy-space plot has been split into the pre-edge and post-edge regions.The EXAFS best fit appears to follow the data better in both the k-space graph, which covers the EXAFS region, and over the peaks and shoulders from 6010 to 6050 eV, which comprise the white line. The one place where the XANES best fit has an advantage is at the sharp pre-edge feature around 5993 eV; both fits overestimate the size of that peak, but the overestimate of the XANES best fit is slightly less.Is that one slight-looking difference enough to account for the better R-factor of the XANES best fit? Notice the difference in scale between the y-axis of the top and bottom graphs in FigureBut why does a fit that looks pretty good above the edge show that kind of mismatch on a pre-edge feature? It suggests a systematic error of some sort. If there were harmonics in the beam, that would emphasize pre-edge peaks at the expense of features above the edge, the opposite of what we see. But, as Section 5.4.2 of Chapter 5 shows, poor energy resolution results in sharp peaks being reduced in amplitude, and the pre-edge peak is the sharpest feature in the spectrum. (Merging individual scans that are slightly misaligned could also cause this kind of effect, but is easy to check for.)In the scenario Robert outlines in Box 7.7, we would identify the EXAFS best fit as superior and report it in any publications arising from the experiment, perhaps broadening the uncertainty ranges a bit to err on the side of conservatism.

The Back-Transform of ExAFS:
In some cases, it might be desirable to use Fourier filtering before performing a linear combination fit in the EXAFS region. Filtering out low R has the benefit of reducing the effect of difference in background subtraction between the data and the standards. Filtering out high R can be useful if the standards are likely to deviate most from the constituents of the sample beyond the nearest neighbor; for example, the sample is made of nanoscale phases, and the standards are bulk crystalline materials.Filtering also allows a more rigorous definition to be used for the number of independent points present (see Section 11.1.1 of Chapter 11).If filtering is desired, it is best to perform LCA on the back-transform, and not directly on the Fourier transform χ(R). While χ(R) of a mixture is in theory a linear combination of the contributions from the individual constituents, the interactions between the systematic errors described in Section 7.6 and the Fourier transform effects detailed in Section 4.5 of Chapter 4 can be difficult to understand intuitively. Alas, dear Simplicio, these data are fairly good. There are always trade-offs involved, especially considering the limited beamtime that is available. Suppose, for instance, that the experimenter collected the standards shown above on a previous trip to the synchrotron; there are too many to realistically expect them to be recollected every time. When attempting to collect the spectrum of the sample, our experimenter notes how poor the signal-to-noise ratio is, and decides more photons are needed. The experimenter tries widening the vertical slits. Mindful of the risk of degrading the energy resolution, the experimenter collects spectra on a chromium foil before and after changing the slit size, and finds the spectra to be indistinguishable. But the spectrum of chromium metal has no features as sharp as the pre-edge feature in question, as one can see from inspection of FigureIn such a case, the experimenter did nothing wrong. Each decision was defensible, given the options available. As long as the experimenter kept good notes, we could, at this point in the analysis, identify the likely source of systematic error and select the superior fit.

INTRODUCTION:
In this chapter, we will learn about principal component analysis (PCA). We will begin by introducing an example system. Next, we will explore the ideas of PCA informally, without dwelling on formal definitions. After introducing the formalism, we will explore some additional ways to use the technique.

An Example from the Literature:
FigureIn their paper, Lengke et al. used linear combination analysis to address those questions. That's a challenging problem, however, as they had to measure the right standards, some of which have spectra that appear differently in solution in their solid form.Is there a way to get a sense of what is going on in that series of spectra without having to identify all the constituents?

Isosbestic Points:
Consider two normalized spectra of pure constituents, f(E) and g(E). Because they are normalized, any spectrum made by mixing them can be expressed as a weighted average of the two; that is, by xf(E) + (1 -x)g(E). At any point where f(E) and g(E) have the same value, then, any weighted 224 Chapter 8 -Principal Component Analysis We're going to use the data from Figureaverage will have to take on that same value-the average of two identical numbers does not depend on how the average is weighted! These points are called isosbestic points. FigureMost spectra representing different mixtures of two constituents will reveal several isosbestic points. It is unlikely, however, for different mixtures of three or more constituents to show isosbestic points, as that would require three (or more!) constituents to happen to have the same normalized absorption at some energy. Thus, isosbestic points can be used as a clue that two (and only two) constituents are being mixed in different amounts to make the series of spectra. Does the gold series show isosbestic points? At first glance, it appears there may be one around 11,924 eV, another around 11,934 eV, and a third around 11,952 eV. But that's worth a closer look, as we can see in FigureOn closer inspection, none of the points appear to be isosbestic; the plots do not intersect at all at 11,934 eV, and in the other two cases, the intersections span more than half an eV. Unless the alignment of the spectra through references or the normalization was sloppy, it appears the fractions of more than two constituents are varying in this set.

ThE IDEA Of PCA:
Suppose we want to focus on the differences between the eight spectra in the gold series. Our first step might be to average the spectra, so that we have a baseline with which to compare them. Next, we might want to construct a function of energy that captures most of the differences that we see. More precisely, we could choose a function that, in linear combination with the average, captures the largest possible fraction of the variance between the original eight spectra. In the following several sections, we will introduce the concepts of PCA, without saying anything about how these quantities are computed, deferring that to Section 8.5.Well, in Section 8.5, we'll give some of the theory behind the computation.But the honest answer is that there is software that will do these computations for you! One of the most notable changing features in FigureThe two functions, on their own, do not perfectly reproduce the highenergy side of the white line for some of the spectra. We will explain the convention for the scaling of component 2 in our graphs in Section 8.5, once we understand more about how PCA works.We can, therefore, seek a third component (i.e., in addition to the two shown in FigureUnlike the first two components (shown in Figure

hOw MANy COMPONENTs?:
As we will discuss in Section 8.5, it takes as many components as there are spectra to account for all of the variance; that is, to be able to perfectly reproduce every point of each spectrum, including the noise. For our eight gold spectra, that means we need eight components. We've already looked at three (the initial average counts for this purpose). Let's jump straight to the eighth (FigureThis component is all high frequency; that is, it is only fitting noise and other nonstructural features. Throwing away the eighth component from our reconstructions would mean throwing away only noise, not signal.So how do we know which components include signal and which are only noise? There is no single fixed, reliable rule. Several approaches, however, yield valuable clues.

Appearance of Components:
As implied in earlier sections, we can use our understanding of the appearance of XAFS spectra to distinguish features related to noise or other nonstructural features to those related to the structure of the material. FigureThe fourth component appears to have some structure to it on the scale of XANES features. The remaining three components appear to consist primarily of noise-like features. It appears that four components are enough to account for all the structural information in the spectra. The original scan parameters for these data switch from a step size of 0.5 to more than 1 eV at 11,950 eV. Because the points are measured further apart above 11,950 eV, the frequency of the noise also appears to change.

fourier Transform of Components:
We can make our visual intuition from Section 8.3.1 a little more concrete by taking the Fourier transform of the components, as in Figure230 Chapter 8 -Principal Component Analysis

BOx 8.1 MAGIC?:
Wait-that sounds almost like magic! We can throw our spectra in machinery called PCA, find some components that are due to noise, and then rebuild the spectra without those noisy parts? Doesn't that violate some rule of statistics or something? How can we improve signal to noise just by processing the data? Suppose we had five scans of the same material under the same conditions, and we tried performing PCA on those five scans to eliminate noise. We would find that the first component-the average-was the only one that wasn't just due to noise, and we could throw the other four away. And in fact, that's what we do-we average the five scans and use that average for analysis! The best PCA can ever do at eliminating noise is the improvement specified by Poisson statistics. But because we usually operate on spectra that aren't supposed to be identical, it doesn't do quite that well; some of the variance is due to structure rather than noise.Kitsune is correct, but that does not mean you have not identified a useful technique, dear Simplicio. Suppose one has a sequence of related, noisy scans, such as is often the case for time-resolved studies. One could use PCA on the series and remove components that appear to correspond solely to noise. The idea is similar to a moving average, but more sophisticated, as the commonalities in all scans, not just subsets, are contributing to the signal. One's eye might be drawn to the peak around 11,959 eV in component 5. It is true that it appears to be of large amplitude compared to the rest of the peaks in that component. Because of the increased step size for data collection in that part of the spectrum, however, it does not appear to be more than a data point or two in width and is certainly far narrower than any structural feature that would appear that far above the edge. It might be associated with a glitch or a transient phenomenon such as a bubble. It is not, however, structural.Yeah, and those jiggles around 11,920 eV in components 5 and 6 have something to do with the edge, but seeing as they go up and down so fast, they are probably an artifact of the way the data were measured, aligned, and averaged. They're not structural either.For components 6 through 8, the spectral weight is distributed roughly equally as a function of frequency; that is, the distributions are white. This kind of distribution is typical for shot noise and many kinds of electronic noise.In contrast, components 2 and 3 have their spectral weight concentrated at low frequencies in energy space, suggesting they are associated with structural features, or perhaps background differences.Components 4 and 5 are intermediate. About half their spectral weight in FigureOur conclusion, then, is similar to that from Section 8.3.1: from three to five components are necessary to account for all the structural information in the spectra.

Compare to Measurement Error:
Another way to evaluate which components are structural is to compare the fraction of the variance between spectra that is explained by a component with the estimated measurement error of the individual spectra. This, of course, raises a problem we discussed in Section 7.4.2 of Chapter 7 (and will cover in more detail in Section 11.1.2 of Chapter 11): it is not clear how to determine measurement uncertainty for XAFS spectra! But even the random part of the error (i.e., the noise) is often difficult to determine for PCA series, as we'll see. multiple scans under identical conditions, a luxury that we do not have for a time-resolved series. That leaves, in this case, either a comparison to a smoothed version of the data or estimation from theory. For this example, we'll use a comparison of the preedge region of each sample to a quadratic fit (although not shown in FigureThe average variance per point between the eight gold spectra over the region used for PCA was 8 × 10 -5 . This should be compared to the square of the measurement error, or about 1 × 10 -6 for the earlier spectra. In round numbers, only about 1% of the variance between samples is due to noise.Because the idea behind PCA is to account for as much of the remaining variance as possible with each additional component, the fraction of variance accounted for by each component is a natural byproduct of the process. For the gold series, the results are summarized in TableWe don't know what fraction of the variance between spectra is due to systematic errors such as energy alignment, normalization, and differences in background, but they surely play some role. If the systematic error is comparable to the amount of noise in this sample set, then components 5 through 8 are all dominated by noise and systematic errors, in agreement with our conclusions from earlier sections.

scree:
The last paragraph of the preceding section probably feels a bit unsatisfying. Because we don't know how much systematic error there is, we really don't know where to draw the line between structural information and the rest.

Chapter 8 -Principal Component Analysis:
Data were also collected beyond 11,970 eV; otherwise normalization would have been difficult. But only the data from 11,910 to 11,970 eV were used in the construction of the principal components.Our language has become a bit of a mess here, I'm afraid. In some fields, PCA is performed by first centering the data; that is, by subtracting the mean of each data set from that set in advance of applying the PCA algorithm. It has not been the tradition in XAFS to do so. The result is that the quantities that are reported as "variance" are not truly variance! In fact, the first component, which is related to the mean, is given the lion's share of this "variance." Some practitioners, therefore, do as we have done and compute the fraction of "variance" accounted for after the first component. As always, one should be clear regarding the procedure one has used when describing one's results. But we do have another piece of information: we're pretty sure that component 8 is dominated by nonstructural differences (noise and systematic error), and it accounts for 1.2% of the variance. (It's comforting that this is on the order of the percentage we expected to be due to noise from our estimates in Section 8.3.3.) The nature of PCA requires each previous component to account for progressively larger chunks of the variance. So component 7 should account for a greater fraction of the variance than component 8, even if they're both due to noise.FigureComponent 7 accounts for a bit more of the variance than component 8, component 6 a bit more than that, and component 5 a bit more in turn. But then, the differences between components become much greater, and the trend line established by components 5 through 8 is broken.While you're thinking about that, FigureBecause of the similarities between the shapes of Figures 8.11 and 8.12, a plot like FigureMany times, as is the case here, the scree plot ends with a nearly constant slope. The components that fall on that nearly constant slope are likely to be predominately due to noise and systematic errors, whereas the components before the break are more likely to be structural.In this case, we can conclude that the first four components incorporate the majority of the structural information, whereas the last four include primarily noise and systematic error. Component 2 would be far-off scale. Components 5-8 fit a trend that is not shared by components 3 and 4.One will sometimes see it stated that the component at the "elbow" of the scree plot-in this case, component 5-should be considered structural. That does not seem sensible to me; it clearly belongs to the less significant components. I agree, Robert, but the more important point is the one made in Section 8.3.5: all hard-and-fast rules about selecting criteria are imperfect. Because of that, I'd rather look at the components in several ways and then make a judgment call.

Objective Criteria:
The criteria in Sections 8.3.1-8.3.4 are all somewhat subjective. Several objective, numerical criteria have therefore been proposed. We won't try to detail all of them here, in part because they can become somewhat redundant. The advantage of objective criteria is, of course, their objectivity, but the disadvantage is that they are more likely to include a component that is not structural, or exclude one that is, than a judgment call based on multiple criteria. Making a decision subjectively also has the advantage that it is more clear how easy or difficult the decision is in a given case.One criterion that is sometimes used in other fields is to include components until they cumulatively explain some percentage of the variance, perhaps 90% or 95%. For XAFS analysis, this kind of criterion is not a good idea. Conceptually, what is important is the contribution of a component relative to the level of noise, not relative to the total amount of variation. To see this more clearly, imagine adding an additional spectrum to the original gold series that was quite different from the other spectra, including a constituent that wasn't present in the rest of the series (this sometimes happens when the spectrum of a quickly-consumed starting material is included in a series). That would 234 Chapter 8 -Principal Component Analysis increase the total variance and would probably mean that the number of structural components needed to recreate the data would increase by one. And yet, using a criterion based on cumulative percent of variance explained would make components that explained the small variations between the rest of the data less likely to make the cut.Another common criterion is to include components that explain an amount of variance at least 70% of the average amount explained by a componentFinally, we should mention Malinowski's factor indicator function (IND), described by

Relationship to Number of Components:
Hopefully, it is clear that components (with the possible exception of the first one) are not spectra of specific materials. Instead, the second component is related to the difference between spectra in the original series, the third is related to differences of the differences, and so on. But the number of meaningful components (i.e., those not associated primarily with noise or systematic errors) does provide us information about the number of degrees of freedom of the system of spectra. If a series of spectra has four meaningful components, then it must have three independent free parameters specifying the differences of the individual spectra from the average; that is, there are three parameters that differ within the set of materials.The simplest way a series of spectra could have three free parameters between them is if the materials being measured are themselves mixtures of four constituents that are present in varying amounts. And in fact, the linear combination analysis performed by Lengke et al. modeled the spectra as being a linear combination of four constituents. This is one of the major uses of PCA in XAFS: to provide an estimate of how many different constituents are present in a series of mixtures. This can be very helpful for distinguishing between linear combination fits-just think how much easier the analysis of the chromium sample in Chapter 7 would have been if we knew it consisted of three constituents! Similarly, knowing the number of constituents can help cut down the time-consuming process of constructing models (Chapter 9 and Part III).

Energy Misalignment:
Let's consider the effect that energy misalignment has on PCA. FigureTo see the effect of misalignment, we made four more copies of this spectrum and displaced them by 0.5, 1.0, 1.5, and 2.0 eV from the original spectrum. We then applied PCA to the set of five spectra.The second component of this set accounted for 93% of the variance, and is shown in Figure236 Chapter 8 -Principal Component Analysis The second component in Figure

Other structural free Parameters:
In some cases, the question as to the number of constituents is semantic. Suppose, for example, that the distribution of cations between tetrahedral and octahedral sites in a spinel (Section 6.2 of Chapter 6) gradually changes across a series. Is a spinel in which 30% of the manganese atoms are in tetrahedral sites a different constituent than a spinel in which 25% are? No, because, then we would think of a series that went from 25% to 30% to 35% to 40% as having four different constituents. Is a 50% tetrahedral sample a mixture of 0% and 100% materials? No, because the local structure of the 50% sample is not exactly given by averaging the local structure of the 0% and 100% cases.But we certainly can say that a series of manganese ferrites in which the distribution of the manganese ions between tetrahedral and octahedral sites varies has a free parameter associated with that variation, and thus it should add to the number of components. If stoichiometry also varied, so that the chemical formula was Mn x Fe 3-x O 4 , that would add another free parameter-if the difference in distribution were not solely dependent on the stoichiometry.

Coupled Constituents:
Suppose a painter were working with an ochre that included a blend of hematite and goethite, and a Prussian blue (another iron-based pigment).The artist might have blended the ochre with the Prussian blue in different proportions at different points on a painting. If that were the case, PCA performed on spectra collected from different areas of the painting would reveal two components, and thus one free parameter, corresponding to the ratio of one pigment to another. And yet, the samples would comprise three mineral phases: hematite, goethite, and Prussian blue. This kind of coupling can complicate the interpretation of PCA.

PCA fORMALIsM:
Up until this point, we've tried to motivate the ideas and uses of PCA. In this section, we'll talk a little bit about the formalism; consult the references if you'd like to know more.PCA is closely related to a technique known as singular value decomposition (SVD)-the results of SVD can easily be transformed to yield the results of PCATo briefly compare the two, suppose we arrange the normalized absorption values from our spectra in columns. Notice that we throw away the information of the energies that those normalized absorptions correspond 238 Chapter 8 -Principal Component Analysis to. It doesn't matter, for instance, if the step size changes across the scaneach measurement gets one value in the column. It does require that the different spectra be measured at the same energies or be interpolated on to the same energy grid.Each spectrum now has a column associated with it. Putting the columns together, we can form a matrix X.In PCA, we form the covariance matrix XX T (Everitt and Dunn 2010,  Malinowksi 2002, Shlens 2003, Smith 2002). The eigenvectors of this matrix are the principal components. If the data are centered, that is, if the mean of each column of X is zero, then the eigenvalues of each are proportional to the amount of variance the component explains. As Robert has noted in earlier sections, we do not usually center data in PCA analysis of XANES, and thus the eigenvalues are not proportional to the true variance. Nevertheless, the term variance is almost universally used in this context, even though the data are not centered. In this chapter, we have gone one step further and only used eigenvalues after the first to compute percentage of variance. This latter practice is not universal; much of the literature assigns a fraction of the "variance" to the first component, even though the first component is closely related to the mean of the data!In SVD, we writewhere P is the matrix consisting of the principal components expressed as columns and S is a diagonal matrix. w consists of principal components of a different kind, taken from the rows of X rather than the columns (e.g., the nth element of the first component is proportional to the average of spectrum n across all points; the nth element of the second component is proportional to how much spectrum n differs from the mean of all spectra, and so on). EquationInspection of EquationWhen the principal components are plotted, however, they are often scaled by their eigenvalues; in effect, it is PS that is plotted, rather than P. This scaling makes the first component a multiple of the mean, but not equal to it; frequently it will even come out inverted. In this chapter,

PCA Formalism 239:
We are glossing over some technicalities having to do with making the number of rows and columns in EquationThere is the potential for confusion here! Sw T provides the weighting for the normalized components, but just w T is the weighting if the components have been scaled by their eigenvalues.we have therefore applied an additional scaling factor to plots of components, so that the first component is the mean of the spectra.

CLUsTER ANALysIs:
SVD tells us how each spectrum depends on the components. This can provide suggestive information. For example, FigureComponent 2 correlates very strongly with time and presumably has to do with the reduction of the gold by the cyanobacteria. Component 3, on the other hand, is telling us there is something different with the 33-hour spectrum. Component 4 is particularly interesting, as it peaks for times of a few hours and then drops back off-perhaps there is an intermediate formed and then was consumed?In this case, we were able to plot the weightings as a function of time. In other systems analyzed by PCA, we might be able to use an analogous way of ordering our spectra-perhaps depth in a soil sample or doping fraction in a materials science problem. But sometimes there is no obvious dimension on which to rank the spectra (see, for instance, the case study in Section 16.6 of Chapter 16)-perhaps the spectra are a set of flakes from the paintings of an artist, a set of soil samples from different sites, or a set of organometallics synthesized using different protocols. Is there another way to look for patterns among the weightings?In those cases, we can plot one of the components as the x-axis, a technique sometimes known as cluster analysis. In the case of the gold series, using the second component as the x-axis would look much like the time series, because the second component is so strongly correlated with time. Instead, let's use the third component as the x-axis and see what we get (Figure

Chapter 8 -Principal Component Analysis:
Our additional scaling should be used for graphing purposes only.The weighting of the first component is not quite the same for each spectrum. If one were to apply the PCA method to unnormalized spectra, the weightings of the first component would reflect differences in normalization. Because PCA does not "understand" XAFS normalization, differences in, for example, the white line can cause modest differences in the weights associated with the first component. Figures like 8.17 are lot of fun, but be careful not to get carried away with identifying components with constituents.The fourth component suggests that there may be a constituent appearing after a few hours and then getting consumed, but that does not mean the fourth component should itself be identified with that constituent-presumably the contributions from that constituent have worked their way into other components as well.There is a cluster of spectra in the upper right, suggesting that the spectra from 2 to 20 hours form a group. The remaining spectra are scattered across the plot, suggesting they should not be thought of as being grouped together.Cluster analysis often helps suggest priorities for the time-consuming process of modeling (Chapter 9 and Part III). One model (with free parameters) is likely to suffice for the cluster of spectra in Figure

TARGET TRANsfORMs:
Often, we are interested in knowing whether specific substances are present in our series. For instance, is one of the constituents of the solutions processed by the cyanobacteria metallic gold? Although in some cases fingerprinting might be sufficient for addressing questions of that kind, it can be difficult to apply if the concentrations of other constituents are also varying from spectrum to spectrum.With PCA, however, this kind of question is remarkably easy to address. Suppose we call a standard spectrum of the constituent we wish to test for T (for "target"). We can then compute PP T T, using only the principal components that we believe contain structural information. If the target can be expressed in terms of those components, T will be left essentially unchanged by this target transform, except for alterations consistent with the systematic errors and noise present in the set of spectraFigureThe result of the target transform is suggestively close to the gold foil spectrum, but shows some small differences. Is that just due to noise and systematic error or does it mean that metallic gold is not present in the mixture? To answer that, we'll use another quantitative function developed byThe SPOIL for the target transform in FigureWe can explore a bit further. Are the cyanobacteria cranking out metallic gold from the first hours of the experiment, or are they first producing an intermediate and only later converting it to gold? To answer this, we can repeat the PCA using only the samples that aged less than 10 hours, and try the target transform again.SPOIL is not an acronym; Malinowski just likes using all caps when he defines functions!Running PCA on only the samples aged less than 10 hours yields only two components that definitely have structural information, with the third being borderline. And in fact, careful examination of FigureFigureThe transform certainly appears noisy this time, but still captures the general contours of the foil. And indeed, the SPOIL is 1.43; we expect a bit more noise when we cut down on the number of spectra, and so Malinowski's measure still confirms the presence of metallic gold.This, perhaps, is a good time to show what happens when we attempt a transform to a material that is not present. For this purpose, we'll use gold cyanide (Figure

PCA Of ExAfs:
So far, we've discussed PCA of normalized XANES. But it can be used with EXAFS χ(k) as well. For example, Beauchemin et al. (We do not, however, recommend the use of PCA with derivative spectra. Because derivative spectra sharpen features, small misalignment in energy will have a greater impact on the analysis.We also recommend against applying PCA to back transforms. PCA works best when it can extract difference between spectra; Fourier filtering tends to reduce those differences.

hOw PCA Is UsED:
PCA is most commonly used as an exploratory technique, preliminary to either linear combination analysis or modeling. In fact, this use doesn't always make it into the literature-if PCA leads you to try modeling your spectra with three constituents, and the modeling is convincing, why mention the PCA?Because it is primarily exploratory, be aggressive! Try separate PCA analyses on subsets of your spectra. Try cluster plots. Try target transforms on the whole set and on subsets.But don't overinterpret! Components aren't constituents and it's difficult to draw defensible conclusions from cluster plots. Use PCA to guide you, but try to confirm what you think it's telling you with other approaches.

fUTURE DEvELOPMENTs:
The application of PCA to XAFS is no longer a new technique-its use goes back to the 1990s-but neither has its use been thoroughly explored. The principles of linear algebra should be able to tell us not just whether a specified constituent is present (via the target transform), but how much it contributes to each spectrum. And if we know all but one constituent in a mixture, we should be able to extract the spectrum of the unknown.You may be wondering how this chapter fits in with Section 1.6.4 of Chapter 1, which spends a page on curve fitting to theoretical standards, and with Part III, which spends five chapters on it. The answer is that in this chapter we'll examine a bit more closely the theoretical underpinnings of theoretical standards and how they are used. Although the primary focus is on EXAFS, the distinction between EXAFS and XANES is not sharp, so we'll touch on some aspects of XANES as well. In the final section of the chapter, we will also provide an introduction to strategies for modeling, which in turn provides a framework for the material in Part III.

FiTTing:
In Chapter 6, we looked at the process of fitting simple mathematical functions, such as Gaussians, to data. In the process, we used a computer program to vary several free parameters (perhaps the amplitude, centroid, and width) so as to achieve the closest least-squares fit between the function and the data. The best-fit values for these parameters were used to infer information about the sample, such as oxidation state.In Chapter 7, we used empirical standards, that is, spectra of possible constituents. By allowing a computer program to vary the contribution from each spectrum, we once again achieved a closest least-squares fit to the data. In addition to the amount of each constituent, free parameters sometimes included energy shifts.These chapters teach us that free parameters serve two purposes: they allow us to compensate for unknown information (e.g., inconsistent energy calibration) and they reveal useful physical and chemical information about the sample (e.g., the relative amount of each constituent).It would be nice if we could design free parameters that corresponded in an easily understood way to the physical arrangement of atoms in our material: to bond lengths, for instance, or coordination numbers. Then we could use XAFS to help discover the structure of novel materials and not just the mixtures of known substances considered in Chapters 7 and 8.The key to doing so was discussed in Section 1.2 of Chapter 1 and is printed on the inside cover of the book: the EXAFS equation. This equation expresses χ(k) as a sum of scattering paths, some of which are single (the photoelectron scatters elastically off of a neighboring atom and then returns to the absorber) and some of which are multiple (any other sequence of elastic scattering, for example, the photoelectron scatters off of one neighboring atom, then another, and then returns to the absorber). According to our version of the EXAFS equation, each path is completely specified by three parameters and three functions of k. The three parameters N i , D i , and σ 2 i , correspond directly to physically useful concepts such as coordination number, bond length, and disorder. f(k), δ(k), and 248 Chapter 9 - Curve Fitting to Theoretical Standards λ(k), however, have to do with the detailed physics of the EXAFS process: how likely is a photoelectron to scatter off of a given atom? How much of a phase shift would it accumulate in doing so? How likely are inelastic processes to occur?In the past three decades of the twentieth century, dramatic progress was made in computing those functions of k, given a structure. And, as the EXAFS equation implies, those functions of k are fairly insensitive to small changes in the parameters N i , D i , and σ 2 i .The outline for extracting physical information about a novel structure should now be clear:1. Make an educated guess as to the structure of the sample being measured. In many cases, the sample will be suspected to be slightly different from the guess (e.g., the sample is doped while the guess is the structure of the undoped material). Some aspects of this step will be discussed in Chapter 13. The case studies in Chapter 16 also provide good examples of how this is done in practice. 2. Use theory to calculate the functions of k that appear in the EXAFS equation for each important scattering path. This is sometimes described as generating a theoretical standard. This will be discussed further later in this chapter, as well as in Chapter 10. 3. Fit the theoretical standard to the data by allowing for small changes in the parameters N i , D i , and σ 2 i . A number of chapters have additional information on this part of the process, including Chapters 10, 12, and 14. 4. If the resulting fit is poor, then the initial guess is probably wrong and a different guess should be made. If the fit is good, then that gives support to the initial guess and provides physically meaningful information such as estimates of the bond lengths and coordination numbers. The question of how we decide whether a fit is "good" or "poor" is covered in Chapter 11. 5. This process can be described formally as curve fitting to a theoretical standard. Less formal terms are modeling or fitting.9.1 Fitting 249 (Continued)

Box 9.1 ExAFS MoDELing SoFTWARE:
This might be a good time to briefly mention some of the common software packages used for modeling EXAFS. The list is not exhaustive, but it does include the most commonly used software. Citations are to papers describing the methods used by each package and don't always refer to the most recent version. More comprehensive lists, which also include software for data analysis and related tasks, can be found at xafs.org/Software and at www .esrf.eu/computing/scientific/exafs. A comparison chart including the most popular packages is also provided in Appendix.

ThEoRETiCAL STAnDARDS:
The calculation of theoretical standards for EXAFS has become quite sophisticated. For the most part, the computation of a theoretical standard can be treated as a black box: if you enter the positions and species of atoms in a structure, software will produce estimates of f(k), δ(k), and perhaps λ(k) for a set of important scattering paths. If you would like to understand more about how they do that, good places to start are the review article byNevertheless, it's helpful to know about some of the approximations that underlie the theory.

Muffin-Tin Potentials:
When theoretical standards for EXAFS are generated by feff, excurve, or gnxas, atoms are usually treated as creating spatially localized, spherically symmetric potentials. These potentials may be treated as not quite touching each other, barely touching, or overlapping, but in any case that feffA "lite" version is freely available; complete versions require a license. Many pieces of software have been written to use those empirical standards in modeling:artemiseda (Kuzmin 1995).exafspak: Developed by Graham George at the Stanford Synchrotron Radiation Laboratory.lasemaxsixpackwinxasxdapviperexcurvegnxaswill leave some space not assigned to any atom. The potential in this space is assumed to be constant.The result is referred to as a muffin-tin potential. FigureRight on both counts, Simplicio. But how big an azimuthal dependence of the potential do you expect? And how much do you expect the potential to vary in the interatomic region?Well, bond energies are typically a few electron volts. I'd expect azimuthal dependence to be that order of magnitude. And I guess variation in the interatomic region would need to be less than that.OK. So let's see how big a deal those effects are. Let's say the EXAFS region starts 30 eV above the edge, which is a bit less than k = 3 Å -1 . Using the approximation E ≈ 4k 2 (with E in electron volts and k in inverse angstroms) and a bit of calculus, I can write dE ≈ 8k•dk.That means an error of 2 eV at 3 Å -1 corresponds to about 0.08 Å -1 . As Robert argued in Section 4.5.8 of Chapter 4, that's probably not enough to matter. with a constant height surface in between. This ice tray, along with most muffin and cupcake tins, thus provides a two-dimensional analog of a muffin-tin potential.

Final State Rule:
Theoretical standards are usually calculated using the final state rule; that is, the absorbing atom is treated in the calculations as if the photoelectron had been removed from it and then the resulting ion allowed to reach a relaxed state, but with the hole corresponding to the missing electron still present.In reality, however, the incoming x-ray doesn't interact with the atom for just an instant. (If it did, then Heisenberg's uncertainty principle tells us that it wouldn't have a definite energy.) Instead, the electric field from the x-ray polarizes the atom, changing the potential around it and placing the resulting ion in a state different from the relaxed state assumed by the final state rule. This is called a local field effect

Losses:
There are a variety of phenomena that cause EXAFS signals to be smaller than typical theoretical calculations predict. Among those are the limited lifetime of the core-hole, inelastic scattering of the photoelectron, multielectron processes, and incomplete overlap of the final and initial states of the absorbing atom.252 Chapter 9 - Curve Fitting to Theoretical Standards Box 9.2 FULL PoTEnTiAL METhoDS (Continued)"Probably" not enough to matter, yes. But Dysnomia's calculation shows us that at the lower end of the EXAFS region, non-muffin-tin effects are just on the verge of being important. Since small difference in potential becomes more noticeable at lower photoelectron energy, this suggests that non-muffin-tin effects can be important in XANES calculations.In fact, full potential methods-that is, methods that use non-muffin-tin potentials-have been developed for the XANES regionThe distinction between "XANES" and "EXAFS" is an artificial one. The muffin-tin approximation becomes increasingly accurate at higher photoelectron energies and is thus more suspect at the low energies we call XANES than at the high ones we call EXAFS.Sections 1.2.10 and 1.2.11 of Chapter 1 introduced us to the S o 2 and λ(k) parameters used to account for these effects. While progress is being made in understanding the theory of these processes, the form in which they appear in the EXAFS equation arose from phenomenological considerations. For example, it was observed that experimental data showed smaller EXAFS oscillations than theory was predicting, necessitating an S o 2 factor. Likewise, the discovery that EXAFS, unlike diffraction, was not dependent on long-range order necessitated the introduction of a mean-free path λ(k) for the photoelectron.Because these parameters are standing in for a rather complex array of phenomena, we will defer further discussion until Chapter 10.

Convergence:
Implicit in the use of the EXAFS equation is the hope that an EXAFS spectrum can be described by a manageable number of paths. Section 1.2.7 of Chapter 1 explained that these include single-scattering paths, in which the photoelectron scatters off of a neighboring atom and returns to the absorbing atom, and multiple-scattering paths, in which there is more than one scattering event.Suppose, then, that we had an isolated diatomic molecule, such as bromine monochloride (BrCl) gas and that we measure the bromine K-edge. Each bromine atom has exactly one neighbor; there is no consistent structure beyond that. There is, therefore, only one single-scattering path: Br → Cl → Br. But there are an infinite number of multiple-scattering paths: the photoelectron could, for instance, bounce back and forth between the two atoms, following the path Br → Cl → Br → Cl → Br, as in FigureIntuitively, a path of that sort will contribute less to the EXAFS signal than the direct-scattering path, simply because it involves additional scattering events each of which occurs with some probability. In addition, the multiple-scattering path we've described is longer than the single-scattering path and is thus reduced by thefrom the EXAFS equation. Higher-order multiple-scattering paths, involving more bouncing back and forth, will have even larger values of D. The exponential dependence on the mean free path, therefore, assures that the EXAFS spectrum will be well described by a small number of paths in this simple case.In contrast, consider the more common example of a solid. A typical length for a mean free path in the EXAFS region is 10 Å (Section 10.2.5 in Chapter 10), so we would have to consider scattering off of hundreds of different atoms. For simplicity, suppose there were a hundred atoms within range. There would then be a hundred possible direct-scattering paths, but how many multiple-scattering paths would there be? Each of the hundred scatterers could be paired with any of the other 99 atoms for a second scattering event, giving us nearly 10,000 possibilities for double scattering. Some of those paths would be quite long-for instance, traveling to one side of the cluster and then all the way to the opposite side of the cluster, and then back to the absorbing atom. Since we've chosen the radius of the cluster to be about the mean free path of the photoelectron, those long multiple-scattering paths would contribute very little. But most of the double-scattering paths would be shorter than the longest single-scattering path in our cluster and would thus need to be considered. Once we consider triple scattering, we're thinking about hundreds of thousands of paths. (FigureFortunately, it turns out that the vast majority of those paths contribute very little to the EXAFS spectrum254 Chapter 9 - Curve Fitting to Theoretical Standards For those who might be curious, Figure• The sheer number of low-amplitude multiple-scattering paths means that all phases are likely to be well represented at each value of k. The statistics of this kind of combination is similar to counting statistics (Section 3.3.3 of Chapter 3). N paths of amplitude A will tend to sum to a signal of amplitude roughly N A . Thus, 10,000 small paths end up contributing an amount roughly equal to 100 times what a single one of them would.• In most cases, each additional scattering event lowers the amplitude of the event because only part of the photoelectron wave is scattered at each atom (see Box 9.3 for an exception). Thus, each double-scattering path tends to be less important than an otherwise similar single-scattering path, triple scattering less important than double scattering, and so on. On the basis of this aspect and path length considerations alone,• The e k -2 2 2 σ disorder factor tends to affect multiple-scattering paths more strongly than otherwise similar single-scattering paths. For example, thermal disorder will cause each scattering atom to vary a bit in position; the more scattering atoms we add to a path, the greater the variance in the path length. Thus, while it is difficult to know a priori exactly how many paths will be necessary to accurately reproduce a spectrum of a given species, the number is unlikely to exceed a few hundred.

The Path Expansion 255 Box FoCUSED PAThS:
Sometimes, an additional scatterer can enhance the amplitude of the path! Consider the multiple-scattering path shown in FigureNote, friends, that paths such as these are also focused: In addition, fitting is often done on just a portion of a Fourier transform (Section 4.5 of Chapter 4), allowing the exclusion of paths with lengths outside of a chosen range.

Full Multiple Scattering:
While EXAFS is generally well described by a manageable number of paths, the same is not necessarily true of XANES. Since the photoelectron has less kinetic energy in the XANES region, the probability of scattering off of each atom is greater, and thus the additional scattering events in multiple-scattering paths do less to suppress the contributions from those paths. In addition, the effect of disorder is less since the factor is weighted by k 2 . Finally, Fourier filtering is not generally applied to the XANES region, in part because of the difficulty of background subtraction. Taken together, these effects mean that in the XANES region the path expansion may not converge wellEven if the path expansion fails to converge, that doesn't necessarily mean it's useless. It could be, for instance, that a manageable number of paths could account for 90% of the variations in a XANES spectrum, but that an infinite number of paths would be necessary to account for 98%. And depending on what you're doing, that 90% might be good enough.256 Chapter 9 - Curve Fitting to Theoretical Standards

Box 9.4 FULL MULTiPLE SCATTERing:
Computers keep getting more powerful, and theorists are really smart. I bet it won't be long until it's no big deal to perform full multiple scattering all the way through the EXAFS region.I'd take the other side of that bet, Simplicio. The computational power required goes up very rapidly with the energyAnd even if we could use full multiple scattering for the whole EXAFS spectrum, we wouldn't want to. The path expansion and the EXAFS equations are nice because they give us a window into what is physically going on in our material: how far apart atoms are, how much disorder there is, how many atoms there are. It is easy, with the path expansion, to see qualitatively and quantitatively the effect of, for example, changing out one ligand or stretching one bond length a little bit. Each individual contribution to the complete spectrum can be identified, graphed, and modified. Full multiple scattering takes away that ability; you put a proposed structure in, and a spectrum comes out, but you don't really know what features in the spectrum correspond to what aspects of your structure. In the regimes where the path expansion works well, this makes it preferable to a full multiplescattering calculation.In recent years, a number of software packages have been written that allow for XANES modeling to be done with full multiple-scattering computations, including fdmnesFortunately, the lower energy of the photoelectron in the XANES region allows for full multiple-scattering calculations to be practical, that is, methods that calculate total spectra directly rather than relying on the enumeration of individual scattering paths

FiTTing STRATEgiES:
While 50 or 100 paths are more manageable than billions, it still means there are a lot of parameters that could potentially be fit. Each path needs to be assigned a value of N, D, and σ 2 . If each of those were treated as a free parameter to be found by least-squares fitting for each

Fitting Strategies 257:
They do this by computing the spectrum for one structure (in the case of fitit, this calculation is performed by other software such as feff), modifying the structure a little bit, computing the new spectrum, and seeing if the result is closer to the data. If it is, the structure is modified a bit more in the same direction; if not, the opposite direction is tried. While computationally expensive, Simplicio is right that modern computing power and clever interpolation schemes makes this kind of thing possible.It's still not the same thing as the path expansion-it's more like a high-powered version of the kind of fingerprinting we did in Chapter 6. It doesn't allow us to directly point at a feature in the spectrum, for instance, and say "this feature is due to the secondshell oxygen." Instead we can change the oxygen to another element, or move it, or do something else like that, and observe where the spectrum changes. Furthermore, the path expansion handles modest disorder easily through the σ 2 parameter, something full multiple-scattering doesn't do.That's kind of the nature of XANES, though. This isn't a theory problem waiting for a breakthrough. When we Fourier transform EXAFS data, it kinda-sorta looks like a radial distribution function and it's that correlation that the path expansion is exploiting. That doesn't happen in XANES. I know you keep saying that there isn't a sharp distinction between XANES and EXAFS, Carvaka, but if it takes a million paths to reproduce some XANES feature well, then I'm OK with saying "the path expansion doesn't work in that case because it's XANES." Insisting the underlying physics is the same is like saying that there's no real difference in transportation across town or across the country. Sure, it might be theoretically possible to fly a plane across town or to walk across the country, but no one would do either of those things except as a stunt. So full multiple scattering is useful for XANES because the path expansion might be impractical, but the path expansion is better for EXAFS because it gives you more insight into what is actually going on.While we're on the subject, let me emphasize that, despite the similar names and the similar regimes in which they are useful, full potential and full multiple scattering are two completely different things. It is quite possible to make full multiple-scattering muffin-tin calculations or use a full-potential path expansion! significant path, there could easily be hundreds of free parameters. While we'll leave a careful discussion of the statistics of EXAFS fitting until Section 11.1 of Chapter 11, our discussion of linear combination fitting in Chapter 7 should warn us that we are unlikely to be able to fit that many variables successfully. We need to do something to constrain our fits! Broadly speaking, there are two common strategies that are used.

Bottom-Up Strategy:
In the bottom-up strategy, you start with a very small number of paths, often only one or two. This can be done by • Fitting only the part of the Fourier transform that is dominated by nearest-neighbor paths (i.e., single-scattering paths from the absorbing atom to one of the nearest-scattering atoms)• Assuming all nearest-neighbor atoms are at the same distance • If consistent with what is known about the sample, beginning with structures where all the nearest neighbors are the same species By doing this, you will likely be left fitting a single path: the singlescattering path off of the nearest neighbors. N, D, and σ 2 for that path can then be determined by fitting. This is known as a single-shell fit.Single-shell fits can be a good place to start, but they often leave a lot of data on the table. Therefore, the goal in the bottom-up strategy is to start with the simple nearest-neighbor fit and then gradually add more information.How to do that depends on the result of the single-shell fit and on your preexisting knowledge about the system. Perhaps, for instance, the fit reports a larger value of σ 2 than is usual for the kind of bond being fit. Perhaps, there is also discussion in the literature about the particular system that you are studying, debating whether or not there are two different bond lengths for the near-neighbors, such as an equatorial and an axial length. If there were two different lengths, then your initial fit (which only allowed for one length) would likely report a large σ 2 , since that is a measure of the variance of the absorber-scatterer distance. In the case just described, you could then proceed by trying to fit two paths, allowing for the scatterers to be at slightly different distances.Likewise, a poor fit might suggest you have the identity of the scatterers wrong; perhaps you tried all oxygen atoms, but in reality some of the oxygen atoms were substituted by sulfur atoms.If, on the other hand, the single-shell fit was good, but the Fourier transform shows data at higher R than the range you fit, you could try to expand the fitting range and incorporate paths corresponding to more distant atoms.

Chapter 9 - Curve Fitting to Theoretical Standards:
Single-shell fits can be a good place to start, and sometimes they can be helpful when used on a series of spectra; for example, one sample measured under different conditions or a set of related samples. But it is not much of an exaggeration to say that a single single-shell fit on a single sample under a single set of conditions is worthless on its own. Curve fitting to theoretical standards has too many potential sources of systematic error to blindly trust an isolated result. But once you start to extend the fit, or to make it part of a series, your confidence can grow.Hopefully, information soon starts to emerge that dovetails with your preexisting knowledge in some way. For example, you might find that your nearest neighbors are nitrogen atoms, and you might also have a prior reason for suspecting that there are ethylenediamine (NH 2 CH 2 CH 2 NH 2 ) ligands in your system. If that were the case, you might hypothesize that each pair of nearest-neighbor nitrogen atoms was attached to a pair of carbons further out; those carbons might show up in the EXAFS. Thus, the degeneracy N of the second-nearest-neighbor path to carbon could be required to be the same as the coordination number for the nearestneighbor path to nitrogen. A requirement of this type that is imposed during fitting is called a constraint. The list of paths and constraints you are using during a fit comprises the model you are using.Continuing in this way, we could gradually add more paths and more constraints, gradually building up a detailed model of our material.

Top-Down Strategy:
You may have noticed that the bottom-up approach starts with very few assumptions-you may not even know with confidence what the nearestneighbor atoms are. In contrast, the top-down strategy works well when you are looking for modifications to a well-known structure or to choose between a handful of structural possibilities.For this strategy, begin with a complete structure, including multiple shells of scatterers. For instance, if you were working with a thin-film-doped zinc oxide, you could begin with the structure of bulk zinc oxide. This would mean including many paths in your model-perhaps as many as a hundred.To keep the number of free parameters down, the initial model would be highly constrained. For example, you might assume that all the atoms in your sample occupy exactly the same positions as they do in the bulk, and with the same coordination numbers. You might further make simplifying assumptions regarding thermal disorder, requiring many paths to adopt the same value of σ 2 . Your first fit, therefore, while including many paths, would only have a few free parameters.If you're trying to decide between a few alternatives, such as two completely different allotropes of a crystal, then a highly constrained fit may be enough to provide you your answer. But frequently, you expect your material to differ from the structure in your initial model in some wayperhaps local distortions, or vacancies, or subtle changes in symmetry. If that's the case, then you'll need to relax your model, bit by bit, to allow for those possibilities. Randomly distributed oxygen vacancies could be modeled, for instance, by replacing the constraint that all oxygen coordination numbers must be the same as in your starting structure with the constraint that all oxygen coordination numbers must be reduced by the same unknown percentage from those in the starting structure. In this way, a free parameter is added to the fit.

Fitting Strategies 259:
It is important to note that the bottom-up and top-down strategies end up in the same place: a multiple-path fit with enough free parameters to reveal the desired structural information about the sample, but enough constraints to keep the problem from being statistically untenable. The bottom-up approach accomplishes this by beginning with few constraints and few paths, and gradually adding both. The top-down approach, in contrast, starts with many paths and strong constraints, which are then gradually relaxed.

Common Fitting PArAmeters:
There are five parameters related to the EXAFS equation (on the inside front cover!) that are frequently allowed to vary when fits are performed: the half path length D, the degeneracy N, the mean square relative displacement (MSRD) σ 2 , the amplitude reduction factor S o 2 , and E o . We'll deal with those first.

Physical interpretation:
For single-scattering paths, the half path length is simply the average distance between the absorbing and scattering atoms. It's half the path length because a single-scattering path must travel from absorber to scatterer and back.There are two reasons we specify that it is an average. First, individual atoms move around due to thermal motion, and the distance between a particular absorbing atom and a particular scattering atom varies a bit. This is called thermal disorder. But it is also true that not all absorberscatterer pairs represented by the path are necessarily identical. There may, for instance, be defects such as vacancies or dopants in the material. There may also be phase boundaries or surfaces. In addition, you might be modeling a distorted shape as if it were more symmetrical, perhaps choosing not to make a distinction between equatorial and axial atoms. These are examples of static disorder.For multiple-scattering paths, the half path length is just what it sounds like: half the average length of the scattering path. It is defined in this way to make it consistent with the definition for single-scattering paths. Note that a multiple-scattering path may have a value of D that is greater than the distance from the absorber to any of the individual scattering atoms.266 Chapter 10 -A Dictionary of Parameters

Box 10.1 AVerAge DistAnCe is not tHe sAme As DistAnCe BetWeen AVerAge Positions:
EXAFS can allow us to find the average distance between atoms and XRD can give us their average positions. But the average distance between atoms is not quite the same thing as the distance between their average positions. To see why, consider three kinds of relative motion a pair of atoms could make:1. Perfectly correlated motion, in which they both move the same distance in the same direction (FigureOne should not use the term bond length too freely. For example, an oxygen atom 4 Å from an iron atom is not bonded to it! You may have noticed that we treat the EXAFS process as if it is instantaneous relative to any atomic motions that occur. This is equivalent to saying that EXAFS photoelectrons travel much faster than atoms undergoing thermal motion in a material. Room temperature thermal motion corresponds to a kinetic energy of about 1/40 of an electron volt. Thus, even 10 eV above the edge, a photoelectron has roughly 400 times the kinetic energy of the atom it scatters off of. Add to that the fact that even a hydrogen atom is about 1800 times heavier than an electron, and you can see that the scattering atom is essentially stationary relative to the electron that scatters off of it.

Typical values:
As a scientist, studying whatever it is that you study, you know much more about what the possible distances between atoms in your samples are than we do! It is worth noting, though, that if a fit yields a value of D that is more than about 0.1 Å different from the value in the standard you are using, then the standard is not entirely appropriate. If you think the fit is on the right track, then you should get a new standard. For theoretical standards, that simply means redoing the ab initio calculation with atomic positions more similar to those found by your fit.

Effect on χ(k):
The larger D is, the smaller the spacing of the peaks for that path in χ(k). 2. Anticorrelated motion along the line joining them, in which the atoms alternately move toward and away from each other (FigureAny relative motion can be made up of combinations of those three types. In two of them, the EXAFS and XRD distances are the same, but in the third, the EXAFS distance is larger. Therefore in a real pair of atoms, the EXAFS distance is larger than the XRD distance.This effect is fairly small and often not explicitly discussed in EXAFS analyses. But it can be important to think about when comparing spectra for which both interatomic distance and thermal disorder differ significantly, such as a temperature series designed to measure thermal expansion.

Effect on Fourier transform:
The magnitude of the Fourier transform will show a peak centered a few tenths of an angstrom below the value of D. For a well-isolated peak, changing D will shift the peak by an amount about equal to the change (FigureThe real part of the Fourier transform won't generally show a single peak, but it is affected by shifts in a similar way (Figure

Common constraints:
If your material is cubic, one highly constrained option is to assume that the entire structure is expanded or contracted uniformly relative to the standard, that is, each half path length differs by the same percent from the standard.Other constraints for half path lengths depend on your knowledge about the material and how it behaves. For instance, you might have a ligand that is relatively stiff, but not know how it is oriented. It would then be possible to use geometry to parameterize the distances from the absorbing atom in terms of a small number of free variables that describe the 268 Chapter 10 -A Dictionary of Parameters orientation. Another example is a material for which you know the longrange structure from x-ray diffraction. In that case, you might suspect local distortions that need to be fit, but constrain them in such a way that the average corresponds to the XRD data.

Correlations:
Half path length can correlate to a number of other parameters, depending on the particular model you are using. In simple fits, it often shows a substantial correlation to E o . But the half path length of particular paths may also correlate to just about any fitted parameters, including the MSRD, degeneracies, and the half path lengths of other paths. This is because, as Mr. Handy pointed out earlier in this section, the amplitude of a peak including contributions from more than one path may be affected by a shift in just one of them. Thus, even though D is primarily a phase parameter, in complicated models, it can also correlate strongly with amplitude parameters.

Degeneracy:
10.1.2.1 Symbol N 10.1.2.2 Nomenclature For single-scattering paths, the degeneracy is often referred to as the coordination number.

Physical interpretation:
N is simply the identical number of distinct ways, per absorbing atom, that the scattering defined by the path can take place. For single-scattering paths, it's therefore the number of atoms of a given type at a given distance: if there are six carbon atoms at a distance of 4.7 Å from a uranium absorber, then the degeneracy of that path is 6.To see how degeneracy works for multiple-scattering paths, imagine a manganese atom tetrahedrally coordinated to four identical oxygen atoms. The shortest possible multiple-scattering path is one that goes from the absorbing atom, to one of the oxygens, on to another oxygen, and then back to the absorber. There are four possible oxygen atoms for 10.1 Common Fitting Parameters 269 the first leg. In each case, there are then three possible oxygen atoms to pick from for the second leg. Thus, there are 4 × 3 possibilities contributing to the path, giving a degeneracy of 12.Consider what happens if there is more than one phase present and the scattering path is only associated with one of the phases. For instance, imagine a mix in which 30% of the iron atoms are present as iron phosphide and 70% are present as iron oxide. Each iron atom in iron phosphide has six phosphorous near neighbors, but only 30% of the iron atoms are in the phosphide. Therefore, the degeneracy for the ironphosphorous near-neighbor path is 30% of 6, or 1.8.Similarly, nanoscale materials often exhibit reduced degeneracies, because atoms near the edge of a nanoparticle or nanocrystal are "missing" some of the nearby atoms, lowering the average degeneracy per absorbing atom.A little thought about your system may yield additional causes for degeneracies of individual paths to be lower than in the equivalent pure bulk material.

Typical values Degeneracies must never be negative.:
For single-scattering nearest neighbors, chemical sense limits the number of coordinated atoms. There aren't many oxides with more than eight oxygens coordinated to the metal atoms, for instance, nor do metals in alloys usually have more than twelve nearest neighbors. Degeneracies for multiple-scattering paths, however, can often be quite high. 96, for instance, is the degeneracy of some multiple-scattering paths found in face-centered cubic (fcc) crystals.Degeneracies can also be quite small, because they are calculated per absorbing atom. A zinc oxide doped with 5% aluminum, for instance, might very well have a degeneracy of less than 0.20 for a particular zincaluminum single-scattering path.

Effect on χ(k):
The amplitude of the path scales with the degeneracy; increasing the degeneracy of a path from 4 to 6 increases the amplitude of that path by a factor of 6/4 (Figure

Effect on Fourier transform:
Again, the amplitude of the path scales directly with degeneracy (Figures 10.8 and 10.9).

Common constraints:
Degeneracies should be constrained in a manner that is consistent with your understanding of the material.If, for instance, you are working with a bulk crystal, the degeneracies are given by the crystal structure and may not need to be fit.As another example, consider a platinum-copper fcc alloy. The total number of nearest neighbors in an fcc compound is 12. But there might be a 270 Chapter 10 -A Dictionary of Parameters Theorists refer to that as time-reversal symmetry, Mr. Handy.Note that if three atoms are involved in a multiplescattering path, then going 1 → 2 → 3 → 1 counts separately from going 1 → 3 → 2 → 1! For that reason, doublescattering paths have even degeneracies.question as to whether the metals show clustering-do platinum atoms tend to be near other platinums, perhaps as a consequence of the synthesis technique, or do they tend not to be nearest-neighbors, perhaps because of strain in the lattice? In such a case, the degeneracy of the platinum-platinum near-neighbor path and the degeneracy of the platinum-copper nearneighbor path could be made to sum to 12. Although neither degeneracy is known individually, only one independent parameter is needed to fit the degeneracies of these paths, rather than two.

Correlations:
Degeneracies can correlate strongly to other "phase" parameters, such as the amplitude reduction factor and the MSRD for the path. The degree to which this is the case depends in part on how highly constrained the 272 Chapter 10 -A Dictionary of Parameters

Box 10.2 An exAmPLe oF A DegenerACY ConstrAint:
Friends! Let us consider an example of a copper atom with two nitrate ligands (right structure in FigureBut what if we didn't know the number of nitrate ligands, but did know that the ligands were some mix of sulfur and nitrate? The copper might have only one nitrate ligand, for instance, as in the right structure in FigureBut no matter how many nitrate ligands there were, the degeneracy for the nitrogen path would always be equal to the degeneracy of the near-neighbor oxygen path, and the double-scattering path would always have a degeneracy twice that.So rather than three independent free parameters (the degeneracy of the near oxygen, the degeneracy of the more distant nitrogen, and the degeneracy of the double-scattering path), we only need to have one, because if we know one of those coordination numbers, then we know the other two. fit is. A fit that uses knowledge about the chemical system to reduce the number of free parameters related to degeneracy to a handful, for instance, may show only weak correlations. At the opposite extreme, it is impossible to fit both coordination number and S o 2 if only a single shell is being fit-they are perfectly correlated!

Physical interpretation:
The MSRD is the variance in the half path length, that is, the square of the standard deviation of the half path length:As described under the entry for half path length, an individual path will have contributions from scattering pairs that are farther apart or closer than the average because of static and/or thermal disorder. The MSRD is therefore a measure of both kinds of disorder.As a practical matter, σ 2 , also serves another purpose: it is used to correct for the difference between the experimentally determined and theoretical χ(k) functions that were discussed in Section 4.4.2 of Chapter 4. It turns out that the necessary correction is equivalent to an additive term to the MSRD

Typical values:
Because the MSRD is the square of a standard deviation, it must be positive. Typical values range from about 0.002 to 0.03 Å 2 . If an MSRD adopts a value much larger than that range, the path is so disordered that it is contributing little to the fit-often a sign that the model is wrong in some way. σ , the effect is much more pronounced at high k (Figure

Common Fitting Parameters 273:
The term Debye-Waller factor is borrowed from x-ray diffraction. But in XRD, it refers to the variance in the position of an atom relative to its mean position in the lattice, whereas in EXAFS, it is the variance in the distance between two atoms. To see how this can make a difference, imagine a pair of near-neighbor atoms. Long wavelength vibrations will tend to move both in the same direction, so that the distance between them doesn't change very much. The EXAFS Debye-Waller factor may therefore be much smaller than the XRD factor. On the other hand, if two atoms are well separated, they may move completely independently. In that case, the EXAFS factor will be roughly twice that of XRD, because each atom's motion contributes to the variation in the distance between them in an uncorrelated fashion.If σ 2 is much smaller than (1/k max ) 2 , where k max is the top limit of the range you are trying to fit, there's a good chance the distribution is roughly Gaussian to within the accuracy of your fit. If it's bigger than that, consider trying a third cumulant or splitting the path into paths at different distances!

Effect on Fourier transform:
The primary effect a change in MSRD has on the Fourier transform is to change the amplitude of that path (FigureThere's also a slight shift to lower R with increasing MSRD. This is because both the 1/D 2 and the mean-free path factors in the EXAFS equation cause high-R contributions to be weighted somewhat less heavily than low-R contributions. Thus, as the distribution is broadened by increasing the MSRD, the new low-R contributions are weighted more heavily than the new high-R contributions, and the peak shifts slightly to the left (Figure10.1.3.7 Common constraints A wide variety of constraint schemes have been employed with MSRD's, some of which will be treated further in Chapter 14. A simple heuristic scheme is to constrain "similar" paths to have the same MSRD, with similarity defined by length and/or species of scattering atom. Other constraint schemes use simple physical models such as those of Einstein or DebyeFigureIt is also worth noting that the area under the curve is much smaller, even though the number of scatterers is the same. This isn't surprising, as the functions at the low-D end of the distribution are not completely in phase with the functions at the high-D end of the distribution. This is another reminder that the Fourier transform is not a radial distribution function, as for a radial distribution function, the area under the curve would be proportional to the number of scatterers! some use the results of calculations such as those employed by density functional theory.

Correlations:
Because MSRD primarily affects the amplitude of a path, it can correlate strongly with other amplitude parameters such as coordination number. If the path is particularly prominent in the fit, its MSRD may also correlate significantly with S o 2 . For fits with more than one free parameter affecting half path lengths, there may also be strong correlations with those parameters.Because MSRD is the only parameter appearing in the EXAFS equation weighted by k 2 , it is often possible to reduce correlations with other parameters by using a wider range of k data or by fitting multiple k-weights simultaneously.

Physical interpretation:
To understand the physical interpretation of S o 2 , we need to understand a little about its history.As the first successful theories of EXAFS were developed in the early 1970s, it was recognized that experimental EXAFS spectra showed oscillations with roughly half the amplitude the theories predictedAlthough theorists were willing to shoulder much of the responsibility for the disagreement between theory and experiment, in some cases, 10.1 Common Fitting Parameters 275 experimental effects contributed to the reduced amplitude as well. As we saw in Chapter 3, inhomogeneity and harmonics both act to suppress the amplitude of EXAFS oscillations for transmission measurements, while self-absorption reduces the oscillations for fluorescence mode.From the beginning of modern EXAFS theories, it was recognized that, all else being equal, the more distant an atom was from the absorber, the less it contributed to the EXAFS signal. In addition to the usual 1/D 2 dependence, this was correctly attributed both to phenomena that could disrupt the electron's wave function (such as inelastic scattering) and to the fact that the core hole in the absorbing atom would eventually get filled. These effects were modeled by assigning a mean free path to the electron, and thus introducing a factor into the EXAFS equation of the form e D k -2 λ( ) . These are sometimes referred to as extrinsic losses, with the idea that they are caused by events subsequent to the formation of the core holeThat left intrinsic losses: those that had to do with the formation of the core hole, and thus did not depend strongly on path length. For example, once the photoelectron is ejected from the absorbing atom, the remaining passive electrons adjust to the presence of the core hole left behind. This adjustment means the overlap between the initial and final quantum states is imperfect, and this suppresses the EXAFS oscillations somewhat. Other intrinsic losses are due to shake-up processes, in which outer electrons are excited when the core hole is formed, and shake-off processes, in which outer electrons are ejected from the atom. Most x-ray absorption events do not include shake-up or shake-off processes, and in those cases, the energy of the photoelectron is uniquely correlated to the energy of the incoming photon, a fact we take advantage of during data reduction by converting from the energy of the x-ray to the wave number of the photoelectron. But if a shake-up or shake-off event does occur, then that robs the photoelectron of energy, and the k we calculate is wrong. Because there are many different shake-up and shake-off processes that can occur, the net effect is that the amplitude of the EXAFS oscillation is smaller than what we would measure in the absence of these processesTheoretical investigations of intrinsic processes are ongoing. Although it is beyond the scope of this text to go through the arguments, it turns out that intrinsic processes result in a reduction of amplitude that does not vary much over the ranges of k and R used in EXAFS analysis. This reduction in amplitude is generally on the order of 10%Examining the EXAFS equation, you can see that S o 2 represents a reduction in amplitude that is independent of k and R. A good physical interpretation, therefore, is that it is an approximation of the effect of intrinsic losses on the EXAFS spectrum.

Chapter 10 -A Dictionary of Parameters:
The distinction between intrinsic losses due to imperfect overlap between initial and final states in the absence of excitation, and losses due to shake-up and shake-off processes, is somewhat artificial. The fact is that when the photoelectron is removed, the new orbitals of the atom do not correspond exactly to the old orbitals. According to the rules of quantum mechanics, the passive electrons will occupy the new orbitals with a probability related to the degree of overlap with the original orbitals. If all of the passive electrons end up in the new orbitals that have the most overlap with the originals, we tend to think of the atom as having "relaxed." If any end up in orbitals with less overlap, we talk about the atom having been "excited." Aside from the fact that in the first case, the energy of the passive electrons is modestly reduced, whereas in the second case, at least one of the passive electrons has its energy increased, the physics of the processes is the same.

Common Fitting Parameters 277:
In practice, determinations of S o 2 are very sensitive to measurement and data reduction issues such as harmonics, self-absorption, sample inhomogeneity, detector nonlinearity, and incorrect normalization. Thus, S o 2 can act as a "canary in a coal mine," with low values signaling that the spectrum may also be distorted in other ways.

Effect on χ(k) Under the usual assumption that S o:
2 is independent of k and R, its effect on χ(k) is simply to change the amplitude uniformly (Figure

Effect on Fourier transform:
Once again, if we assume that S o 2 is independent of k and R, its effect on the Fourier transform is to change the amplitude uniformly

Common constraints Because S o:
2 is due to intrinsic effects, it should be the same for every scattering path associated with a given absorbing atom, at least for a given value of k and R.

Box 10.3 is S o 2 inDePenDent oF k AnD R?:
Some leading theorists argue that intrinsic effects are weakly dependent on both k and RSure, Carvaka, but for most EXAFS measurements, any k or R dependence is likely to be less than a 5% difference between paths, or between low k and high k. So almost all EXAFS analyses based on modeling ignore this issue.Alternatively, one can treat S o 2 as a phenomenological parameter that accounts for any amplitude suppression independent of k and R, regardless of physical cause278 Chapter 10 -A Dictionary of Parameters -5 But what if the same element is present in different environments? For instance, you might be comparing silver metal and silver oxide. Should both be given the same value of S o 2 ? This is the question of chemical transferability and is a matter of ongoing investigation. Given that most materials are expected to exhibit values of S o 2 in the range 0.8 to 1.0, the variation between environments of a given element in a given measurement should be at most 0.2. Measurement, data reduction, and fitting effects make it difficult to determine to an accuracy much better than that. Therefore, we recommend initially considering S o 2 to be chemically transferable between different environments of the same element. But it's also reasonable to try allowing each environment to have its own S o 2 , and see if that results in a statistically significant improvement to the fit.On the other hand, much greater caution should be exercised before assuming that different edges of a sample should have the same S o 2 , even when those edges refer to the same atoms, such as in the case of a K and an L edge. Not only is the electronic structure of the resulting core hole very different, but measurement effects may also vary considerably with energy.But what does it correspond to physically?Although it's difficult to ascribe a rigorous physical meaning to E o , differences in E o , even when smaller than 1 eV, can still be meaningful. Oxidized materials, for instance, typically have E o 's an electron volt or two higher than the corresponding element, because the electrons are held more tightly.10.

Common Fitting Parameters 279:
We've taken care in our discussion of S o 2 , in part because there's a good possibility the theory will progress after this book is published. It is generally best for us to start out fits by treating it as independent of k, R, and chemical environment, but to allow it to be different for different edges. In most cases, that will be good enough even for our final fits, but it's good to be aware that we are making some simplifying assumptions when we do that.10.1.5.5 Effect on χ(k) Shifting E o results in a shift of χ(k) in the same direction that is most evident at low k (Figure

Box 10.4 WHAt Does E o CorresPonD to PHYsiCALLY?:
That's easy. Below E o , the core electron doesn't absorb any photons. E o is just the edge energy.But, my dear Simplicio, that won't work. Edges do not start abruptly. In Section 4.3.3 of Chapter 4, we discussed some arbitrary choices that can be made for E o at the data reduction stage, promising that we would leave it as a fitting parameter later.In fact, there may not even be a unique E o . If channels including shake-up and shake-off events do contribute significantly to the spectrum, then some photoelectrons have different energy origins than others. In addition, theoretical standards are usually based on a muffintin potential (Section 9.2.1 of Chapter 9), which is certainly a simplification of the actual interatomic potential. If we've got the potential the electron is traveling through a little bit wrong, it means we also have E o a little bit wrong.E o also doesn't have to be precisely the same as any of the usual energy levels tabulated for a material; it's not quite the same thing as the Fermi energy, or the ionization energy, or the energy of the conduction band.But the bottom line is that it WORKS! In the EXAFS region, the spectrum behaves as if there are electrons with a well-defined momentum, meaning E o is well defined too. some evidence that theoretical standards may not always account properly for charge transfer, and that allowing for a slightly different E o for each type of scattering atom can compensate for thatHey, the spectrum is only sensitive to small differences in E o at low k, right? So although it might make sense that you'd need an extra E o to successfully extend a fit an extra inverse angstrom or two lower in k-space, is it worth it? That depends on the scientific question you're trying to answer, your data quality, and your personal style.10.1.5.8 Correlations E o correlates strongly with the D of the most prominent paths. This correlation can be broken somewhat by using a wide k-range, or a combination of different k-weights.

Less Common Fitting PArAmeters:
These parameters are less commonly fit; in fact, not all fitting software allows all of them. But for some systems, they can still be important.

Cumulants:
We've already alluded a few times to the fact that an EXAFS spectrum samples a large number of absorbing atoms (typically millions), and the distribution of scattering atoms around each absorber is not identical, both because of static and thermal effects. The cumulant expansion is a method for approximately characterizing half path length distributions that are reasonably narrow, such as when the absorber-scatterer distance in a solid varies primarily because of thermal vibration, or when a single path is used to model a modestly distorted near-neighbor environment. The effect of those kind of distributions on χ(k) can be described by introducing factors of the form eAre these cumulants already lurking in the EXAFS equation we've presented? If they are, the first cumulant should appear multiplied by k as the argument of an exponential, and the second cumulant should appear multiplied by k 2 . You've hopefully identified σ 2 as the second cumulant, C 2 .It turns out that all cumulants with n ≥ 2 have a physical meaning associated with the shape of the distribution, but that C 1 does not have a unique physical meaning, as it depends on the choice of origin used to compute itOthers, however, note that there is already a factor in the EXAFS equation that affects the phase of χ(k) and appears weighted by k, just as the first Friends! Before reading further, please look at the EXAFS equation on the inside front cover now and consider if we can see any factors of this type.

Chapter 10 -A Dictionary of Parameters:
Recall that earlier we noted that the decrease in EXAFS amplitude with increasing D causes σ 2 to have a modest effect on peak position in the Fourier transform. For the same reason, odd cumulants do have some effect on amplitude, and even cumulants have some effect on phasecumulant would: namely, the D that appears in the factor sin ( ) 2kD k + ( ) δ . They therefore choose C 1 to be equal to the average value of DFor a Gaussian distribution, all cumulants with n > 2 are zero, yielding the EXAFS equation we printed on the inside front cover. But if the distribution deviates from a pure Gaussian distribution, then higher cumulants may be important.Because we've already met the second cumulant, and the definition of the first is arbitrary, let's take a closer look at a few of the higher cumulants.

Physical interpretation:
The third cumulant turns out to be the mean cubed deviation of the distribution from its mean value.

Typical values:
The third cumulant may be negative, positive, or zero, although for near neighbors, it's frequently positive. The magnitude is usually less than 0.001 Å 3 . Unusually large magnitudes provide a warning that the distribution is broad enough that the cumulant expansion may not converge well; consider other methods of modeling the distribution, such as using more than one path.

Effect on χ(k):
A change in the third cumulant results in a shift in the same direction in χ(k). Because the third cumulant appears weighted by k 3 in the EXAFS equation, the effect is much more evident at high k (Figure

Effect on Fourier transform:
Changing the third cumulant will shift the peak in the Fourier transform in the opposite direction (e.g., making the third cumulant more positive will shift the Fourier transform peak lower), as FigureThe change in the real part of the Fourier transform is less easily described than is the case for changes in D or E o , as the shift is most visible for the sharpest features (Figure

Less Common Fitting Parameters 283:
The many different definitions in use for the first cumulant can make reading the literature on the topic confusing! In any paper discussing the first cumulant, be sure to figure out how they're choosing to define it.An ordinary chemical bond is usually a bit easier to stretch than to compress. To see why, notice that it only takes a few electron volts to break a bond; that is, to stretch it infinitely far. But compressing a bond very far becomes much more difficult, because you are trying to place two atoms on top of each other. Thus, bonded atoms will often exhibit a modestly positive third cumulant. Although the bond may behave roughly symmetrically near equilibrium (perhaps allowing you to approximate the third cumulant as zero), once the excursions get further from equilibrium (for example, at higher temperatures), the asymmetry will become more pronounced and the third cumulant will rise.

Common constraints:
The most common constraint on the third cumulant is simply to assume it is zero. This constraint is most questionable when comparing spectra exhibiting different amounts of disorder. For example, if the spectra being compared were taken at different 284 Chapter 10 -A Dictionary of ParametersFiguretemperatures or if the samples had different doping fractions or different dimensions on the nanoscale, then caution is warranted.In most materials, the nearest-neighbor third cumulant is larger than the third cumulant for more distant single-scattering paths. Thus it may be reasonable to constrain the third cumulant for more distant paths to zero even when it is allowed to vary for nearest neighbors.

Correlations:
The third cumulant can show significant correlations to D for paths on which it is fit, particularly at low k-weight or if the data range being fit in k is short. A multiple k-weight fit (Section 12.2.2 of Chapter 12) is often successful at reducing troublesome correlations. , right? But if C 4 is positive, then that factor will blow up at high k, and χ(k) will go to infinity. That doesn't seem right! But it is right, my dear Simplicio. It simply reminds us that one needs even more terms in the cumulant expansion to go out to an arbitrarily high k.

Fourth Cumulant:
This odd-looking definition is used so that it yields C 4 = 0 for a Gaussian distribution. Thus, it is expected to yield only a small correction for distributions that are nearly Gaussian.(Continued) 10.2.3.5 Effect on Fourier transform A positive fourth cumulant acts to increase the magnitude of the Fourier transform peak for the path (FigureAs with the third cumulant, examination of the real part of the Fourier transform (Figure

Common constraints:
It is very common to constrain the fourth cumulant to zero. You are only likely to use this parameter if you want to carefully compare the MSRD as a function of temperature or some other measure of disorder, such as nanoparticle size.

Correlations:
The fourth cumulant can show significant correlations to the MSRD for the same path. Fitting multiple k-weights or extending the k-range being fit can help to reduce the correlation.

Box 10.5 ConVergenCe oF tHe CUmULAnt series (Continued):
Almost nobody includes terms beyond the fourth cumulant anyway. So Simplicio's observation provides a rough way of testing for convergence. If the fourth cumulant is overpowering the second cumulant (σ 2 ) inside the fitting range, then the cumulant expansion isn't converging very well and it would be better to consider some other approach, like splitting the path up. In other words, we want

Fifth and Higher Cumulants:
Some fitting software allows you to vary cumulants beyond the fourth, but it is very rarely done.

mean Free Path:
10.2.5.1 Symbol λ(k) 10.2.5.2 Physical interpretation As discussed earlier in the entry for the amplitude reduction factor, extrinsic losses in the EXAFS process are phenomenologically modeled by introducing a k-dependent mean free path for the photoelectron.

Typical values:
The solid line in FigureFigure

Effect on χ(k):
Different fitting software will allow the mean free path to be altered in different ways, so we won't be too specific about parameters here. Instead, we'll use the mean free path shown by the dashed line in the previous graph as a concrete example. Note that this represents a considerably shorter mean free path than the default. Although the effect is particularly pronounced for k < 2 Å -1 , that region is not generally used for EXAFS analysis.The effect of shortening the mean free path is to reduce the amplitude of χ(k). Even after shortening the mean free path significantly, at high k, the mean free path is much longer than the path length, so almost no reduction in amplitude is visible in that part of the spectrum (Figure

Effect on Fourier transform:
The extent to which mean free path models are transferable is, however, questionable, perhaps particularly in the case of nanoparticles

Correlations:
As an amplitude variable, mean free path changes correlate strongly with the amplitude reduction factor and coordination numbers. The k-dependence is different, however, as the mean free path 10.2 Less Common Fitting Parameters 289Figure

sCAttering PArAmeters:
An examination of the EXAFS equation reveals the following two parameters we have not yet discussed in this chapter: f(k) and δ(k). You will see the EXAFS equation written in a variety of ways in the literature and in tutorials, depending on the features the presenter wants to emphasize. For example, higher cumulants are usually left off. What we have written as f(k) and δ(k) show a particularly large amount of variation as to notation, because they represent parameters that are not varied during a fit, but are instead drawn from either theory or experiment. In that sense, they are of less interest to someone interested in practical applications of EXAFS. But it is still wise to know something about them.To understand what f(k) and δ(k) represent, consider the journey of a photoelectron leaving the central atom, scattering off of a neighboring atom, and then returningThe notation used in this book, in which f(k) represents the amplitude factor from scattering and δ(k) represents the phase factor, is one convention in use in the literature. Another convention is to have f(k) represent the effect of the scattering atom, and δ(k) represent the effect of the absorbing atom. Each must then be complex, so that it can affect both phase and amplitude. Under the latter convention, the real part of f(k) affects amplitude and the imaginary part phase, whereas for δ(k), the real part affects phase and the imaginary part amplitude-a state of affairs that admittedly could cause confusion. There is no good reason for the strange convention; it is just one of those historical accidents that happens from time to time. if it spent the whole time traveling at the speed corresponding to the k we calculate from E o . This phase shift is part of what we've written as δ(k).Sooner or later, the electron encounters the scattering atom. As it descends into that potential, it speeds up again, adding an additional shift to δ(k). In addition, there can be a phase shift of π (or -π, if you prefer … the physical result is the same) associated with reversing direction. Once again, we lump that in as part of δ(k).Of course, the electron does not necessarily scatter at all-it may go sailing on by. (As a wave, we can think of it as "partially" scattering, which is perhaps a better description, given the interference that subsequently occurs.) So we need to include a probability for scattering. That probability is part of what we've written as f(k).After scattering, the electron once again descends into the central atom's potential, causing an additional phase shift identical to what it experienced on the way out. Sometimes you'll see these various phase shifts written separately in the EXAFS equation, instead of lumping them all together the way we did.Finally, it should be noted that the central atom's potential "pulls in" the scattered electron a bit as it returns, somewhat as the Sun pulls in a comet. This increases the probability of a successful round trip. On the other hand, it's possible the returning electron will scatter off of the central atom, spoiling its chances to complete the round trip. These two factors affect the probability of the particular round trip path being complete, and we thus incorporate them into f(k).In the olden days, scientists used to look at the spectra of known materials similar to a sample of interest to extract f(k) and δ(k), and some still do. But nowadays most people rely on software such as feff to calculate f(k) and δ(k).Below we'll provide a few examples calculated by feff, so that you can get a sense of what the functions look like.(The phase variables can, of course, have integer multiples of 2π added to them without changing the physical meaning. For the purpose of these graphs, this ambiguity was resolved by requiring 0 < phase < 2π at k = 16 Å -1 .)To start with, let's look at a calculation for a typical path with an iron absorbing atom and an oxygen scatterer (FigureNow, an iron absorber and a bismuth scatterer (Figure

Scattering Parameters 291:
These kinds of graphs allow for some nice fingerprinting tricks. Suppose you've got an iron oxide, and you're not sure if a peak in the The most important things for us to know about f(k) and δ(k) are• f(k) and δ(k) are fairly gentle functions of atomic number. This is the origin of the conventional wisdom that EXAFS cannot easily distinguish scattering species that are near each other on the periodic table.

Box 10.6 WHY tHe FoUrier trAnsForm oF PAtHs PeAK BeLoW tHeir HALF PAtH LengtH:
Friends! We can now see why the Fourier transform of a path peaks at a lower value than the D of that path, although it takes a few steps to reach our destination:In the semiclassical explanation for the phase shift highlighted in FigureBecause δ(k) generally falls off with increasing k, each peak in will be less and less advanced in phase; in other words, the peaks will come later and later relative to what would be suggested by the 2kD part alone. This mimics the effect of a smaller D, and the peak in the Fourier transform thus shifts to the left.It's sometimes possible, when a narrow k-range near a resonance of a high-Z scatterer is used, to have δ(k) mostly rising with increasing k. In that case, we can actually have the Fourier transform of a path peak a little higher than the D for the path. If a reasonably broad k range is used, however, the downward trend of the part of δ(k) attributable to the absorbing atom will prevail, shifting even high-Z scatterers down in R. • For low-Z scatterers, f(k) drops off with k. Oxygen scatterers, for instance, contribute to χ(k) primarily below 6 Å -1 .• At high enough k, f(k) always drops off.• As Z increases, peaks in f(k) shift toward higher k. As scatterers, 3d transition elements give peak values of f(k) between 5 and 8 Å -1 . χ(k) for an iron oxide, therefore, will be dominated by oxygen backscatterers at low k, but by iron backscatterers at higher k.• For scatterers heavier than the 3d metals, f(k) shows multiple peaks in the EXAFS region. This, in turn, can yield split peaks in the magnitude of the Fourier transform even when only a single absorber-scatterer distance is present.For a further discussion of the trends shown by the scattering parameters, see294 Chapter 10 -A Dictionary of Parameters Box 10.7 We neVer sAiD tHis WAs eAsY (Continued )And be sure to measure a standard: a known material with a similar structure. Measure it under conditions as similar to those you measure your sample under as you can conveniently manage. Then fit the standard. By understanding how amplitude variables interact for your standard, you'll learn how to handle them for your sample too.And although some amplitude parameters, like S o 2 and N, have the same k-dependence, others do not. Fitting multiple k-weights and comparing the effect of different k-ranges and weights are both effective tactics for sorting out these parameters. One shouldn't just look at the parameters that come out of a fit; much can often be learned by carefully looking at χ(k) for different fits and different samples in a series.Well, I'll try, but I'm not filled with confidence.All any of us can do is try, Simplicio. Everyone who fits EXAFS data is faced with the same sorts of problems when it comes to these parameters. And yet we've managed to successfully discover many, many nifty things about the materials we've studied. Chapters 11-14 will hopefully give you some idea as to how this is done. But maybe the case studies in Chapter 16 will be the most helpful: there's nothing like seeing how experts have dealt with these issues in their publications.

WHAt i'Ve LeArneD in CHAPter 10, BY simPLiCio:
• Parameters can be divided into those that primarily affect the phase of χ(k) and those that primarily affect its amplitude.• Half path length shifts χ(k) more at high k than low, whereas E o shifts χ(k) more at low k than high.• The MSRD suppresses amplitude, particularly at high k.• The amplitude reduction factor has complicated theoretical underpinnings, but can be thought of as a phenomenological parameter accounting for k-and R-independent amplitude suppression regardless of cause.• Similarly, E o is the effective energy origin for photoelectrons in the EXAFS region, but it is difficult to assign a more precise physical meaning to that energy.• The third cumulant can be used when moderate disorder is present to address asymmetry in the radial distribution function for a path.• For low Z scatterers, f(k) drops off with k. Oxygen scatterers, for instance, contribute to χ(k) primarily below 6 Å -1 .• As Z increases, peaks in f(k) shift toward higher k, eventually resulting in multiple peaks in the EXAFS region for scattering elements beyond the 3d metals. The most obvious method for comparing two fits is probably to use a statistical measure. But as discussed in Section 7.4.2 of Chapter 7, it can be difficult to apply traditional statistical tests to XAFS analyses. It turns out, however, that EXAFS has some advantages over XANES in this area.

number of Independent Points in eXaFS:
In EXAFS modeling, we are usually fitting either the Fourier transform or the back-transform. According to the Rayleigh criterion (Section 4.5.2 of Chapter 4), we can just barely resolve peaks in the Fourier transform with a spacing ofIf we fit over a range from R min to R max , that means we could consider there to be a number of independent points given byThis is often known as the Nyquist criterion, due to its relationship to an analysis of the information content of telegraph signals by Harry NyquistIn the 1980s and 1990s, there was some discussion as to whether EquationTo explain what is meant by ideally packed information, let's consider an attempt to model a chlorine-and methyl-substituted ferrocene, such as the one shown in FigureSuppose we plan to use iron K-edge data from 3.0 to 10.0 Å -1 and to fit from 1.0 to 3.0 Å in the Fourier transform. According to the Nyquist criterion, that gives us 8 independent points (rounding down), or 10 if you use the "+2" from the Stern formula. Let us suppose we are trying to fit the following parameters:• S o 2 and E o • The distance from the iron atom to the carbons in the rings, and an MSRD for that distance 298 Chapter 11 -Identifying a Good FitIn the first three sections of this chapter, we will frequently cite the Error Reporting Recommendations of the International XAFS Society (IXS Standards and Criteria Committee 2000). This is in part because it is a frank summary of statistical issues related to EXAFS, and in part because the IXS has made suggestions as to how to standardize the treatment of statistical issues in EXAFS.In this chapter, we shall discuss eight criteria for deciding whether one fit is better than another. We have grouped them so that related ideas are presented sequentially; they are not presented in order of importance. At the end of the chapter, we shall briefly discuss how to use them in combination.• The number of chlorines substituted on to the rings, and the number of methyl groups• The distance from the iron atom to the chlorine atoms, and an MSRD for that distance• The distance from the iron atom to the carbon atoms in the methyl groups, and an MSRD for that distance That's 10 free parameters in all, so we have just enough independent points by the Stern formula. But the first two parameters affect the fit overall, the next two the near neighbors only, and the remaining six the scatterers further out. To see how the scatterers distribute themselves through the Fourier transform, we use FEFF to generate theoretical standards for each of the scatterers (FigureInformation about the chlorine atoms and the methyl carbons is almost entirely limited to the range from 2.0 to 3.0 Å. Because that's half the range being fitted, we only have four or five independent points that apply to that part of the Fourier transform-not enough to fit the six parameters that have to do solely with the chlorines and the methyl carbons.That is what is meant by nonideal packing.In general, it is unlikely that the free parameters distribute evenly across the Fourier transform, and thus the Stern formula, or even the more conservative Nyquist formula, can mislead us. It's better to have a bit of a cushion to account for this nonideal packing; dropping the +2 is one way of doing that.

BoX 11.1 tHouGHtS on nyQuISt anD Stern:
Since 2.0-3.0 Å is half the size of 1.0-3.0 Å, we said the number of independent points associated with the top half of the range is half the total number of independent points. That's OK for the Nyquist criterion, but for the Stern formula that means we divided the +2 in half too. If I were to use the Stern formula on just the range from 2.0 Å to 3.0 Å, I get six independent points-the four points that come from Nyquist, plus two! So wouldn't we still be OK? But we can't actually fit like that, Simplicio. If you look at FigureFrankly, the Nyquist criterion is less confusing than the Stern formula to think about. For Nyquist, fitting half the range gives half the number of independent points. Since we want a cushion to allow for nonideal packing anyway, why not also choose the version that has a simple proportionality?It is good to realize, though, that the Stern formula can be helpful and appropriate when performing a first-shell fit. Packing is not as significant an issue when one is only considering a single-scattering path.A warning: before publication, double check your analysis software's calculation of the number of independent points! Some will use Nyquist, some +1, and some +2-and they also may round differently. This argument has nothing to do with the relative amplitude of the peaks! There isn't "more information" between 1.0 and 2.0 Å just because the peak is bigger!

Measurement uncertainty:
Measurement uncertainty can be divided into two types: random and systematic. Random errors vary from measurement to measurement in such a way that, if the experiment were to be repeated enough times, the average result would converge toward the result that would be found if no random error were present. In the context of Section 3.3.3 of Chapter 3, we called this kind of error noise. Systematic error, then, is any source of error that cannot be made arbitrarily small simply by averaging a large enough number of repeated measurements.There's a problem, however, in that we are trying to evaluate the quality of a model that has been fitted to data. Suppose, for instance, that we model our material as rutile and it is actually anatase. We are, in some sense, then making a systematic error. But if we were to include that error as part of the estimate of the measurement uncertainty, we would conclude we had a statistically "good" fit. In essence, we would be building our uncertainty about what the material is into our statistics. Used that way, statistics would be a method for telling us how uncertain our process was, rather than how good the fit was. The bottom line is that whether you're using Nyquist or Stern, it's just a guideline. You'd always like to leave some cushion, particularly for multishell fits. Personally, for multishell fits, I don't like having more free parameters than about two-thirds of the Nyquist value.Wait-you're being conservative, Dysnomia? I thought you liked to take shortcuts! Adding more and more free parameters until I reach some theoretical limit that's dependent on my data and model having characteristics they probably don't have isn't a shortcut; it's just being greedy. Trying to squeeze more information out of data than is present is, at best, a waste of time, and I don't like wasting time. At worst, it could produce a misleading fit, and I don't like that either. I like getting pretty close to the right answer quickly, not taking forever to get an answer totally wrong! In any case, we will use the Nyquist criterion for statistical purposes for the rest of this book, my friends. This is also the recommendation of the International XAFS Society (IXS Standards and Criteria Committee 2000).To avoid this, we exclude errors in the model from being counted as "measurement uncertainty." Thus, if the fitted model deviates from the data by an amount much greater than what we've classified as measurement uncertainty, it must be because the model is poor.But that distinction is not clear-cut. While shot noise is certainly part of measurement uncertainty, and the decision of whether to model anatase or rutile is not, what about if we used an inaccurate mean free path for the electron (Section 10.2.5 of Chapter 10)? What about errors caused by the muffin-tin approximation (Section 9.2.1 of Chapter 9)?The International XAFS Society has considered this question, and concluded that the distinction should be between errors that can be identified and fixed as opposed to those that are "poorly understood" (IXS Standards and Criteria Committee 2000). Thus, the IXS recommends that the following not be considered as contributing to measurement uncertainty:• Thickness effects, such as those described in Section 3. Estimating the contribution of systematic error is much more difficult. While it might seem that the measurement and analysis of a known 302 Chapter 11 -Identifying a Good Fit Some items, such as thickness effects and energydependent absorption, are in both lists!The idea is that if the effect can be calculated for and then corrected, it should not be considered part of the measurement uncertainty. That is why the IXS places self-absorption in the first category, for example (although, in practice, it is usually not possible to completely correct for self-absorption, and it should, therefore, have some contribution to the measurement uncertainty as well). Some kinds of thickness effects are fairly easy to correct for, while others are not.standard could provide a good estimate, several potential sources of systematic error discussed earlier in the book might affect a sample differently from a standard. A uniform standard, for instance, is less affected by a nonuniform beam profile than a sample which incorporates some inhomogeneities. Ab initio calculations are more likely to be based on unjustified assumptions for a novel material than for a known one. And radiation damage, which is rarely a problem with standards, may cause significant changes in a sample.The IXS is working to improve this situation (IXS Standards and Criteria Committee 2000):At the present time the magnitude of the systematic error in a 'typical' XAFS experiment is not known. Planned future activities of the IXS Standards and Criteria committee include round-robin type measurements at various XAFS beamlines around the world and modeling of various analytical procedures. The goal of these activities will be to determine the magnitude and distribution of the major systematic errors.Until then, the IXS recommends that measurement uncertainty be initially calculated as if it were due solely to random error. In Section 11.3, we'll discuss how this recommendation is used to compute uncertainties, and in Chapter 15 we'll indicate how statistical quantities should be reported in a paper.

reduced χ 2:
We are now able to compute a preliminary reduced χ 2 for an EXAFS fit with N ind independent points and ν degrees of freedom, given by EquationThis is identical to EquationUsing the random error in place of the total measurement error has important ramifications, however.

Criterion 1: Statistical Quality 303:
When fitting the Fourier transform (the most usual case for EXAFS), it is noteworthy that Equation 11.3 is applied to the real and imaginary parts of the Fourier transform separately, and the results added together to give χ ν 2 . This is a different result than what one would get by using the magnitudes of the data and the fit! And fitting the magnitude of the Fourier transform rather than the real and imaginary parts would be a terrible idea! You would be throwing much of the information in the spectrum away! First of all, χ ν 2 should not be compared for two fits for which the measurement uncertainty is expected to be significantly different. This is true even if it is only the random contribution that differs. Thus, χ ν 2 should not be used to compare two otherwise identically constructed fits that are made on different ranges of χ(k) taken from the same data set.To understand this better, imagine performing a fit on a range of k 2 -weighted data from 3.0 to 10.0 Å -1 , and that the measurement error at 3.0 Å -1 is 5% from random sources and 95% from systematic errors. How do those contributions vary with k? That depends on many factors, including data collection decisions such as integration time (Section 5.8 of Chapter 5). But, for the sake of argument, let's assume that the random error in the unweighted χ(k) is independent of k, while the systematic error in the weighted k 2 χ(k) is independent of k. This is not unreasonable if the k 2 -weighting was chosen so as to keep the amplitude roughly constant across the fitting interval. In that case, the contribution to the measurement error at 10.0 Å -1 from random sources would be 11 times as large as at 3.0 Å -1 . But the total measurement error would only increase by 50%, because most of the error still comes from the systematic contribution, which we've assumed is independent of k.While it is possible to use one of the methods suggested by the IXS to estimate the random error as a function of k, this example reveals why, in cases for which systematic error is expected to dominate over random error, it is better to estimate an average random error and apply it throughout the range; that is, to set ε ε= in Equation 11.3. Otherwise, you may overestimate the importance of the low end of the fitting range relative to the high end. If your data are very noisy, on the other hand, then random error is more likely to dominate, and it might be appropriate to use a method that estimates ε i random 2 at each point.In any case, where systematic error is important, it should now also be clear why χ ν 2 cannot be used to compare two fits to different k-ranges from the same data set. Continuing our above example, if we extended the k-range so that it included from 3.0 to 14.0 Å -1 , the average random noise over the entire interval would increase by about 78%, even though the true measurement uncertainty would only have increased by about 17%. Using the random error alone to estimate the measurement uncertainty would result in a dramatic overestimate of the ratio between the measurement uncertainties for the two ranges, and a corresponding bias toward the second fit.

While χ ν:
2 cannot be used to compare fits on different data, that doesn't mean it's useless. It is a very good tool for comparing two fits to the same data. Those fits may differ in the number or identity of free parameters, for instance, or in the range of Fourier transform used.

Chapter 11 -Identifying a Good Fit:
We've included this example for those of you who want some sense of why you shouldn't compare the reduced χ 2 of fits on different data. To show how that might cause problems, we've made a lot of iffy assumptions, such as the idea that the systematic error is independent of k-in the k 2 -weighted data. The point here is not whether that assumption is good or not-it's that you have no way of knowing, in a given case, how systematic error varies from one part of χ(k) to another or from one sample to another. And if you don't know that, then any statistical claims you make in comparing the two are completely unjustified! If the R-factor is small, then the fitted model is very similar to the data. In such a case, does it really matter whether the remaining mismatch is a random error, a systematic error, or a failure in the model? It may well be that in some particular case that a 1% mismatch between fitted model and data is primarily due to an error in the model; but it is likely to be a small error, such as constraining two bond distances to be the same when actually there is a very slight splitting. In such a case, the major features of the model are still accurate. In another case, the 1% mismatch might be due primarily to noise-but once again, we would be likely to conclude that the major features of the model are accurate.

R-Factor:
But how close is "close enough"? 1%? 10%? The IXS (IXS Standards and Criteria Committee 2000) says "as long as the signal-to-noise ratio (S/N) of the data is good, the R-factor of adequate fits can be expected to be not more than a few percent."We'll supplement that with our own informal guidelines, provided in TableThe R-factor really isn't telling you anything that you can't quickly judge by looking at a plot of the data and the fit. For example, Figures 11.3 In case you're curious, these fits are to the nickel edge of a sample of nickel-zinc ferrite nanoparticles. The fits are conducted on the Fourier transform of the data from 3.0 to 9.0 Å -1 using k 2 -weighting.through 11.6 show four fits to the same data, one for each of the four ranges identified in Table

Hamilton test:
There is a difference between preferring one fit as "better" than another, and drawing conclusions about the physical structure of the material based on the improvement seen.For example, suppose that, for some set of data, the introduction of a splitting into the nearest neighbor distances causes the preliminary χ ν 2 306 Chapter 11 -Identifying a Good Fit Our eye is a better guide than the R-factor alone, my friends! Does the fit reproduce the features of the model, although off a bit in amplitude or phase, as is the case in FigureThese are exactly the kinds of questions the Hamilton test, which we described in detail in Section 7.4.5 of Chapter 7, can address. If the Hamilton test does not show a statistically significant difference between the fits, then we should not assert anything stronger than consistency. If it does show a statistically significant difference, then we can temper our language on the basis of the degree of significance.While the procedure for applying the Hamilton test is given in Section 7.4.5 of Chapter 7, there are a few notes concerning its application to EXAFS modeling that are worth making:• The Hamilton test is only appropriate for comparing two fits on the same k-and R-ranges; that is, two fits that use the same data but fit it with different models. To compare fits on two different R-ranges, use reduced χ 2 . Fits on different k-ranges cannot be compared statistically, since the systematic error is unknown; other criteria from this chapter must be used to select between them.• Since the Hamilton test depends on the ratios of closeness-of-fit parameters, there is no difference between applying it to χ 2 , χ ν 2 , or the R-factor, as long as each point is weighted the same (e.g., a constant measurement uncertainty is applied across the fitted In fact, if the fit is that close, trying to make it closer may mean that you are trying to fit systematic errors better, or even noise. Fitting details of the noise is certainly a waste of time.We haven't actually given the criterion for determining if a change in reduced χ 2 is statistically significant. That's because it's usually not an important question-if you're tinkering with the k-range, you're almost always looking for the best fit, and not trying to show that it allows you to reject some other fit. And if you're not tinkering with the k-range, then the Hamilton test is the way to go.If you would like to know more about statistical significance and reduced χ 2 , the work done byrange). Conventionally, EXAFS fits have applied it to the R-factor• Changes in the theoretical standards used by the model can often be accounted for as if they were changes in parameters. For example, if fit A assumed that the nearest neighbors were oxygen and included coordination number, bond length, and MSRD for those oxygen atoms as free parameters, while fit B assumed the nearest neighbors were sulfur and used the same three parameters, then a Hamilton test could be applied with the assumption that those three parameters were "changed"instead of applying to oxygen, they now apply to sulfur. If E o and S o 2 were also free parameters, they would not be considered to be changed, as they are primarily a property of the absorber, not the scatterer.

CrIterIon 3: PreCISIon:
Suppose you'd like to know whether the iron in your sample is tetrahedrally or octahedrally coordinated. You, therefore, make the coordination number for the iron a free parameter, and get a fit with an R-factor of 0.01. That fit gives a coordination number of 4.6 ± 2.1. Are you done fitting?The answer, of course, is no. Considering the precision of the result, you have not answered your question; the fit is consistent with a coordination number of either 4 or of 6, that is, it's consistent with either tetrahedral or octahedral coordination. In this case, you'd want to continue trying to refine the fit to reduce the uncertainty.

Finding uncertainties in Fitted Parameters:
Of course, this raises the question of how to find the uncertainties in fitted parameters. The IXS recommends that the uncertainty be defined as the amount by which the fitted parameter would need to be changed (while still allowing all other fitted parameters to vary) so as to cause the χ 2 to increase by 1.But didn't Section 11.1 teach us that we don't have a good way of computing χ 2 , because we don't have a good way of estimating systematic error?Not quite. Recall that there are three categories of phenomena that contribute to a mismatch between fitted model and data: random error, systematic error, and poor fit due to, for example, a poor choice of model. Therefore, if we believe a fit is "good," (i.e., the model chosen is appropriate and we've corrected for other quantifiable effects), then all of the mismatch between fitted model and data is due to measurement error,

=:
, where ν is the number of degrees of freedom of the fit. and since we can estimate the random contribution to the measurement error, we know the rest must be systematic.And we necessarily believe our final fit is pretty good, or we wouldn't consider it our final fit! In fact, this chapter provides several criteria for judging the quality of a fit, most of which don't depend on statistical methods.For a good fit, χ ν 2 should be around 1 (see Section 7.4.6 of Chapter 7). So, if our preliminary value for χ ν 2 , calculated by neglecting the contribution of systematic errors to the measurement uncertainty ε, were, say, 100, we would know that our preliminary value for χ ν 2 was 100 times too large, meaning that ε 2 was 100 times too small, and thus ε was 10 times too small. With the new, more accurate estimate of ε, we could then apply the rule given in the first paragraph of this section to find the uncertainty for each fitted parameter.

Calculations of uncertainties by analysis Software:
Almost all software designed for fitting will report uncertainties in the fitted parameters. It can be shown that the definition of uncertainty provided in the first paragraph of Section 11.3.1 is roughly equivalent to taking the square root of the diagonal elements of the covariance matrices used in fitting routines (IXS Standards and Criteria Committee 2000). A few software packages, including those based on ifeffit, automatically scale the uncertainties by χ ν 2 , which is equivalent to the scaling procedure for accounting for systematic error in good fits described in the last paragraph of Section 11.3.1. Other software packages allow for the user to provide a scaling factor, which could then be chosen to be χ ν 2 . Some simply report the square root of the diagonal elements of the covariance matrix, leaving any scaling to be applied manually by the user. And at least one package introduces an additional factor of 2 to produce a more conservative estimate of uncertainty. A somewhat out-of-date, but still informative, comparison of the methods used by several of the more packages can be found in (IXS Standards and Criteria Committee 1998). Be sure to check the documentation of the software you are using so that you know what additional scaling, if any, you should apply.

How Precise?:
While it is necessary to determine fitted parameters with sufficient precision to answer the scientific questions you are posing, this doesn't mean that every fitted parameter needs to be found to the same level of precision. For example, if you are interested in determining a nearest-neighbor bond length to within 0.02 Å, it doesn't matter if the same fit only determines the distance to the next-nearest neighbor to ±0.04 Å. Likewise,

Criterion 3: Precision 309:
Remember that this procedure is only valid if the fit is good! If the fit is not good by the other criteria presented in this chapter, then this rescaling procedure does not produce meaningful uncertainties!The square root of a diagonal element of the covariance matrix is exactly equivalent to the definition provided in Section 11.3.1 (to wit, the amount by which the corresponding fitted parameter must be changed to increase χ 2 by 1 when still allowing other fitted parameters to vary) if the response of χ 2 to changes in that parameter is parabolic. Near a minimum in χ 2 that is generally a reasonably good assumption.the uncertainty in the fitted value of S o 2 is largely irrelevant unless you plan to use the value you have found to constrain another fit.For the parameters that do help answer the questions you are studying, it is important to realize that, if all errors are normally distributed, the true answer will lie outside the specified uncertainty range nearly onethird of the time, and more than twice the specified uncertainty away from the best-fit value nearly 5% of the time. Thus, a coordination number of 4.1 ± 1.1 is only good enough to tentatively choose tetrahedral over octahedral coordination, since an octahedral coordination of 6.0 is less than twice the stated uncertainty away from the best-fit value. A fit that satisfies the other seven criteria in this chapter well but reduces the uncertainty from, say, ± 1.1 to ± 0.8, is, therefore, to be preferred.

Correlations:
Many software packages report the correlations between fitted parameters. If the procedure in Section 11.3.1 is used to compute uncertainties, then the effects of these correlations have already been included in those uncertainties.So, why report correlations at all? A high correlation between a pair of parameters can explain why the uncertainties for those parameters are large, and thus may provide clues as to how to reduce those uncertainties. For example, constraining one of the parameters by using information from another source (e.g., another characterization technique, a fit to an empirical standard, or a theoretical computation) will likely reduce the uncertainty in the other.

CrIterIon 4: SIZe oF Data ranGeS:
Even without knowing anything about someone's EXAFS problem, we can likely say a few things about the structure being studied. There is probably a big peak between about 1 and 2.5 Å in the Fourier transform due to the near neighbors. Above that first peak, multiple scattering by near neighbors becomes a factor. For most materials, there will also be a signal from more distant atoms (in some cases, such as metal carbides and high-Z oxides, the peak associated with the second-shell scatterers may even exceed the amplitude of the peak associated with near neighbors). As we go higher and higher in R, the Fourier transforms of EXAFS spectra of different substances show a greater range of behavior; some may be nearly featureless except for a bit of noise and sidebands from truncation effects on lower peaks, while others may show strong structure from ordering of distant scatterers, perhaps amplified by focused multiple scattering.This means that a fit on the near-neighbor peak alone is reasonably likely to be "good" in the sense of criteria 1 through 3, even if the model is incorrect. While some of the later criteria in this chapter (notably Criterion 7) 310 Chapter 11 -Identifying a Good Fit Of course, it is not OK to simply arbitrarily constrain one correlated parameter to achieve an apparent reduction in the uncertainty of another! For example, fixing the value of a parameter to the best-fit result of a prior-fit to the same spectrum is a definite no-no! Recall from Section 6.2 of Chapter 6, for instance, how tetrahedral or octahedral occupancy in a spinel is most easily fingerprinted by data from will help to weed out incorrect models, there is still generally the potential for a fit using a poor model to seem OK when only the first peak is being fit. Extending the fit to higher R, as long as there is structural signal, will increase the probability that a good fit indicates a correct model. This improvement is not just the "statistical" improvement that comes from increasing the number of independent points-it is because beyond the first shell, the candidate models tend to produce Fourier transforms that differ from each other more dramatically than they do at low R.While so far we've been arguing that, all else being equal, a fit that includes more of the Fourier transform is better, this argument does not apply in the same way to the amount of k-space data that is selected. χ(k), unlike the Fourier transform, includes data from the whole structure throughout its range. It is true that low-Z scatterers are emphasized more at low k than higher-Z scatterers (Section 10.3 of Chapter 10), but each still contributes at all k. On the other hand, we cannot use reduced χ 2 to guide us as to whether we are gaining a statistical improvement by extending the k-range, because the relative importance of random and systematic error is likely to changeHow does the fit "know" what to do with data it was not instructed to fit? Presumably because the structural model is essentially correct. Behavior similar to that shown in FigureA similar check can be made in k-space; a good fit should follow the data for a bit above the end of the k-range being fit.It is less important that the fit agrees with the data below the fitted range in the Fourier transform, or below the data range in k-space, however. In both cases, the region below the fitted range is heavily influenced by details of background subtraction, and thus a mismatch is not necessarily indicative of a faulty model. I think it's more a semantic conflict than a substantive one. A fit similar to the one in FigureA good fit, therefore, should give the same results for the parameters of interest (i.e., those which answer the scientific questions the experiment is intended to address) when any of the following are changed slightly:• Slightly (perhaps 0.5 or 1.0 Å -1 ) different low end of the k-range. This probably cannot be changed too drastically, as both background and theoretical approximations become important at low k.• Different high end of the k-range. This can probably be changed somewhat more than the low end of the k-range. Yes, of course. One must always adapt to circumstances.• Different low end of the R-range. As with the k-range, this probably can't be changed by much-perhaps only 0.2 Å, because it should remain between the background region and the peak of the first-shell data.• The high end of the R-range. The amount by which this can be changed depends in part on whether you are employing a bottomup (Section 9.4.1 of Chapter 9) or top-down (Section 9.4.2 of Chapter 9) strategy. For the top-down strategy, you may be able to change the high end by an angstrom or more.• Different k-weight.• Modestly different background subtraction.• Modestly different constraint scheme. For example, grouping together or splitting up the MSRD of similar paths.Of course, asking for a fit to be stable under these stresses does not mean that nothing changes at all. Some fits will have higher R-factors, for instance, and as you move away from your best fit, the R-factor may degrade significantly. Precision may also vary substantially, so that some fits, while giving values that are consistent with the best fit, may be insufficiently precise to be considered good themselves. The best-fit values of the parameters of interest will, of course, drift around some, but hopefully in such a way that the range defined by the new values with their uncertainties overlaps the range defined by the best fit values with their uncertainties. Finally, it is also possible that for relatively severe stresses, the fit may flip into a completely different "false minimum," giving significantly different values for the parameters of interest. As long as this false minimum can be identified as significantly inferior by other criteria described in this chapter, the better fit can still be considered to have passed its stability check.How does a fit fail its stability check? A fit fails if the parameters of interest change in such a way that the range as defined by the reported uncertainties of one of the fits from the stability test does not overlap the range from the best fit, and the new fit is not significantly inferior given the other criteria in this chapter. If that's the case, more work has to be done so that the two fits can be distinguished, allowing one to be rejected as a false minimum. For example, a bond angle must be between 0° and 180°, and a site occupancy must be between 0 and 1.

CrIterIon 8: How DeFenSIBle IS tHe MoDel?:
Finding a good fit usually involves a lot of trial and error. If done thoughtlessly, that trial and error can amount to additional free parameters.Suppose, for instance, that the Nyquist criterion indicates that a fit of a nanoscale lead sulfide has 10 independent points, and that the fit includes 7 explicit free parameters: E o , S o 2 , a lattice parameter, the MSRD for the nearneighbor sulfur scattering, a third cumulant for the near-neighbor sulfur scattering, an MSRD for the fifth nearest-neighbor sulfur scattering, and an MSRD for all of the other scattering in the material. Looking at the list of free parameters, a question naturally arises: why assign the fifth nearestneighbor sulfurs a different MSRD, when that's not done for any other scattering except for the near neighbors? The near neighbors are special, because they will tend to vibrate in sync with the absorbing atom, but what's special about the fifth nearest-neighbor sulfurs? If the answer is "there's nothing special about them; I just kept trying different combinations of free parameters until I got a good fit," then there are, in effect, unreported free parameters in the fitting process. The process of searching for a fit by arbitrarily releasing constraints on parameters can itself be a kind of minimization process, with the person helping the computer change one parameter at a time until a "best fit" is found. In the example discussed, this could very well mean that more than 10 parameters were effectively allowed to vary during the process. When more parameters are allowed to vary than there are independent points, we are exceeding the information content of the data, with the result that a "good" fit may actually be a meaningless coincidence. There is a difference between something you don't expect and something that's physically impossible. You may be expecting six sulfurs coordinated to your absorbing atom, and find only four. That may be surprising, but it's within the realm of accepted chemical behavior, and you could not reject the fit out of hand. But if a fit told you there were 20 sulfurs directly coordinated to the absorber, you'd know that was wrong-there just isn't room for them. Likewise, you might find a bond between iron and sulfur that's a bit longer or shorter than what you expected-but if it were 1.0 Å, you would know it was wrong-only bonds to hydrogen are anywhere near that short.On the other hand, suppose some theories suggested that our nearestneighbor distances were split, while others suggested that all of our nearest-neighbor distances are the same. We test both models, and find that allowing the nearest-neighbor distances to be split does not give a significantly different result in any of our parameters, or in the quality of the fit. Because we tested the possibility of two different nearest-neighbor distances in one fit, does that mean we must forever after count the amount of splitting as a free parameter? The answer is no, because we are testing two conceptually defensible models against each other, either of which would have been considered a reasonable possibility a priori. This is, admittedly, a subtle distinction to make, and one that allows for gray areas. While we would have been very unlikely to speculate about a special MSRD for the fifth nearest-neighbor sulfurs a priori, what about assigning different MSRD's to lead and sulfur scatterers? That sounds less arbitrary, and more like a model that might be considered a priori. Thus, there is no sharp distinction between a "defensible" model and an "arbitrary" one-it's a matter of degree.Defensible models usually have at least one of these characteristics:1. Simplicity. Simple models have very few free parameters, and incorporate constraints that rely on simplifying assumptions. For instance, there might be a single parameter representing an overall lattice expansion. For many materials, such a model is likely to be a simplification of the actual behavior of the material as it changes temperature, but it might work as a first approximation.A top-down strategy usually starts with a simple model. 2. Flexibility. Flexible models can fit a wide variety of candidate structures. Such a model will have few constraints, but many free parameters. A bottom-up strategy often starts with a flexible model. 3. Physical accuracy. A model gains physical accuracy when it is constrained by a physical understanding of the material. This understanding may be derived from other characterization techniques or from theoretical tools such as density functional theory.

evaluatInG a FIt:
Evaluating your fits based on these criteria will necessarily involve tradeoffs, sacrificing how well one criterion is met to improve others. To help sort through how to use them, Table• Comparison only? Statistical measures should be used only for comparing fits.• Critical? The criteria listed as critical must be met; otherwise the fit should be rejected, regardless of how well other criteria are met.

IdenTIFy your QuesTIons:
Before beginning to fit, you should decide on the primary scientific questions you are trying to answer (see Section 2.1 of Chapter 2). It is often sensible to identify secondary questions as well: information that would be nice to know, but which is not necessary for the analysis to be considered a success.We will next present two examples, which we will follow for the rest of this chapter.12.1.1 example: Which Ligand?Consider an unknown iron compound in a soil sample, for which we have collected EXAFS data at the iron K edge. It is known that there is a fairly high sulfur content in the soil, and there is plenty of oxygen as well. As a researcher, we are interested in discovering whether the iron is bonded primarily to sulfur, to oxygen, or to a mix of the two; that is our primary question. (A mix would not necessarily mean that an individual iron atom had ligands of both types-it might mean that some iron was present in the form of a sulfide and some in the form of an oxide.)Secondary questions might include a more specific identification of the iron phases that are present (e.g., which of the many possible iron oxides are present?), and an estimate of the fraction of each iron mineral present, if it does turn out there is a mixture. Nothing wrong with that, Kitsune. But it's not a slam dunk that it's easier. There are a lot of oxides and hydroxides of iron and more than one sulfide too. Fitting to theoretical standards might actually be easier and faster in this case, especially if you haven't already built up a big library of iron oxide and sulfide standard spectra.You could try the analysis both ways. If they yield the same answer (to within the statistical uncertainties), then you can be more confident that systematic errors aren't distorting your results, since they would tend to affect the two analysis methods differently. And if they yield different answers, then you've been warned that there's a problem that needs looking at!Even when fitting a known substance to practice how to fit, one should imagine a scientific question one might be trying to answer. For example, "does this material show local distortions that are not reflected in the structure derived from x-ray diffraction?" 12.1.2 example: Where's the dopant?Consider a thin zinc oxide film doped with 1% manganese. Data were collected on both the zinc and the manganese K edges. It is not known whether the manganese substitutes for the zinc in the lattice, is situated interstitially, or forms distinct manganese oxide clusters. Deciding between those possibilities is the primary question. Important secondary questions depend on the answer to the primary question. If the manganese substitutes directly for zinc, do manganese atoms tend to cluster near each other in the lattice? If they are interstitial, how is charge balance maintained (zinc vacancies, interstitial oxygens, etc.)? If distinct manganese oxide clusters are formed, what is the typical size of those clusters?12.2 PrePAre your dATA

Transform to χ(k):
This was covered in Chapter 4. It is not important at this stage to make sure you have the "perfect" normalization and background subtraction, as long as it's reasonable. At the end of the fitting process, you can check the effect of your data reduction choices.

Choose k-Weighting:
The choice of k-weight is one of those things that shouldn't end up making a significant difference to your final fit. In addition to choosing a single k-weight, some fitting software provides you the option of choosing multiple k-weights. If, for instance, you choose k-weights 1, 2, and 3, the software will perform three separate Fourier transforms (one for each k-weight) on the data and on the theoretical standard. Each χ(R) resulting from the data is then treated as if it were a separate data set, but with one set of free parameters being simultaneously applied to all three sets.

Choose k-range:
Choose an initial range of χ(k) data to fit. You usually want to start out conservative and try expanding the range later in the fitting process once you have a reasonable model in place.The high end of the χ(k) range is generally the easier one to choose because the problem at the high end is the signal-to-noise ratio. Most of us can tell when the signal begins to get lost in the noise and can choose the high end of the range accordingly.For example, FigureThe two scans stop following each other after about 9 Å -1 , so if these were the only two scans 8 or 9 Å -1 would be a good value for the high end of the range to be used for fitting. Averaging more than two scans might allow that value to be pushed a bit higher.The lower end of the χ(k) range is a more difficult question. One reason is that the low-k part of the data tends to be more sensitive to the choice of background. Beginning with all three k-weights simultaneously might reduce correlations. Why not start by using all of the information? But Kitsune! Fitting all three k-weights simultaneously takes more computation! If your fitting model becomes complicated, that could end up slowing you down! It's also a little harder to explain the statistics behind a multiple k-weight fit to someone unfamiliar with it, although that is becoming less of an issue as the technique becomes more common.The truth is that you are all just making choices you find convenient. The systems you are studying don't have a "correct" k-weight hidden inside them, and as long as you transform your data and your theoretical standard in the same way, a good fit shouldn't depend on the k-weight chosen.Saying that two scans "follow" each other doesn't mean they have to lie smack on top of each other. You can see differences between the scans starting at around 5 Å -1 , but those kinds of differences are why we average multiple scans in the first place. By the time you get above 9 Å -1 , though, you've often got one scan going up when the other is going down, and the structure is pretty much lost in the noise.Notice that the different background subtractions primarily affect the data below 6 Å -1 and have a more dramatic effect below 3 Å -1 . If you weren't sure which background was the appropriate one, 6 Å -1 would be a conservative choice for the lower end of the data range to be fit, and below 3 Å -1 would certainly be unwise.  standards (Section 9.2 of Chapter 9), then that will have the greatest effect at low k as well.Similarly, broadening due to limited energy resolution, either because of intrinsic effects such as the core-hole lifetime or instrumental effects such as those caused by finite slit height (Section 5.4.2 of Chapter 5), will have the greatest effect at low k.In Section 10.2.5 of Chapter 10, we saw that the mean-free path also affects the data more strongly at low k. The simplifying assumption that S o 2 is independent of k is least accurate at low k as well (Section 10.1.4 of Chapter 10).Finally, multiple-scattering paths are particularly important at low k (Section 9.3.2 of Chapter 9).The choice of the minimum value of k for which you want to fit your data is thus as much a choice about how much complexity you want to include in your model as a question of data characteristics. If you are prepared to wrestle with triangle paths, multiple E o 's, discriminating between choices for the background, accounting for energy resolution, and correcting for errors in your theoretical standards, then you could choose to begin fitting at 2 Å -1 or even lower. If you want to minimize the effect of all those complications, then a choice like 6 Å -1 is more appropriate. Values in between, of course, represent a compromise.324 Chapter 12 -The Process of Fitting

Box 12.3 THe TrAnsITIon FroM exAFs To xAnes:
Once again, I am reminded that there is no physical distinction between XANES and EXAFS. Over the years, scientists have discovered that there are many approximations that work pretty well in the range of 6-20 Å -1 or so: expanding in terms of scattering paths, using a single energy origin for all the photoelectrons, employing a mean-free path formalism, finding a "smooth background" through the data, and so on. Those approximations are never perfect, but they become less and less accurate as we extend toward lower and lower k.All those low-k approximations make me nervous. So I usually start fitting at 5 Å -1 and then check later to see if I can push that lower without messing up my fit.That is probably because you are an industrial chemist, Dysnomia, so you usually have pretty good data to work with. My data are often very noisy because some of my samples are present in such low concentrations. If I did not start my fitting range until 5 Å -1 , then sometimes I would have almost nothing to fit at all! So I have to start my fitting lower than that, knowing that it may make my background subtraction and modeling a bit more complicated.

Box 12.4 sTArT under THe LIGHT:
This may be a good time to remind yourself that EXAFS is not the only arrow in your quiver. X-ray diffraction, for instance, while it probably won't tell you where the manganese is located may be able to identify which zinc oxide structure has been formed. And examination of the manganese XANES using the methods of Chapter 6 is likely to narrow down the possibilities for manganese oxide species. Other tools, such as x-ray photoemission spectroscopy, could also be very helpful. And while theory and the existing literature on the system aren't always right, they can at least identify particularly likely structures.Kitsune has helped you narrow the list down, but you'll still have choices. After all, if you're down to just one possibility, then why do we need EXAFS? (Well, it never hurts to have confirmation, but still.) Which structure should you try first? I say try the easy ones first. Interstitials, for example, are hard because it takes some thought to figure out where there's space for an atom and even then it's likely to distort the structure around it more than a substitution would. And some structures, like Mn 3 O 4 , have more than one crystallographic site for the manganese atom and that's a pain in the tail. Clustering and nanoscale phases sound annoying too, so I wouldn't start with those unless other evidence already pointed toward them.I once heard a joke about someone at night looking for their car keys under a street light because that's where the light was. Isn't that what you're doing, Dysnomia? Looking where it's light because it's easier? You missed the point of that joke, Simplicio. The original joke is funny because the guy knows he dropped his keys somewhere else. If he's not sure where he dropped them, it makes perfect sense to start by looking in places where they'd be easy to find. After all, if he finds them, he's done, and he never has to look in the hard places, like under a car.Be careful, Dysnomia! How do you know the choices you're not checking don't also fit well? You're right, Mr. Handy. I do have to know that the choices I'm not investigating thoroughly would produce spectra different enough from the one that works that they couldn't also work. But I can find that out either by just trying a quick-and-dirty fit on each of the candidates or because I have a good enough theoretical sense of how spectra look. For example, if an oxide fits well according to the criteria from Chapter 11, then I don't need to bother to try a metallic form because the nearest-neighbor distance would be so different.Since zinc oxide is the predominant phase, the first part of the task would be to determine what its structure was in this sample. It turns out that the EXAFS of the wurtzite and zincblende structures look quite similar, as both feature a zinc atom in an oxygen tetrahedron which connects to 12 identical tetrahedra through the vertices. But wurtzite is hexagonal while zincblende is cubic, so it's a cinch to tell them apart by x-ray diffraction.Once the correct zinc oxide phase was identified, we could follow Dysnomia's advice and try the easiest choice for the manganese edge first, which probably means substituting a single manganese absorbing atom for a zinc atom in the zinc oxide structure we had established. Initial fits could use very few parameters. As a first simple model, we could approximate the effect of the manganese atom as changing the size of the local tetrahedron without disturbing atoms farther out in the lattice much. That would mean fitting S o 2 , E o , a half-path length for the nearest-neighbor oxygens, an MSRD for the nearest-neighbor oxygens, and an MSRD for more distant paths. The key to this approach is to fit out well beyond the nearest-neighbor peak in the Fourier transform from the start, without giving much freedom to the structure to move around. If the structure is wrong because, for instance, the manganese is interstitial, the peaks beyond the nearest-neighbor oxygen scatterers will not even be close. If, on the other hand, the initial simple model is on the right track, there may be some mismatches in both amplitude and phase, but the major peaks of the fit will qualitatively follow the data.If the structure we first try appears to be completely wrong, the plan is to move on to another structure. If it's on the right track, we can then gradually make the model more realistic, perhaps introducing features from our secondary questions, such as clustering of manganese atoms on nearby sites.Hopefully this plan will eventually yield an extremely convincing fit according to the criteria of Chapter 11. At that point, you can be fairly confident that you have the right structure. But, as Dysnomia mentioned in Box 12.4, it might still be helpful to make simple models for some of the other possible structures that you haven't tried yet as that will strengthen your argument as to which structure is the correct one.12.4 FIT!

Choice of R-range:
The low end of the R-range is usually chosen based on the background subtraction. While different software packages use different schemes for background subtraction, there is often a value of R below which the data

Fit! 327:
The strength of this plan is the complexity of the theoretical standard-there are loads of paths and tons of wiggles in the Fourier transform-matched with the simplicity of a model with just a few free parameters. If a fit like that works, you can feel pretty good that you've nailed down the right basic structure.are compromised by the background subtraction algorithm; that value, or a few tenths of an angstrom above it, is generally a good lower end for our fitting range.If it is not clear from your software documentation how to establish that value, take the Fourier transform after performing different, but reasonable, background subtractions and choose the point where the background stops having a strong effect on the Fourier transform. For example, look back at FigureChoosing the high end of the fitting range is a more involved question, which we will address in the context of the next two examples.12.4.1.1 Example: Which ligand? In Chapter 10, we used the scattering off of the nearest-neighbor sulfurs in pyrite as the example path for our graphs. Since pyrite is one of the candidates for the iron species present in the soil sample of the example from Section 12.1.1, let's continue that example by examining, in Figurefrom the second-nearest neighbors and even a substantial contribution from the more distant iron scatterers (D = 3.82 Å). This reminds us of two facts we should consider when choosing the R-range over which to fit:1. As discussed in Box 10.4, the Fourier transforms of paths peak at a value of R well below their half path length D. 2. The Fourier transforms of paths are not sharp spikes-they have substantial width. This is in part because of disorder, but is also because of the effect of taking a Fourier transform on a finite range of data.It's not surprising, therefore, that a path with half path length D = 3.45 Å peaks around R = 3.0 Å and has substantial amplitude well below that.If we knew we were dealing with pyrite and only wanted to fit the first path, FigureOf course, in this example we wouldn't know whether the material was pyrite when we began fitting. But if the nearest-neighbor atoms were all sulfurs, then the next-nearest neighbors couldn't be a whole lot closer than the D = 3.45 Å found in pyrite-there just isn't room. That means that, while the Fourier transform for a different iron sulfide mineralpyrrhotite (Fe 1 -x S), for example-might not bear much resemblance to FigureBut what if we did begin to suspect we were dealing with a pyrite-type structure and wanted to extend the fit to include the sulfurs at D = 3.45 and 3.59 Å, but were not yet ready to try to include the iron scattering at D = 3.82 Å? That would be a bit more challenging, as FigureIt's also worth noting that the sulfur path at D = 3.45 Å and the path at D = 3.59 Å could initially be lumped together into a single sulfurscattering path since they overlap substantially and feature the same scattering species. In fact, if the material is structurally similar to pyrite but is modified in some way, it's not unlikely that the details of how many sulfurs are at what distances could be quite different than in the pure pyrite that we are using for a model. Whether or not particular data are fit better by a split shell with two sulfur distances, or by a single shell with one, is the kind of thing that can be addressed as the fitting process 12.4 Fit! 329But, but 2.1 Å would put the top edge of our fitting range below the D for the first path! That doesn't seem right! Get over it, Simplicio! The Fourier transform is not a radial distribution function. The sooner you come to grips with that, the better.progresses. (This is a great chance to try the Hamilton test, described in Sections 7.4.5 and 11.2.2 of Chapters 7 and 11, respectively!) 12.4.1.2 Example: Where's the dopant? For a top-down approach like that we began in Section 12.1.2, the choice for the high end of the Fourier transform is somewhat more arbitrary. If, for instance, we are attempting to model the manganese edge by using a manganese atom substituted into a zinc site in a wurtzite structure, we actually know more about what the structure should look like far away from the absorber than near it. After all, the manganese substitution will introduce distortions near the site, but those distortions may be smaller farther away.Nevertheless, there are limits to how high in the Fourier transform we can fit. For one thing, both the 1/D 2 and the e -2 D λ in the EXAFS equation will result in a decrease in the signal-to-noise ratio as R increases.Besides the loss of signal at high R, high-R data are more sensitive to errors in theory and the fitting model. For example, suppose that at some value of k within the data range the mean-free path should be 9 Å, but the model is using a value of 10 Å. For a path at D = 2.0 Å, the error in the amplitude, as given by the e -2 D λ factor, would be about 5%. But for a path at D = 6.0 Å, the error in amplitude would be 14%. High-D paths also oscillate much more rapidly in k-space, causing them to be more sensitive to a proper choice of energy origin. Thus, the correct treatment of E o and energy broadening are more important for high-D paths. (As another example of a high-R effect, Kitsune will show us how an oversimplified fitting model can affect high-R data more than low-R data in Box 12.5.) Geometrical considerations also dictate that multiple-scattering paths become more numerous relative to single-scattering paths as D increases. For many materials, it can be difficult to know what constraint scheme is most appropriate for multiple-scattering paths (see Section 14.5 of Chapter 14), and as their relative importance increases, the question becomes more pressing. (Mandelbrant will show a graphical example of this in Box 12.5.) Finally, extending fits to higher R (and thus including paths at higher D) is computationally taxing.

Art of Fitting:
As Field Marshall Helmuth Carl Bernard Graf von Moltke wrote, "No plan of operations extends with any certainty beyond the first contact with the main hostile force"

Perfecting your Fit:
Often, you will find that early in the fitting process the fit seems very fragile-perhaps it only works for a certain k-weight or when a certain path is excluded. But as the process continues and you get a feel for the data and the model, it will seem to become more robust. By the time that you're satisfied that you have a good fit, you are likely to have several 12.4 Fit! 333

Box 12.6 FITTInG AdVICe:
My take is: be fearless. Nobody's hovering over your shoulder when you're doing fits; you don't have to have a reason for the stuff you try. Follow hunches, try wacky things, explore the fitting space. Once you start to understand how to fit a set of data, there will be plenty of time to confirm your conclusions, stress test your fit, and construct a convincing argument.One should keep proper notes. Much as it pains me to admit, Dysnomia and the Field Marshall are correct: much of fitting is improvisation. Because of that, it is very important to take notes as one fits, both on the attempts one is making and on what results arise.Continuously evaluate your results and your constraints, using the methods of Sections 11.7 and 11.8 of Chapter 11. You can waste a lot of time by constructing a fit where the fitted model matches the data beautifully-and makes no physical sense. I suggest you do more than just fit. As you begin to learn more about your system through EXAFS analysis, you may think of aspects of the system that you or your colleagues could investigate in other ways. Perhaps, in a soil sample, the fits suggest the presence of a compound you did not expect. Before continuing too far, consult with a soil scientist who knows something about your sample to see what they think. Or maybe a fit raises questions about morphology that a look at transmission electron microscopic images would address.One way or another, friends, we should start simple, either by beginning with very few paths or by starting with very few free parameters. A fit with too many free parameters early in the process is only likely to cause confusion. We can always add complexity as we go.I've got one! Don't be afraid to make mistakes. I bet I can learn as much about my system (and how to get better at fitting) from things that don't work as things that do. good fits, in the sense that while details of the fitting model or ranges vary, the results are consistent. You will naturally pick the "best" from this set, using the criteria of Chapter 11. But it's also usual at this stage to systematically explore the fitting space a little bit around the fit you have chosen. Here are some things you could try:• Move the lower end of the k-range up or down, perhaps by 0.5 Å -1 at a time.• Move the upper end of the k-range up or down.• Move the lower end of the R-range up a bit (perhaps 0.2 Å at a time). You can't move it down, though, if you've already set it at the minimum determined by the background subtraction procedure.• Move the upper end of the R-range up or down a bit.• Try a different (but reasonable) background subtraction.• Try relaxing a constraint that you're not sure about. For example, if you've forced all MSRD's beyond the nearest neighbor to adopt the same value, try splitting off the next-nearest neighbors.• Try adding a constraint. For example, if two related MSRD's adopt values that are consistent to within their uncertainties, try constraining them to each other.• Try adding multiple-scattering paths that you'd previously considered negligible or paths that have most of their amplitude above the range over which you are fitting.While you don't have to do all those things, each one that you do try provides you more fits to choose from, and more of an opportunity to use the approaches of Chapter 11 to pick the one you like best.

stressing your Fit:
As the Nobel Prize-winning physicist Richard Feynman said, "The first principle is that you must not fool yourself-and you are the easiest person to fool" (Feynman 1997). So once you have a fit that you really like, the next step is to try to prove it wrong.The process for this is very similar to the process we just described in Section 12.4.3 except that now instead of looking for the best fit, you're looking for other perfectly reasonable fits that yield different results: in other words, you're checking the "stability" criteria from Section 11.6 of Chapter 11. Picture yourself as a rival researcher trying to knock down your argument, and try to do things to the fit that give a different result that is also plausible.In the end, you will often find that you have a set of conclusions that are quite robust. In the example we introduced in Section 12.1.2, for example, you might come to a firm conclusion that the manganese is substituting for the zinc in a wurtzite-type structure. You might also have conclusions that are fairly robust, but leave some room for doubt-perhaps you have needs to be able to construct a potential (most likely a muffin tin, as in Section 9.2.1 of Chapter 9). Depending on the algorithm used, the calculation may incorporate aspects such as charge transfer between atoms, overlap of the atomic potentials, and values for the interstitial potential. If we only feed the calculation the absorbing iron atom and the six oxygen atoms immediately surrounding it, those aspects are likely to be approximated poorly; that is, we would be calculating a theoretical standard for an FeO 6 molecule, which is significantly different from an FeO crystal.How many more shells would we need to include in the calculation to get the first shell nearly right? To see, examine FigureAs expected, including only the near-neighbor oxygen atoms in the calculation yields a noticeably different result from using a large cluster, especially at low k. But it is intriguing that even when the second shell of iron scatterers (and all atoms interior to them) is included, the amplitude is still off by nearly 5% for the peak just below 4 Å -1 . When we consider what a 5% amplitude error means to, for example, the determination of coordination numbers we can see that this is a significant source of systematic error.Convergence has essentially been achieved, however, by the time the third shell of iron atoms is added. For the peak just below 4 Å -1 , the amplitude of the "three iron shells" theoretical standard only differs from that of the "five iron shells" theoretical standard by 0.3%.It is also worth noting that by 7.5 Å -1 , the systematic error introduced by using a smaller cluster is much more modest: even the "one iron shell" amplitude is only off by about 0.5%. (The "near-neighbor oxygen atoms only" standard, however, still exhibits a 5% amplitude error, along with a modest phase shift.)Please notice, friends, that FigureNow suppose we were trying to model an amorphous iron oxide and we think the iron valence is close to +2, and the coordination is mostly octahedral. Since the material is amorphous, the radial distribution function is likely to be quite different from that of crystalline iron(II) oxide. We would not use a top-down approach-we are really just interested in a path to model the near-neighbor oxygen scattering, and perhaps the iron-iron scattering from the shell beyond that. The exact locations of the oxygen and iron atoms beyond that first iron shell are irrelevant to our analysis.And yet, if we constructed a cluster of an iron atom surrounded by 6 oxygen atoms, or even an iron atom surrounded by 6 oxygen atoms and 12 iron atoms beyond that, we could be introducing significant error into the nearest-neighbor path. Instead, we would want to create a cluster extending out several additional shells. As long as we had to do that anyway, why not just use the iron(II) oxide crystalline structure? That's why it's often a good idea to use a similar crystal as a starting point for analyzing an amorphous material, even if we plan to use a bottom-up approach (Section 9.4.1 of Chapter 9).

Cluster Size and XaNeS:
When learning XAFS analysis, it is easy to get the impression that XANES depends primarily on the absorbing atom and its nearest neighbors; after all, those are the characteristics most often identified by XANES fingerprinting (Sections 6.3 and 6.4 of Chapter 6). But FigureUnlike for the EXAFS shown in Figure

Sources for Crystal Structures:
While it's possible to find crystal structures by searching the primary literature, it is often efficient to first search databases such as the Crystallography Open Database

CalCulated StruCtureS:
Many techniques now exist for optimizing the structures of small molecules, atomic clusters, crystals, and solutions. It is, therefore, usually possible to take a rough guess as to the arrangement of atoms in a material and compute a set of stable atomic positions, perhaps even including MSRDsRealize that the process described in Section 13.2 is generally a two-step process: first you use one piece of software to generate the coordinates for a reasonable starting structure and then another (such as feff) to compute a theoretical standard that corresponds to that structure. We've got to leave it there, Simplicio. Calculating structures for a particular material is about as discipline-specific as it gets. It is really a question for textbooks on the individual systems being studied, not for a book on XAFS.We can at least provide a few examples from the literature, even if the list is in no way comprehensive: if you're interested, you can check out

MIXtureS:
What if your sample comprises a mixture of materials contributing to the measured edge, such as was the case in the examples from Chapters 7 and 8? Then, just as in those chapters, we can treat the spectrum as a linear combination of standards, only this time, they will be theoretical standards.The EXAFS equation could then be written as where the outer summation is over the different constituents j of the mixture, each with fraction X j . As in Section 7.1.1 of Chapter 7, the fraction X j is the fraction of the absorbing atoms present as constituent j.In principle, a mixture can now be fit in the same manner as a pure-phase sample. There is more than one theoretical standard, each weighted by X j . The X j 's can be fitted or constrained to values determined in other ways. At least one constraint can certainly be applied:While EquationOne way to modify Equation13.3 Mixtures 343

BoX 13.2 aN eXaMPle:
Perhaps an example will be useful, friends. Suppose we have a material where 24% of the cadmium is present as monteponite and 76% as greenockite, as in Section 7.3 of Chapter 7. Formally, we would wish to take each path in the theoretical standard for monteponite and multiply it by 0.24, and add the result to each path in the theoretical standard for greenockite multiplied by 0.76. This is what EquationBut a lot of XAFS software won't let us do that. Instead, we've got to work with the regular path parameters. So here's what Equations 13.3 and 13.4 tell us we can do. Suppose, the S o 2 for cadmium in the measurement is 0.87. (Remember, S o 2 shouldn't be different for the same edge of different constituents.) We could use the paths for monteponite, but make the S o 2 for monteponite 0.24 × 0.87 = 0.21, and use the paths for greenockite, but make its S o 2 0.76 × 0.87 = 0.66. To a fitting program, it looks like one big theoretical standard with a lot of paths, some of which have different S o 2 's. It doesn't make any physical sense, but it works.Hrumph. I would prefer something with a more physical basis. And that can be done using Equations 13.5 and 13.6. Monteponite takes on the rock salt structure (Section 13.1.1), so an absorbing cadmium atom in pure monteponite has six nearest-neighbor oxygen atoms. Since 24% of the cadmium atoms are in that environment, the average degeneracy for that path is 0.24 × 6 = 1.44. The next-nearest-neighbor path in monteponite is made up of 12 cadmium atoms, so in our mixture that path would have an average degeneracy of 0.24 × 12 = 2.88. Greenockite, on the other hand, takes on the wurtzite structure (Section 12.4.1 of Chapter 12). In pure greenockite, there would be four nearest-neighbor sulfur atoms. In our mixture, the average degeneracy of that path would be 0.76 × 4 = 3.04. All paths in both theoretical standards, including multiple-scattering paths, can be handled analogously and doing it this way gives the parameters a physical meaning.In either method, the fraction of each constituent in the mixture could be a free parameter of the fit.Whichever method one uses, however, the results should be reported in the same way. Dysnomia would report S o 2 as 0.87, not 0.21 or 0.66, and Carvaka would report the coordination number of the near-neighbor oxygen atoms in monteponite as 6, not 1.44.

INeQuIValeNt aBSorBING SIteS:
Some chemical compounds or crystal structures have the same element present in different environments within the same structure. For example, in Section 6.2 of Chapter 6 we discussed the spinel structure, in which cations can occupy tetrahedral or octahedral sites. Consider magnetite, a spinel with chemical formula Fe 3 O 4 . In this material, one-third of iron atoms are in tetrahedrally coordinated sites, while two-thirds are in octahedrally coordinated sites.For EXAFS, this can be thought of as if it were a mixture of two constituents and the methods of Section 13.3 applied.

HIStoGraM MetHodS:
Up until now, we have accounted for thermal and static disorders by relying on the MSRD in the EXAFS equation, possibly with the modifications introduced by higher-order cumulants (Section 10.2 of Chapter 10). In some systems, however, the cumulant expansion does not provide a good description of the actual distribution of distancesFor these cases, one approach is to divide nearest-neighbor bond lengths into ranges (e.g., 1.90-1.91 Å, 1.91-1.92 Å, 1.92-1.93 Å …) and calculate a theoretical standard for a configuration corresponding to each. This is sometimes known as a histogram approach

MultIPle-edGe FItS:
Often, it is possible to measure more than one edge of a sample (e.g., the nickel K edge and the iron K edge). Both edges can then be fit simultaneously.In Sections 13.3 through 13.5, different environments around absorbing atoms of a particular element were contributing to the same set of data. For multiple-edge fits, in contrast, the environment around each kind of element is contributing to different sets of data.What, then, makes this a "simultaneous" fit? It must be that constraints are used to couple the two edges to each other. Consider, for instance, an alloy of copper and zinc. The theoretical standard for the copper K edge will include a path for scattering by nearby zinc atoms, while the theoretical standard for the zinc K edge will include a path for scattering by nearby copper atoms. It is geometrically necessary that those paths be the same length, have the same MSRD, and exhibit the same cumulants. This is not quite as straightforward as it may sound, however. We are ask- This approach is new to me, and I'm a little confused as to the point. If I am using a theoretical method to calculate the distribution of bond lengths, why I am using EXAFS at all? What information am I extracting from the measurement itself?Fitted parameters can still be introduced. For example, an average bond length could be fit by introducing a parameter that changes all the bond lengths in the histogram by the same fraction. Or an additional MSRD adjustment can be applied, allowing for more or less disorder than in the theoretical calculation. Or the results of the calculations can be used to fit the distribution as a superposition of a small number of Gaussian peaks, each of which could then be treated as an ordinary path with the usual adjustable parameters.The conventional EXAFS equation makes the assumption that the distribution of lengths for a given path can be modeled well by a Gaussian. The histogram method can be thought of as simply allowing one to substitute a more appropriate distribution function.And we should not forget a simple application, my friends. Even without free parameters, we can use the EXAFS to confirm or refute the theoretical computation. This in itself provides us valuable information about our system.acute if different k-weightings or different k-ranges are applied to the data sets associated with the different edges.Despite the frequent appearance of multiple-edge EXAFS fits in the literature, this issue has not, to our knowledge, been adequately addressed.Fortunately, there is a solution which is relatively easy to implement.Recall from Section 11.3.1 of Chapter 11 that when we are done fitting, we generally assume we have a good fit and use the preliminary value of χ ν 2 to estimate ε. If we follow the procedures from Chapter 11, then for each edgewhere we use one of the usual ways to estimate ε random,m (see Section 4.2.2 of Chapter 4) for each edge. We then rerun the fit using our new estimates of ε m for each edge. If necessary, we can repeat the process iteratively until the estimates for ε m stabilize.

SIte oCCuPaNCy:
We are often faced with materials that adopt a crystal lattice, but with substitutional disorder. For example, an iron-nickel alloy might adopt the nickel crystal structure (i.e., face-centered cubic), but with roughly 20% of the sites occupied by iron atoms. If the iron atoms don't form a regular superstructure but are scattered about more or less at random, we would say that the material adopts the nickel structure with site occupancy of 80% nickel and 20% iron. The lattice constant of the alloy would differ from pure nickel, and there might be short-range distortions such as a modest difference between the nearest-neighbor distance of a nickelnickel pair and a nickel-iron pair, but the face-centered cubic nickel structure would provide an excellent starting structure for calculating a theoretical standard.Substitutional disorder can also involve vacancies, that is, site occupancies that total to less than 100%.

Vacancies:
Vacancies are the easiest to model, so we'll address them first. They can be modeled as a reduction in path degeneracy. For example, holmium arsenide adopts the rock salt structure we discussed in Section 13.1, but can include arsenic vacanciesWhile fitting two (or more) edges simultaneously adds substantially to the number of independent points by the Nyquist criterion, the information is likely to fall substantially short of ideal packing (Box 11.1 of Chapter 11).

Chapter 13 - Starting Structures:
The right reason to use multiple-edge fitting is to take advantage of constraints between the edges. If, instead, you're tempted to fit more than one edge simultaneously so that you can "borrow" independent points from one edge to help fit the other, you're abusing the fact that your data aren't ideally packed! Notice that it does not matter what the absorbing atom is. A singlescattering path at the arsenic edge with a holmium scatterer would not be reduced due to arsenic vacancies, as data reduction has already normalized the data to the size of the arsenic signal.

treating as a Mixture:
When a site may be occupied by more than one element, one strategy is to compute a theoretical standard for each possible occupant. For example, let us return to the example from the beginning of this section of an alloy, that is, 20% iron and 80% nickel. When analyzing the nickel edge, one standard could be computed from a face-centered cubic structure with every site occupied by nickel, while another could be computed from a face-centered cubic structure with every scattering site occupied by iron. (The absorbing site should always be occupied by the atom corresponding to the edge being analyzed.) The problem can then be treated in the manner of Sections 13.3 and 13.4.

Creating a Mixed Model:
An alternative to the method in Section 13.7.2 is to use a model structure that already contains atoms of both types. For example, consider attempting to model the nickel edge of the 20% iron, 80% nickel alloy we have been discussing. In the face-centered cubic structure, the nickel absorber has 12 nearest neighbors. A model could be built with three of those near neighbors occupied by iron atoms and the remaining nine by nickel atoms. Since that makes 25% of the neighboring sites iron, rather than 20%, the degeneracy of each of the iron-direct scatterers would need to be multiplied by f/25 = 20/25 = 0.80, while the degeneracy of each of the nickel-direct scatterers would need to be multiplied by (100 -f )/75 = 80/75 = 1.07. More distant scatterers can be treated analogously. For example, a triangle path involving scattering off of a nickel atom followed by an iron atom would

Creating Multiple Mixed Models:
The most accurate approach is to create theoretical standards based on multiple mixed models, with the substituted atoms placed in various possible configurations. To take advantage of this level of detail, it is probably best to use theoretical tools to predict the distribution of substitutional disorder, much as theory is used to improve the modeling of absorberscatterer distances in the histogram method (Section 13.5).If the absorbing atom is found in more than one site or constituent, as in Sections 13.3 and 13.4, then vacancies can change the fractions X j . But EquationYeah, I might be a little nervous to use it with an alloy of say copper and mercury, because they have very different atomic weights and so the scattering is very different. But iron and nickel? The scattering differences are subtle anyway. The method of Section 13.7.2, while an approximation, would probably be good enough there, and it's a lot less work than figuring out all the proper ratios required by the method in Section 13.7.3.

Constraints:
The Multi-Tool of Modeling Why do we need constraints? Shouldn't we just let the fit sort everything out? There are usually not enough independent points in the data (Section 11.1.1 in Chapter 11) to let all one's parameters be free, my dear Simplicio.And constrained fits are usually more stable.There may be physical reasons that a constraint must be true. For example, the distance from atom A to B must be the same as the distance from atom B to A. Not including constraints that enforce those kinds of truths withholds knowledge from the fits.You may get information about your system from other sources, such as other experimental probes. This should be fed into the model as well.  • The distance from A to B is the same as from B to A. This can be used, for example, to constrain the path from A to B back to A (measured at the A edge) to be the same length as the path from B to A back to B (measured at the B edge). This also means that the MSRDs and higher cumulants of those two paths must be the same.• Geometrical constraints can be based on the symmetry of the material. If, for example, all iron sites in a crystal are known to be equivalent, and the length of iron-iron path B is twice that of ironiron path A in the model structure, then if path A is lengthened by an amount x, path B must be lengthened by an amount 2x.• The fraction of the atoms of an element that are present in each constituent or site must add up to 1.• Certain multiple-scattering constraints are also rigorously true;we will cover those in Section 14.5. the chromium species present in a soil sample. Because chromium (VI) exhibits a strong, distinctive pre-edge feature (this can be seen in Figure

CONSTRAINTS fOR SImPLIfICATION:
At the start of this chapter, our panel provided us several different reasons why constraints are useful. Those reasons can broadly be divided into two categories: constraints add information to a fit, and they simplify a model. Sections 14.1 and 14.2 provided examples of constraints that add information. We almost always want to use constraints of this type when possible.Constraints that are intended to simplify a model, on the other hand, have pros and cons. On the positive side, they increase the degrees of freedom of a fit, increase a fit's stability, make statistical tests more likely to show significance, reduce the error bars on fitted parameters, and can sometimes make a model more defensible. On the other side, they can introduce bias into a fit, reduce our ability to detect aspects of our material's structure, increase a fit's R-factor, and can sometimes make a model less defensible. Choosing how much to use constraints of this kind is thus intimately connected to deciding how to weight the criteria for a good fit we discussed in Chapter 11.

Constraints Based on Grouping:
Usually, simplifying constraints involve grouping parameters together. For example, consider a measurement on the iron K edge of a mineral incorporating iron, oxygen, and sulfur. A theoretical model might include many single-scattering paths, including both sulfur and oxygen near neighbors; sulfur, oxygen, and iron next-nearest neighbors; and more distant paths. There are several ways these paths could be grouped, as suggested by the Venn diagram in FigureThere are thus multiple ways to constrain the MSRDs of the paths in this theoretical standard:• Force the MSRD for all paths to be the same, giving only one fitted parameter.• Allow one fitted MSRD for near neighbors, and another for all other paths.

Constraints for Simplification 351:
Often, your a priori knowledge may be somewhat uncertain. In such cases, it might be better to use a restraint (see Section 14.6.1) than a constraint.Sometimes, though, it is helpful to try removing a rigorous constraint as a check on fit stability (see Section 11.6 in Chapter 11). If, for example, a multi-edge fit does not report that the distance from A to B is the same as from B to A (within the specified uncertainties, of course), it is a warning flag that there may be other problems with the fit.• Allow one fitted MSRD for scatterers with an oxidation state of -2 (i.e., oxygen and sulfur), and one for iron scatterers.• Allow one fitted MSRD for each scattering element (making three in all).• Allow one fitted MSRD for near neighbors, one for next-nearest neighbors, and one for more distant paths.• Allow one fitted MSRD for near-neighbor oxygens, one for near-neighbor sulfurs, and one for all other paths.• Allow one fitted MSRD for near-neighbor oxygens, one for near-neighbor sulfurs, one for more distant oxygens, one for more distant sulfurs, and one for irons.• Allow one fitted MSRD for near-neighbor oxygens, one for nearneighbor sulfurs, one for more distant scatterers with an oxidation state of -2, and one for irons.• Allow one fitted MSRD for near-neighbor oxygens, one for near-neighbor sulfurs, one for next-nearest-neighbor irons, one for more distant irons, and one for all scatterers with an oxidation state of -2 that are not near neighbors.• Allow one fitted MSRD for near-neighbor oxygens, one for nearneighbor sulfurs, one for next-nearest-neighbor oxygens, one for next-nearest-neighbor sulfurs, one for next-nearest-neighbor irons, one for more distant oxygens, one for more distant sulfurs, and one for more distant irons.We have not exhausted all of the reasonable possibilities for ways to group the MSRDs, but you get the idea! To decide which to use, you need to evaluate fits using the criteria from Chapter 11. All of the listed possibilities are defensible, so other criteria will have to come into play. One trade-off to consider is the number of parameters to allocate to a category that can be grouped in various ways. In the above example, it is likely that one MSRD for all single-scattering paths will not work very well; most structures require more flexibility than that.On the other hand, the last option in the list, which allocates eight free parameters to MSRDs, probably has more detail than the fit can support. Successful constraint schemes are likely to lie somewhere in between.As a second example of constraints that simplify by grouping, consider a mixture of iron metal, an amorphous iron oxide, and an iron carbide. How should the E o 's be constrained in the theoretical standards? Some possibilities are as follows.• Force E o for all paths to be the same, giving only one fitted parameter.• Allow one fitted E o for each constituent.• Allow one fitted E o for the metal and the carbide, since they are both zerovalent, and one for the oxide.• Estimate the difference between the E o for the oxide and the E o for the zerovalent forms, using either fitting to standards or theoretical computations, and use the estimate to constrain the difference in E o between constituents. This requires only one free parameter, but does account for the effect of oxidation.• Allow one E o parameter for the oxygen scatterers in the oxide, one for the iron scatterers in the oxide, one for the carbon scatterers in the carbide, one for the iron scatterers in the carbide, and one for the metal.Again, this list is not comprehensive, but does give a sense of some of the trade-offs and options.

Constraints Based on Estimates:
Another type of simplifying constraint is that based on an estimate. For example, S o 2 might be constrained to some reasonable value such as 0.9; bond lengths might be fixed at the values provided by XRD or "typical" values for the bond in question; or vacancies and surface effects might be neglected (thus fixing oxidation numbers to those of a bulk crystal).When using constraints of this type, it is crucial to modify your reported uncertainties accordingly. For example, suppose you set S o 2 to 0.9 and then fit the coordination number of the nearest neighbors in a single-shell fit and your fitting software, using an algorithm based on the method described in Section 11.3 of Chapter 11, reports a coordination number of 5.7 ± 0.6. The use of an estimate like 0.9 for S o 2 might be reasonably assumed to have an uncertainty of 15%. Since coordination number is entirely coordinated with S o 2 for a single-shell fit, the 15% uncertainty Constraining a correlated parameter to an estimated fixed value and then not including the uncertainty in that estimate in the error budget of correlated parameters is one of the most common errors in the literature! If you are reviewing a paper that does this, make sure to point it out and ask for the problem to be fixed before publication.applies to the coordination number as well. 15% of the best-fit value of 5.7 is 0.9. This uncertainty of ±0.9 is independent of the ±0.6 that came from the fitting algorithm, so they should be added in quadrature, giving an overall uncertainty of 0 9 0 6 1 1 2 2 . . . + = . Thus, the coordination number found by the fit should be reported as 5.7 ± 1.1.Of course, the calculation in the previous paragraph depended on the two parameters being completely coordinated, while in most cases (e.g., S o 2 and σ 2 ), the correlation is only partial. In those cases, the uncertainty due to the simplifying constraint can be ascertained by forcing the constrained parameter to the two ends of its range and observing the effect on the best-fit values of other parameters. For example, if constraining S o 2 to 0.75 gives a best-fit value of 0.012 ± 0.001 Å 2 for σ 2 and constraining S o 2 to 0.75 gives 0.016 ± 0.002 Å 2 , then reporting σ 2 as 0.014 ± 0.003 Å 2 is reasonable.

Constraints Based on Standards:
Another simplifying strategy is to fit a related standard, and use the results to constrain values for the sample. In fitting the zinc K edge of an aluminum-doped zinc oxide thin film, for instance, it is reasonable to measure the zinc K edge of a pure zinc oxide thin film of known structure (i.e., an empirical standard), fit the standard with S o 2 as a free parameter, and then constrain the S o 2 for the doped sample to have the same value.While this strategy is very commonly used for S o 2 , it can also be used for other parameters. For example, if your sample is unlikely to be in a different oxidation state from that of your standard, it might make sense to constrain E o in this way.Sometimes the constraints may involve the relationship between parameters rather than individual values. For example, suppose a sample consists of rutile titanium oxide doped with terbium, and is measured at the titanium K-edge. The pure rutile structure is shown in Figure

Chapter 14 -Constraints:
You may be wondering how we got an uncertainty of 0.003 Å 2 in the last example shown in Section 14.3.2. The answer? We eyeballed it. It doesn't really matter what fancy statistical technique you use; if the software is reporting uncertainties of a thousandth of a square angstrom or two, and changing the correlated parameter causes the fit to change by up to two thousandths of a square angstrom from the reported value, then it's pretty clear that the overall uncertainty is about 0.003 Å 2 . It's not necessary to get too picky with these uncertainty estimates, as long as you're not lowballing them by leaving off altogether the uncertainty due to correlation with an estimated parameter. Don't forget to account for the uncertainty in the fitted value taken from the standard when reporting the uncertainties in parameters fitted for the sample! This is done in essentially the same way as we discussed in Section 14.3.2. For the sake of discussion, let's label the single-scattering paths in this way:• Call scattering from the six near-neighbor oxygens Path O1.• The next nearest atoms to the absorber are the two titanium atoms along the c-axis (the c-axis runs roughly perpendicular to the page as shown in Figure• Call the path to the eight remaining titanium scattering atoms in Figure• Call the path to the eight remaining oxygen scattering atoms in FigureNote that the length of paths T2 and O2 are very similar; they will show substantial overlap in the Fourier transform. Path T2 will probably show the greater amplitude, because titanium has an atomic number that is significantly higher than oxygen.Let's suppose that a process like that described in Section 14.3.1 has resulted in three MSRDs being fit for the pure rutile standard: one for O1, one that is shared by T1 and T2, and one for O2. This might work fine for the pure rutile. But compared to the standard, the doped sample might exhibit more disorder, resulting in higher MSRDs. This might make the contribution from O2, while still enough to affect the spectrum, very difficult to stably fit, particularly because of the overlap with T2. In addition, the presence of an atom of terbium somewhere in the structure must also be considered, adding more free parameters to the fit.Under those circumstances, it's highly desirable to find a way to constrain the MSRD for O2 in the doped sample. But, just forcing it to adopt the value from the standard won't work, because the sample is much more disordered than the standard.One solution is to assume the disorder affects the MSRD for O1 and O2 to the same extent, so that We can thus use the results from a fit on the pure sample to express the MSRD for O2 in the doped sample as a multiple of O1, reducing the number of free parameters by one.

Constraints for Simplification 355:
In this section, we're discussing using standards to constrain fits to samples. But one thing you should almost never do is use previous fits to a sample to constrain values of parameters for subsequent fits to that sample! For example, it is not acceptable to fit the first shell, and then constrain the parameters for the first shell to their best fit values before fitting the second. Doing so builds in systematic biases and wreaks havoc with statistics, including uncertainties! Well, I can think of a couple of exceptions, Mr.Handy. For example, if I'm not sure whether the coordination number of my nearest neighbors is four or six, and I convince myself from a single-shell fit that it's six, I might then constrain it to that value, because that's more a choice between structures than something like a bond length or an E o would be. But your basic point is good.the theoretical standard. Substituting a dopant or changing temperature or pressure, for example, might expand or contract the lattice by, say, 5%, but that change to the lattice scale should make every path longer or shorter by the same percentage.That's not to say that atoms can't change relative positions within the unit cell under those circumstances, necessitating different free parameters for the half path length of different paths. But a uniform lattice scaling can serve as a useful first approximation for changes relative to the theoretical standard when pursuing a top-down strategy (see Section 9.4.2 in Chapter 9).A uniform lattice scaling can also work fairly well, particularly for relatively short paths, when the unit cell is only slightly distorted from cubic (e.g., there is a modest tetragonal distortion) or ideal hexagonal (e.g., c/a is slightly smaller or larger than 1.633). It is unlikely to work, however, when the starting structure for the theoretical standard is a molecule rather than a crystal; such systems often contort under stress rather than expand or contract uniformly.

Correlated Debye model:
Several attempts have been made to theoretically predict MSRDs for systems fulfilling certain special requirementsIn most cases, this has been done primarily to test the accuracy of either EXAFS theoretical standards or the theory used to make the prediction, rather than as a practical method for generating constraints for MSRDs for use in fitting unknown samples. The correlated Debye model (Beni and Platzman 1976), however, has been shown to have utility for an important class of materials, namely, cubic monatomic metals where the atoms reside on a Bravais lattice such as face-or body-centered cubic metalsWhile the Debye model only applies quantitatively to a relatively small fraction of the materials studied by XAFS, it is interesting to examine its dependence on several parameters of interest.

Some Special Cases 357:
You may be wondering why anyone would bother using EXAFS to analyze a plain-vanilla metal such as copper or iron. After all, we already know their structure! While that's true, they play a role in more interesting systems, often as constituents in a mixture. Examples may be found among nanostructured materials, cathodes, and biogeochemical systems.Let us begin by examining the dependence on the half path length D. FigureWhile the details of the plot are particular to the correlated Debye model, some of the qualitative features apply to a wide variety of systems:• The nearest-neighbor scattering path has an MSRD that is significantly smaller than paths further out. This is because bonded atoms tend to move in a correlated manner, making the bond length less variable than would be expected if the atoms moved independently.• Most of the distance dependence is found in the first few shells.Although a gradual increase with D can be seen over the entire range in Figure• Triangular multiple-scattering paths exhibit similar MSRDs as single-scattering paths with the same half path length.Next, let's consider the temperature dependence, shown in FigureThe temperature dependence is roughly parabolic well below the Debye temperature, and linear above it. This functional dependence on temperature is observed in a broad range of systems, even when the Debye model itself does not applyIf data are collected on a sample as a function of temperature, therefore, it may be worth seeing if it can be fit with a quadratic function at low temperatures, a linear function at high temperatures, or both. The spectra for multiple temperatures can be fitted simultaneously, and the simple functional dependence, if present, can thus be used to reduce the number of free parameters in the fit. Spectra of the same edge of the same sample taken at different temperatures are certainly not independent, and the number of independent points should not be taken to be that given by the Nyquist criterion (Section 11.1.1 in Chapter 11) multiplied by the number of spectra! A good rule of thumb is to make sure that each spectrum has enough independent points by the Nyquist criterion to fit all the parameters that directly impact the fit to that spectrum.In addition, observing the temperature dependence of the MSRD can give an estimate of how much would remain at 0 K. While this residual MSRD is partly due to zero-point thermal motion (that is, the origin of the zero-point value shown in Figure

focused Paths:
In Box 9.3 in Chapter 9, we introduced the concept of a focused path. For convenience, we reproduce FigureWhile atom B affects the amplitude and phase shift of the path starting at A, it does not affect its half path length, MSRD, or higher cumulants, which are the same as for the single-scattering path from A to C back to A. This provides a constraint on those parameters; a focused path of this type does not add any free parameters to a fit. What if the path starts at B instead? The path is still geometrically identical to the path from A to C back to A. If either A or C is an absorbing atom in our model, either because one of them is the same element as B or because we are performing a multiple-edge fit for which either A or C is an absorbing atom, then we can still constrain the geometry for this path rigorously. If not, we have to handle it more like a triangle path (see Section 14.5.4). While Figure360 Chapter 14 -Constraints BOx 14.2 hOW SmALL IS SmALL?How good is the approximation? How far off can B get from the midline before we shouldn't use it?It is based on the small-angle approximation, dear Kitsune. The triangle ABC has two small angles, one with its vertex at A and one at C. To the extent that those angles are small, to first order the distance from A to C is the same as the sum of the distances from A to B and B to C. An estimate of the accuracy of this approximation can be determined by seeing how much cos θ varies from 1 for the small angles. For example, if the angle with the vertex at C is 10°, cos 10° = 0.985, suggesting the approximation is good to something like 1% or 2%. Even at 30°, the approximation is good to about 10%-15%.And that's the error in the difference from the starting structure, not the error in the half path length! For example, the half path length for the multiple-scattering path in the starting structure might be 2.45 Å. That number is exact for the starting structure, and accounts for the geometry. If fitting finds that the single-scattering path from A to C to A should be made 0.04 Å longer, then assuming that the multiple-scattering path should change length by the same amount represents an error of only around 15% of 0.04 Å, or less than 0.01 Å. That's probably OK for most fits.Thanks-that tells me the accuracy of constraining half path lengths. What about MSRDs?Let us find out, friend Kitsune. To estimate this, let us start with a set of simple assumptions. We'll treat the three atoms as if they move in an uncorrelated, isotropic, harmonic manner, each with the sample amplitude. While this is certainly not the case, it will help us understand how sensitive the MSRD is to the geometry. In the interest of simplicity, let us also place B equidistant from A and C. We can then place A at the origin, C at the coordinates (D,0), and B at the location (D/2,y). The half path length D MS is thenAs a check, at 30° EquationThe case where the focusing atom is equidistant from the others is relatively favorable for keeping the deviations from the single-scattering case small.) Differentiating with respect to D gives1 4 (14.5)

Double Paths:
Go high enough in D, and you're likely to encounter a path that looks like FigureThis strange-looking path involves the photoelectron traveling from the absorber A to a scatterer B, then back to A, back to B again, and finally back to A once again. There's no widespread terminology for this kind of path, so we'll call it a double path.Unlike focused paths, double paths are not particularly high amplitude, but the double path off of the nearest neighbor can sometimes have Because we have assumed the atomic motions are uncorrelated, isotropic, and the same for all atoms, σ σwhere σ 2 is the MSRD for the single-scattering path from A to C back to A. Since we are treating the focused case, y << D. After a bit of mathematics we get, to lowest order in y/D,This provides us the estimate we sought. If the angle were 30°, then y/D = 1/2 tan 30° = 0.29, and EquationIn reality, the MSRD for the focused path is likely to be even closer to that of the singlescattering path than is shown in your simple model, Mandelbrant. You've treated the motions as uncorrelated, but when atom B moves down, shortening the path, atoms A and B are likely to move apart, lengthening it. When atom B moves up, the reverse will occur. So, the addition of atom B to the path causes smaller variations in path length than your uncorrelated model assumes.The bottom line, then, is that for almost all purposes we're OK constraining a path where the small angles of the triangle are 10° to have the same changes to path length and MSRD as the corresponding single-scattering path. If the small angles get up to 30°, though, the approximation (particularly for the MSRD) is getting a bit iffy. Fortunately, the amplitude of paths with angles that big also tends to be smaller! an effect on your fit. Fortunately, double paths can be rigorously constrained: the half path length is twice that of the corresponding singlescattering path, and the MSRD is four times as great. Even the third cumulant can be rigorously constrained for this kind of path; it is eight times larger than that for the corresponding single-scattering path. The half path length for this path can be rigorously constrained; it is the sum of the half path lengths of the two related single-scattering paths.

Conjoined Paths:
The MSRD is more difficult, however. If the single-scattering paths were uncorrelated, then the MSRD for the conjoined path would be the sum of the MSRDs for the two single-scattering paths. But the paths are not uncorrelated; aside from the possible effects that the motion of B and C might have on each other, A is certainly common to both! If A moves a little to the left in FigureBox 14.2 anticipated this issue: Mandelbrant provided a derivation that accounted for the atom in common, but did not account for the fact that the motions were likely to be correlated, and Carvaka then provided an argument as to the qualitative effect of the correlation. Short of simulating the atomic motion of the entire structure (difficult, since the structure is at least partially unknown if we're investigating it!), a rigorous estimate of the MSRD for this kind of path is difficult! Fortunately, the amplitude of paths of this kind is generally small.

Triangles, Quadrilaterals, and Other minor multiple-Scattering Paths:
This brings us to triangle paths, such as the one shown in FigureIf your model allows the average positions of the atoms to be calculated (e.g., it is parameterized in terms of crystallographic parameters, or in terms of bond lengths and angles), then the half path length can be rigorously constrained. If not (most commonly because the model is parameterized in terms of distances from absorber to scatterers only, without specifying bond angles or positions), a reasonable estimate can probably still be made. The paths shown in Figures 14.8 and 14.9 aren't too far off from being focused. To make it far from focused, we could place atom C over near B, but then, the structure would be crowded. Important triangle paths aren't as big a problem as you might think, because many of them can be approximated as focused. Don't confuse a double path with double scattering, which is a reasonable description of a triangle path (see Section 14.5.4)! There is still a problem, Dysnomia! What if the path starts at A, and neither B nor C are atoms for which we've measured the edges? Even if the atoms were collinear, we couldn't constrain them in terms of an existing single-scattering path, because the path from B to C back to B wouldn't be part of our theoretical standards! While your idea that some of these paths can be thought of as focused is helpful, sometimes we need other solutions. But as with conjoined paths, it is not immediately clear how to constrain MSRDs for triangle paths (or related paths such as quadrilaterals).The correlated Debye model gives us a hint as to how to proceed with this kind of path, as well as other low-amplitude multiple-scattering paths. In FigureTo find out, we will now provide an example using nickel (II) fluoride.The k 3 -weighted χ(k) data are shown in FigureNickel (II) fluoride adopts the rutile structure (Section 14.3.3), with nickel in place of titanium and fluorine in place of oxygen. This structure does not feature a lot of short collinear paths, but does provide several possibilities for triangular paths. The highly polar bonds between nickel and fluorine make it quite different from the monatomic metals that are well-specified by the correlated Debye model, and should thus provide a good test of the effect of small-amplitude multiple-scattering paths in a material where the Debye model does not hold.For our model, we will use the following free parameters:• S o 2 and E o .• The lattice parameters a and c. These two parameters determine the position of all nickel atoms relative to each other.• A parameter u that determines the placement of the fluorine atoms within the lattice.• Three MSRDs: one for the paths to the 6 near-neighbor fluorine atoms, one for the paths to the 10 nearest nickel atoms, and one for all other single-scattering paths. This is the same parameterization that was used as an example in Section 14.3.3. Fits are performed on the data from 5 to 14 Å -1 , fitting from 1.4 to 5.0 Å in the Fourier transform. This gives 20 independent points by the Nyquist criterion, which is plenty to fit the 8 parameters above.As a baseline fit, we will include all important single-scattering paths through D = 7.0 Å, but no multiple-scattering paths. The result is shown in FigureThe fit appears fairly close to the data, and in fact gives an R-factor of 0.0112, well within our informal guidelines for satisfactory agreement (see Section 11.2.1 in Chapter 11). The fitted parameters are shown in TableAll parameters have adopted reasonable values.In fact, this fit generally satisfied the criteria provided in Chapter 11, with one notable exception: the fit does not agree with the data above the fitted range (Section 11.5 in Chapter 11). In fact, examination of Figure364 Chapter 14 -Constraints  Nevertheless, this is probably an acceptable fit for most purposes. Would the inclusion of multiple-scattering paths make it better?To answer that, we added one multiple-scattering path at a time, starting with the highest amplitude path (as determined by curved wave amplitudes by feff6l; seeThe MSRD for focused paths starting at a nickel atom was constrained to that of the related single-scattering path and double paths were handled correctly, but all "difficult" paths (triangle, quadrilateral, focused stating on a fluorine atom) were given MSRDs equal to each other. Each fit was run in two ways: in one set, the MSRDs of these multiple-scattering paths were constrained to be equal to the "Other MSRD" already defined in the fitting model; in the second, they were assigned a new free parameter.The resulting R-factors are shown in FigureWhile the fit was already satisfactory without multiple-scattering paths, adding them improved the closeness of the fit, even without increasing the number of free parameters. To get a sense of the level of significance, let's use the Hamilton test to compare the fit with no multiple-scattering paths to the one with the maximum number shown in Figure14 The mismatch at the top of the fitting range seen in FigureThe R-factors are a bit better yet if we do use a free parameter for the triangle MSRDs, though. Is the improvement due to that free parameter significant?This time, we can use the Hamilton test without hesitation, as we're adding a parameter. We'll compare the fit with all 102 multiplescattering paths but no additional free parameter (that's the one shown in FigureWhen we fit EXAFS, therefore, we should not concern ourselves too much with the effect of individual multiple-scattering paths. A good procedure is to add paths in groups, perhaps by choosing all that are above a certain amplitude.We'll close this section by repeating TableFor seven of eight parameters, the uncertainty is smaller in the fit with the multiple-scattering paths than the one without. The additional paths help to reduce correlations that are otherwise present between parameters. And while none of the parameters shifted by more than the original uncertainties, several shifted by amounts nearly that large. While the last few paths in FigureI use them as additional evidence that a model is correct. If one's model is poor, then there is no reason that adding groups of multiple-scattering paths should result in substantial improvement in the closeness of the fit. That improvement could be added to the list from Chapter 11 as a ninth criterion.Well, even so, maybe I'll just leave the triangle paths off. I'm afraid of constraining them wrong."Leaving them off" is a constraint, and a very poor one-you're constraining their amplitude to zero, which you know isn't right! A great thing about small-amplitude multiplescattering paths is that they're relatively insensitive to reasonable choices of constraints.Of course, the more physically accurate your constraint scheme for multiple-scattering paths is, the better it should fit. We might be able to do even better with the nickel (II) fluoride fit if we used a more realistic model to constrain the MSRDs of multiple-scattering paths. For example, we could construct a model that accounted for the multiplescattering paths that incorporate legs from near neighbors, since the MSRD for the near neighbors is different than that for the outer paths.Sure, Carvaka, but there's a point of diminishing returns. I don't want to go to all the trouble of sorting out the geometry of 102 multiple-scattering paths, just for a little additional improvement! Perhaps you can write some code to do what you're suggesting, Carvaka? Then we'd only have to put the effort in once, and we could automatically apply it to future systems of study.This is an interesting discussion, my friends, but it is time to move on. We have learned enough to incorporate multiple-scattering paths into our fits and have the tools to go further if we wish.

ALTERNATIvES fOR INCORPORATING A PRIORI KNOWLEDGE:
In Sections 14.2 and 14.3, we discussed incorporating a priori knowledge, estimates, or information gained from fitting a standard into a model by using constraints. Often, however, a priori knowledge is uncertain, and likewise we might expect the values of parameters for a sample to be similar to, but not identical to, those for a corresponding standard.Several methods have been developed to address the issue of partial knowledge.

Restraints:
One method for incorporating uncertain a priori information is to penalize the fit to the extent that the fitted value of the parameter deviates from the a priori value, just as the fit is penalized to the extent that the spectra for data and fit don't match. If that is done, the condition is called a restraintFor example, suppose we believe that the nearest-neighbor distance in a material is 2.02 ± 0.03 Å. This means that we would be a bit surprised to find that the bond length is actually 1.98 or 2.06 Å, and quite surprised if it were 1.95 or 2.09 Å.In accord with the suggestion from the IXS that we compute uncertainties in fitted parameters by specifying how much they would need to change to increase χ 2 by 1 (see Section 11.3.1 in Chapter 11), we apply the same principle here, forcing χ 2 to increase by 1 if the restrained parameter differs from the a priori value by an amount equal to the uncertainty in that value.Since we don't know the measurement uncertainty until we achieve a good fit, however, we don't know how to weight the restraint initially. We, therefore, recommend the following procedure for weighting restraints:1. Set the weighting by trial and error for preliminary fits, so that the restrained parameter seems to generally stay within the desired range while still being able to vary within it. 2. Once a satisfactory fit (as judged by the criteria discussed in Chapter 11) is achieved, observe the value of χ ν 2 reported by the fitting software. Weight the restraint so that changing the parameter by its uncertainty penalizes the fit by an amount equal to χ ν 2 ; this corresponds to changing the χ 2 of the rescaled fit by 1, as desired. 3. Of course, re-weighting the constraint may cause the reported value of χ ν 2 to change. Repeating step 2 in an iterative process should allow the fit to converge within a few iterations. If it does not, it is an indication that the restraint may not be compatible with the fit and should be reexamined.

Alternatives for Incorporating a Priori Knowledge 369:
Restrained parameters reduce degrees of freedom, just as free parameters do! Restraints are not a way of getting around statistical restrictions.For a discussion of using restraints to incorporate the concept of bond valence sums, seeThe weighting we suggest in step 2 is equivalent to dividing the uncertainty in the a priori assumption by χ ν 2 ; depending on your software, this may be the way the weighting is adjusted.

Bayes-Turchin Analysis:
In the conventional approach to curve fitting to a theoretical standard, it is up to the judgment of the person doing the fitting to choose what parameters to constrain and which to vary freely. One contributing factor in that decision is the importance of a parameter to the fit; if its value has little effect on the fitted spectrum, perhaps because the path it is associated with has low amplitude, then it is not worth assigning a free parameter to. The process of choosing parameters and constraints is generally iterative, with the scientist deciding on constraint schemes based in part on the behavior of previous fits.The subjective nature of this process is unsettling, and requires considerable experience to do well. While it has been applied successfully for decades, it still seems desirable to find a way to make the progress more rigorous.In Bayes-Turchin analysisIf that were all there was to the approach, it would fail: for multi-shell fits, there are usually not enough independent points in the data to support varying that many parameters. But Bayes-Turchin analysis mathematically selects the parameters that most strongly affect the fit, in effect automating the process of choosing free and constrained parameters traditionally undertaken by the scientist doing the analysis. The merits of this technique are summarized effectively by"The Bayes-Turchin approach makes it possible to start the data analysis with a very large model-parameter space. The method itself, rather than a mere guess, yields the subspace where the data determine the outcome of the fit. This allows the simultaneous analysis of atomic-like background and structure parameters."The method also has a notable disadvantage, however. While a scientist can choose constraint schemes that make physical sense, the Bayes-Turchin method is driven only by the ability to reproduce the fit.Consider, for example, the Bayes-Turchin fit to copper metal provided byDepending on the criterion chosen for significance, the Bayes-Turchin method identified no more than 40 of the 158 potential free parameters as having a significant impact on the fit. So far, this is similar to 370 Chapter 14 -Constraints what a scientist might do using the traditional method. But the Bayes-Turchin method assigned values more than two standard deviations different from the a priori value for the first, second, third, fifth, and sixth single-scattering paths, and not for the fourth or seventh. The MSRD also differed by more than two standard deviations for eight of the multiplescattering paths, distributed throughout the path list. But it is more objective than the traditional method, dear Carvaka. The way a person chooses constraints is haphazard, and leaves more room for biasing the results. I like this method.

REfERENCES:
There's no reason we can't do both. The Bayes-Turchin method, for example, could be applied with the Debye temperature of copper and the temperature of the measurement as potential free parameters, rather than the MSRD for each path. That would prevent the kind of unphysical result Carvaka is concerned about, while still increasing the objectivity of the procedure.

WhAT I'vE LEARNED IN ChAPTER 14, BY SImPLICIO:
• Constraints can be used to add information to a fit, or to simplify a model.• Quantitative constrains and restraints can be based on a priori experimental or theoretical knowledge about the sample, or the results of a fit to a standard. Quantitative constraints should not, however, be based on the results of preliminary fits to the same standard.• Constraints that group parameters together, however, are usually selected by trial and error, using the criteria discussed in Chapter 11 to evaluate which schemes work best. An alternative to this trial and error is the Bayes-Turchin approach.• Most focused and double multiple-scattering paths can be constrained rigorously.• Multiple-scattering paths that cannot be constrained rigorously, such as triangle paths, should usually be included in the fitting model. The exact constraint scheme is not crucial, and usually does not need to involve new free parameters.• Restraints can be used to incorporate uncertain a priori knowledge to a fit.

KNOW YOUR AUDIENCE:
As with any writing or presentation, it's important to tailor your paper or talk to the audience. 𝝌(k) graphs, for instance, would be meaningless to a group unfamiliar with EXAFS, but their absence would be a serious flaw to those who are.Of course, in many situations the audience is mixed, comprising people with a variety of knowledge levels. With presentations, that is not such a big deal-specialists can ask their questions afterward. But with papers it often used to be an awkward problem, particularly in formats where there were length limits.In recent years, however, the increased use of supporting information by journals has helped address the problem. This allows information of interest to specialists to be available online without cluttering up the main article.

EXPERIMENTAL DETAILS:
The following should be included in a paper or its supporting information:• The beamline on which the measurements were made.• The edges that were measured.• The method for harmonic rejection (mirror, percentage of detuning).• Special environmental conditions, such as temperature (if not room temperature).• Measurement mode: transmission, fluorescence, electron yield, quick-XAFS, microprobe, and so on. For fluorescence, the type of detector should also be identified.• The identity and mode of measurement of the energy reference:measured simultaneously with the sample, between each measurement, only at the start of the experiment, and so on.• Some information on sample form. At a minimum, indicate whether it is powder on tape, a pellet, a thin section, and so on.Additional information, such as a summary of the method for assuring small particle size and the number of absorption lengths above the measured edge can also be helpful.• It is helpful to know if scans were averaged to produce the data.While that seems like a long list of items, it can generally be accomplished in a paragraph.Unless they are unusual, scan parameters are often not reported; the signal-to-noise ratio visible in 𝝌(k) speaks for itself.One example where it might not be practical to graph all the data is for quick-XAFS experiments, since you might have hundreds or thousands of scans. In a case like that, graph a representative subset of the data.376 Chapter 15 - Communicating XAFS For presentations, you can use the "hidden slide trick" as if it were supporting data. For an audience with mixed levels of knowledge, for example, you can put the 𝝌(k) data on a slide after your conclusions and only flip to it if someone in the audience asks a question that needs it.

What to Include:
If a paper includes EXAFS, it is necessary to provide 𝝌(k) graphs (with your chosen k-weighting) for all the data analyzed, if practical. In short papers for audiences less familiar with EXAFS, some or all of these can be presented in the supporting information.For XANES analyses, normalized XANES graphs are needed. Always include at least a few representative spectra in the main article; the rest can go in supporting information.Aside from those requirements, you can use some discretion as to what to present graphically.For an audience relatively unfamiliar with EXAFS, for instance, the magnitude of the Fourier transform can be useful, as long as you make it clear that while it's correlated to a radial distribution function, it's not the same thing. For a more savvy group, the real part of the Fourier transform can be more informative.Supporting information also allows data to be uploaded as text files. This can be a great boon to those interested in working further with your results, but is not yet commonplace. We heartily endorse this practice and hope it will become widespread in the future!

Labeling Graphs:
This is a more contentious subject than you might suspect.First off, there are the graphs we have called unnormalized absorption and labeled M(E). Unnormalized data appear in publications relatively rarely, but when it does there is the question of how to label the y-axis. Options such as μ(E), xμ(E), and ln(I o /I t ) have been used, but none are unambiguously accurate. If you use M(E), on the other hand, no one will know what you're referring to unless they've read this book or unless you take the time to explain it in the text. While there is no ideal solution to this quandary currently, it is at least a question worth putting some thought into before creating your graph! Next is the question of the y-axis units. M(E) is dimensionless. Normalized data are not just dimensionless; they've been scaled so that information from M(E) has been lost-we no longer know the edge jump. Some have chosen to label the units on the y-axis of a normalized absorption plot as "arbitrary units" or "a.u." In this text, we label the unit as edge fraction, which is somewhat more specific.Since the convention of normalizing so that the edge jump is one is firmly established, numerical values and tick marks should be provided on the y-axis of normalized data. In this way, readers or viewers can see your choice of normalization for themselves.

Data 377:
It is acceptable, and often advisable, to plot "stacked" data; that is, to shift spectra up or down to provide separation between them, as we've done in FigureIn this book, we solve that issue by carrying the "edge fraction" unit through, labeling the units of the y-axis of k 3 𝝌(k) graphs, for instance, as Å -3 •edge fraction.The units of Fourier transforms are the same as the units of the k-weighted 𝝌(k), but with one more factor of k since the Fourier transform integral involves multiplying the k-weighted 𝝌(k) by dk.One more issue with units: angstroms are not part of the SI system. Some journals, and some referees, prefer the use of nanometers or picometers.

Estimate of Noise:
Using one of the methods from Section 4.2.2 of Chapter 4, provide an estimate of ε random , the measurement uncertainty due to noise.

DATA REDUCTION:
If the procedures followed are not too far out of the ordinary, only a few sentences need to be allocated to data reduction. Citing the software used for normalization and background subtraction, for instance, will provide most of the information as to how they were done.k-weighting and the range of data in k-space used for Fourier transforms should be specified.

Curve Fitting to Theoretical Standards:
There are no hard-and-fast rules as to how much detail should be provided on models used for fitting, as it will be strongly influenced by the nature of the article or presentation and the nature of the model. • The software package used for analysis (with citation, if available).• The software used to compute theoretical standards (with citation).• How S o 2 was determined or constrained (this is a specific suggestion of IXS Standards and Criteria Committee, 2000).• Whether the fit was performed in k-space, the Fourier transform, or the back-transform.• The fitting range.• An estimate of the number of independent points and how it was arrived at (e.g., by the Nyquist criterion).• The number of free parameters (if the fit is in some way iterative, all free parameters must be reported, not just those in the final iteration). The degree of detail concerning the free parameters depends on the nature of the article or presentation.• The number of degrees of freedom of the fit (computed as independent pointsfree parameters).

Linear Combination Analysis and Principal Component Analysis:
We suggest that when linear combination is employed in a paper, the following be reported:• The list of standards successfully used in fits or as target transforms. A list of standards or targets considered but not used in final fits is also helpful, but not required.• The fitting space on which the analysis was performed: normalized XANES, derivative of normalized XANES, k 2 𝝌(k), and so on.• The range over which the analysis was performed.• For linear combination analysis, whether energy shifts were allowed.

RESULTS:
As with the model, the detail provided in the results will depend on the nature of the paper or the presentation.

Graphs of Fits:
It is typical to show graphs of a few representative fits along with the corresponding data, whether modeling, linear combination, or target transform is being used.

Closeness of Fit:
An R-factor should be reported for all EXAFS fits to theoretical standards. Since there are several different definitions in use (Section 7.4.3 of Chapter 7), don't forget to indicate which you are using!  In a paper, make sure to indicate that this was the method used to compute ε sys .Linear combination analyses feature the combination of being easy to interpret, difficult to characterize statistically (the measurement uncertainty problem again), and dominated in terms of closeness of fit by the edge. Accordingly, the issue of reporting quantitative measures of closeness of fit for linear combination analyses has garnered less attention than for EXAFS modeling. We recommend reporting an R-factor for any fits that are not graphed. (Fits that are graphed provide the same information visually, so the R-factor is optional.) Alternatively, if there are multiple spectra for which linear combination fits were performed and all were acceptable, then the one with the highest R-factor can be graphed and the rest reported as having smaller residuals.Principal component analysis should report how much variance is accounted for by the included components. While we recommend reporting this as a fraction of the variance after the first component, it is more common to include the first component in the total. Make sure you're clear as to which you are reporting! SPOIL plays a role for target transforms analogous to R-factor for fits to theoretical standards. With that in mind, we recommend providing SPOIL values for all target transforms, including rejected ones. In addition, it is very helpful to show graphically the poorest fit for a target transform that was accepted, as that will allow readers to see where, if anywhere, the components failed to reproduce the target.

Uncertainties:
It is vitally important to report uncertainties with all fitted parameters.For curve fitting to theoretical standards, the method given in Section 11.3 of Chapter 11 works well, as it accounts for both random and systematic errors. Be sure to provide an indication of how the uncertainties were computed-a citation to software is usually sufficient. Do not just use an arbitrary rule of thumb, such as "EXAFS can determine coordination numbers to within such-and-such percent." What EXAFS can doReporting the measurement uncertainty due to systematic error provides the same information as the IXS suggestion of reporting 𝝌 2 , but with less potential for confusing those not familiar with the field.depends intimately on the particular parameter being fit for the particular data using a particular model.For linear combination analysis, the uncertainty is often dominated by systematic error, particularly associated with normalization. The uncertainties reported by software do not account for systematic error and are thus underestimated.One good method for estimating uncertainties in linear combination analysis, analogous to the established method used for curve fitting, is given in Section 7.4.6 of Chapter 7. Alternatively, a reasonably good estimate can be made by estimating systematic uncertainties, such as that due to the edge jump during normalization of the sample and standards, and incorporating the estimate into the reported uncertainties.

CONCLUSIONS:
Be honest with the strength of your conclusions. If two models can be distinguished by using the Hamilton test on fits, you can with confidence select one over the other. If not, you may still be able to make a conclusive argument by bringing together multiple lines of argument; for example, XANES fingerprinting and EXAFS modeling, or EXAFS modeling and nuclear magnetic resonance.If results are only suggestive and not conclusive, that should be made clear.Much of the time, the same paper or presentation will include both kinds of conclusions.  • For EXAFS analyses, we should always include 𝝌(k) graphs somewhere-either in the main text or in supplementary information.

REFERENCE:
• We should provide an estimate of the measurement uncertainty due to noise.When modeling EXAFS, we should also provide an estimate of the measurement uncertainty due to systematic error.• We should report R-factors for both modeling and linear combination fits.• All fitted parameters should be reported with uncertainties.

Case Studies:
The Real DealAt last! I get to see how XAFS is actually done! But have we covered enough? Will these papers be too advanced?Nope. In fact, no paper really uses every single trick and technique we've talked about.The cool thing about looking at case studies is that you get a feel for what's really important in a good study.And how to cope when things go wrong!Yes, dear colleagues, but one can also observe the commonalities; those represent the best practices that all good authors follow.

IntroduCtIon to the CaSe StudIeS:
In this chapter, we will examine a number of actual studies from the literature. These studies come from multiple disciplines and use the full range of techniques we have learned about in this book.In order to get the most out of this chapter, it's best if you peruse a copy of the article being discussed, although you'll still be able to follow the gist even if you do not. Many of these articles are freely available on the Web, while others are in commonly held journals.In each case, we discuss what can actually be gleaned from the papers, sometimes using a little detective work to deduce quantities that aren't explicitly stated in the article. We offer a few critiques as well-no paper is perfect, if for no other reason than that the field has advanced since the paper was written. But the five papers chosen here stand as exemplars in their fields, demonstrating the effective use of XAFS to solve scientific problems.

the Scientific Question:
Lead titanate is a ferroelectric material up to 763 K. At the time Sicron et al. wrote this paper, the phase change at 763 K was widely thought to be displacive, meaning that the distance between atoms changes sharply across the phase transition. FigureSome other ferroelectric materials, however, showed evidence of undergoing order-disorder transitions. In an order-disorder transition, the local structure does not change abruptly across the phase transition, but the longer range structure becomes less ordered. FigureSicron et al. chose to investigate the structure of lead titanate as a function of temperature through the transition, to see whether it was purely displacive or whether it also exhibited aspects of an order-disorder transition.When discussing these case studies, we sometimes simplify the scientific questions for the sake of brevity. You can refer to the full articles for a more careful treatment.16.2 Lead Titanate, a Ferroelectric 385

Why XaFS?:
EXAFS, as a probe of local order, is well suited to the changes accompanying a displacive transition. But "local" goes well beyond the nearest neighbor, and EXAFS may also be able to detect features of an orderdisorder transition. The same, however, can be said of tools such as x-ray diffraction (XRD), which could in principle reveal either kind of change (the difference between the bottom structures in Figures 16.1 and 16.2 would show up as a change in the Debye-Waller factor).

Above transition:
FigureThese authors used feffit (a predecessor to ifeffit) to analyze their data.

Chapter 16 - Case Studies:
But what if the truth is messier than one or the other? If the transition has aspects that are displacive and aspects that are order-disorder, then the picture might be more difficult to sort out. Interpreting either EXAFS or XRD would be fraught with pitfalls. The answer is to make an interpretation informed by both kinds of probes, along with other experimental and computational techniques.Both lead L 3 (13,035 eV) and titanium K (4966 eV) edges are readily available to many beamlines and are well separated.

the Structure:
Lead titanate adopts the perovskite structure. In the ideal form of this structure (FigureThe ideal perovskite structure commonly undergoes several kinds of distortion. On the basis of diffraction and previous studies of perovskites, Sicron et al. anticipated the following possibilities:• A tetragonal distortion, in which the unit cell is stretched along one axis. At 150 K, diffraction revealed that the long (c) axis was 4.17 Å, whereas the short (a = b) axis was 3.90 Å. (Below 183 K, there is a slight orthorhombic distortion as well, so that a ≠ b. The orthorhombic distortion was considered negligible by the authors.)• A displacement of the lead atoms along the c axis. This breaks the 12 near-neighbor oxygens into the following three groups: the four on the edges of the face the lead atom is displaced toward, the four on the edges of the opposite face, and the remaining four in the middle. This also breaks the eight nearby titanium atoms into the following two groups relative to the lead: the four on the corners of the face the lead atom is displaced toward and the four on the corners of the opposite face.• A displacement of the titanium atoms along the c axis. This breaks the six near-neighbor oxygens into three groups as follows: the one the titanium atom is displaced toward, the one it is displaced away from, and the four in the middle. The eight nearby lead atoms split into two groups.

experimental Considerations:
The measurement of the lead L 3 edge was fairly straightforward. The sample was finely ground, mixed with graphite, and pressed into a pellet.The dilution fraction and thickness of the pellet were chosen to make an edge jump of around one.The titanium measurement, however, was more challenging. Sicron et al. point out that the absorption length of lead titanate above the titanium

BoX 16.1 SaMPLe thICKneSS:
We can use the information the authors provide in their paper to estimate the total absorption of their sample. We begin by computing the absorption length of lead titanate just above the lead L 3 edge. At 13.1 keV, the mass absorption coefficient of PbTiO 3 is about 120 cm 2 /g. Using 7.5 g/cm 3 as the density of lead titanate (this density is readily available on the Web), we arrive at an absorption length of 11 μm, which matches the number provided by the authors. So far, so good, my friends! Just below the lead L 3 edge, at 13.0 keV, we get about 55 cm 2 /g. Thus, the edge jump is just a little over half of the total absorption, and an edge jump of one should correspond to a total absorption of more than two.But we have not yet accounted for dilution. The paper does not specify the thickness of the pellet used for transmission, but it does say the pellet "could be easily handled." Let us suppose, then, that it were 1 mm thick. At 13 keV, the absorption length of graphite is about 4 mm, so the graphite would add about another 0.2 absorption lengths to their sample.Taken together, we can estimate that the total absorption of their sample above the lead L 3 edge was most likely between 2 and 2.5.That's a little higher than the ideal we recommended in Section 3.3.4 of Chapter 3, but well within the range of the recommendation of experts given in TableYes, and the authors of the paper take pains to note that they detuned 75% to be especially vigilant about suppressing harmonics. Note the sign of skilled experimenters: a slightly thick sample is paired with aggressive detuning! 388 Chapter 16 - Case Studies edge is only 2 μm. It would be difficult to grind the powder down to be small compared to that length, so they decided to measure in fluorescence.However, fluorescence raises its own problems. We can easily calculate, using the methods of Section 3.3.1 of Chapter 3, that the mass absorption coefficient of the sample just above the titanium edge would be 610 cm 2 /g, of which 110 cm 2 /g, or almost 20%, is attributable to the titanium. That is enough to cause considerable self-absorption. Nevertheless,

BoX 16.2 What eLSe CouLd theY haVe done?:
Why didn't they just dilute the fluorescence sample?It wouldn't do any good; they switched to fluorescence because they couldn't grind the particles to well below 2 μm, as they would have needed to in order to get undistorted transmission data! If there were particles several absorption lengths thick embedded in a graphite matrix, they still would have seen almost as much self-absorption (because the local concentration was just as high), but would have degraded signal to noise (because the average concentration was lower)! Besides, Simplicio, their main interest was in displacements, which are phase variables, not amplitude ones. Phase variables aren't affected by self-absorption. If they wanted to fit changing coordination numbers or perform a XANES linear combination fit, then their choice wouldn't be so good, but as it is, it's not a big deal.Alternatively, they could have made the choice to measure in transmission and taken the distortion that comes with uneven thickness. But that would have been somewhat more inconvenient in terms of sample preparation; a millimeter of graphite is more than four absorption lengths at energies near the titanium edge, so they couldn't have used their pellet method.They could have used powder on tape, although even tape absorbs noticeably at those energies. They would also have had to choose between making the edge jump fairly small or the sample really thick, because although the titanium absorbs enough to cause self-absorption, it's still a small fraction of the absorption compared to the lead. This is a "tweener" sampleit's somewhat inconvenient to measure, no matter which strategy you use.They could have tried sedimentation to get the fine particle size they needed.Yeah, they could have, but they didn't really need to. They were transparent about how they did the measurement, described and attributed the distortions they observed, and got the answers to the scientific questions they were looking for. That's a win! 16.2 Lead Titanate, a Ferroelectric 389 the authors chose to take their self-absorption lumps and measured a 1 mm-thick undiluted pellet in fluorescence.One more experimental aspect is worth discussing. Because they were using a Si(111) crystal in the monochromator, there was no second harmonic. The third harmonic above the titanium edge (5.0 keV), however, would be around 16 keV. Because the L 1 edge of lead is at 15.9 keV and the L 2 edge at 15.2 keV, any intensity in the third harmonic would at least exacerbate the fluorescent background and might additionally contaminate the titanium scans with lead EXAFS. To mitigate these issues, the authors were especially vigilant about harmonics, detuning I o by 75%.

the Model:
The authors collected spectra on both the lead and the titanium edges at a variety of temperatures above and below the known tetragonal to cubic phase transition. Although a modern analysis might include a simultaneous refinement of all spectra, these features were not widely available in EXAFS modeling software in the early 1990s, in part because such fits would strain the capabilities of typical computers of that period. Therefore each edge and each temperature was refined separately. Although this prevented the implementation of some of the efficiencies described in Chapter 14, it yielded redundant information on the scientific questions they were trying to address, increasing their confidence in their results.In this case study, we will focus on their analysis of the lead L 3 edge.

Paths:
The model comprised 11 single-scattering paths as follows:• Three paths for the three groups of nearby oxygen scatterers (four atoms each).• Two paths for the titanium scatterers (four atoms each).• Two paths for the lead scatterers (one with two atoms and one with four).• Four paths for more distant oxygens: four oxygen atoms in the +c direction from the +c titanium atoms, eight oxygen atoms in neighboring unit cells parallel to the +c face, eight oxygen atoms in neighboring unit cells parallel to the -c face, and four oxygen atoms in the -c direction from the -c titanium atoms. The furthest of these oxygens was located at a distance of D = 5.38 Å in the model.In addition, the paper indicates they included 34 double-scattering and 14 triple-scattering paths.Above room temperature, the signal degraded (as expected, due to increased MSRDs), so they only fit the near-neighbor oxygen and titanium scatterers. To make sure this didn't introduce an artifact into their analysis, they also tried the shorter range fit on the low-temperature spectra and confirmed that it yielded consistent values for the fitted parameters. Presumably, based on the year the paper was written, Sicron et al. were using a Lytle-type detector to measure fluorescence.With an energy-discriminating detector, the contamination from the lead L edges would have been less of a concern.The only one of these path sets that is not clear from the geometry is the lead. It appears that the assumption was made that the lead atoms were displaced in concert along the c-axis, with adjacent stacks being displaced in the opposite sense.No discussion is provided in the paper for this choice.But the conclusions they end up drawing in the paper don't depend on this choice, as they end up being mostly based on the behavior of the leadoxygen, lead-titanium, and titanium-oxygen scattering.390 Chapter 16 - Case Studies 16.2.6.2 Free parameters Free parameters included the following:• S o 2 and E o .• The displacement of the lead atoms along the c axis and the displacement of the titanium atoms along the c axis.• MSRDs. One for each of the following direct scattering groups:the four nearest oxygen atoms, the eight other near oxygen atoms, the four nearest titanium atoms, the other four near titanium atoms, the lead atoms, and the more distant oxygen atoms.• A third cumulant for the four nearest oxygen atoms.16.2.6.3 Constraints Perhaps the most interesting constraints are geometric. Rather than using absorber-scatterer distances as free parameters, Sicron et al. used displacement of individual atoms relative to their lattice sites. From there, half path lengths can be readily calculated. Other half path lengths, including those for multiple-scattering paths, could be expressed in similar ways. This form of geometric constraint dramatically reduces the number of free parameters needed for the fit.Other constraints included the following:• For the lead L 3 edge, a and c were constrained to have the values they adopt, according to XRD, at 150 K. The authors justify this by referring to the results from their titanium EXAFS analysis, which shows that these parameters are nearly constant across the temperature range measured.• Above room temperature, the MSRD for the two groups of near oxygen atoms that are not the nearest (i.e., paths 2 and 3 when the single-scattering paths are sorted from short to long) was constrained to be 28% larger than for the nearest neighbors. This ratio was taken from the results found at lower temperatures. This constraint was imposed because, as the temperature was raised, the contribution from these oxygen paths decreased.• The MSRDs for multiple-scattering paths were constrained in terms of the MSRDs for direct scattering paths. Although the exact method of constraint is not provided in the paper, Section 14.5.4 of Chapter 14 tells us that the precise method of constraint is not likely to be crucial. The results for the lattice parameters from the titanium edge appear at first glance to conflict with the diffraction results. The titanium edge results show a and c differ from each other by about 0.2 Å even well above the point where diffraction shows that they are equal. The authors speculate that this means that, above the phase transition, "the crystal consists of correlated nanoregions of distorted cells with different distortion orientations." This would yield the cubic structure on average, as indicated by diffraction, and the tetragonal structure locally, as indicated by EXAFS.16.2 Lead Titanate, a Ferroelectric 391• S o 2 and E o were constrained to be the same for all lead L 3 edge fits, with values chosen "so as to optimize the fits at all temperatures." In a modern study, this probably would have been done by refining spectra from multiple temperatures simultaneously. Although the process used by the authors was doubtless somewhat more laborious than that, the effect is similar.16.2.6.4 Degrees of freedom Because MSRDs increase with temperature, the range of usable χ(k) tends to shrink with temperature. In addition, the shells further out in the Fourier transform were not fit at higher temperatures. For these reasons, the number of independent points according to the Nyquist criterion (Section 11.1.1 of Chapter 11) ranged from 24 at their lowest temperature measurement to only 10 at their highest temperature. Their constraint scheme reflected this loss of information, allowing for nine free parameters at the lowest temperature and only six at the highest temperatures. (In addition, temperature-independent values of S o 2 and E o were extracted from the entire data set.)

drawing Conclusions:
EXAFS is best at making choices between alternatives. There are enough arbitrary choices made during data reduction and modeling that it is difficult to know how much to conclude from a single fit.In this paper, Sicron et al. presented not only their primary fits, but also some important alternatives. Because the diffraction data indicated that the average structure above the transition was cubic, they tried to fit the lead L 3 data with a model that was locally cubic (i.e., one with a structure like FigureEven with the two questionable parameters allowed to vary, the leastsquares residual was five times that for the primary fit.Let's apply the Hamilton test to these results. Going from the cubic to the tetragonal high-temperature fit adds four free parameters: two displacements, a third cumulant for the nearest oxygens, and an additional MSRD for the titanium atoms. Thus, b = 4/2 = 2. The number of degrees of freedom of the tetragonal fit is roughly 10 -6 = 4 (S o 2 and E o are almost entirely determined by the spectra from lower temperatures), so a = 4/2 = 2. r, according to the 392 Chapter 16 - Case Studies paper, is 1/5 = 0.2. Computing, I 0.20 (2,2) = 0.10, which is not quite enough to say the tetragonal fit is better at the 95% confidence level.That's not the only evidence they had, however. They also had the fits to the titanium edge. Although we have not given the details of those models here (consult the paper if you're interested!), the high-temperature tetragonal fit used three free parameters and seven independent points, giving four degrees of freedom. The cubic fit, utilizing one fewer free parameters, yielded a least-squares residual that was twice as high. Thus, for a Hamilton test on the titanium data, b = 1/2 = 0.5, a = 4/2 = 2, and r = 1/2 = 0.5. I 0.50 (2,0.5) = 0.12. Once again, on its own, this result is not conclusive.The results were, however, determined essentially independently. If for the lead L 3 edge the tetragonal model has a 10% probability of being as much better as the cubic model as was seen by chance, and for the titanium K edge the figure is 12%, the probability of them both being that much better by chance is approximately 0.12 × 0.10 = 1%. The cubic fit can, therefore, be rejected in favor of the tetragonal fit.And with that, the core of the scientific question is answered. If the local structure is still tetragonal above the transition while the average structure becomes cubic, then there must be some order-disorder aspect to the phase change.

Presentation:
The paper includes figures giving the k-weighted χ(k) for all data. Fits for both edges and several representative temperatures are presented with the real part, imaginary part, and magnitude of the Fourier transforms. There is also a plot of the magnitudes of the Fourier transforms of each of the path groups from the fitted model at the lowest temperature. Fitted parameters are presented, with error bars, as a function of temperature.If the two edges had been refined simultaneously, then the conclusion would presumably have been the same. The higher degrees of freedom of the combined data set would have likely yielded a result that passed the Hamilton test.

BoX 16.3 What We Learned and LIKed:
Let us follow Simplicio's lead, my friends, and each say something we learned from this paper, or perhaps simply something we liked. We should like that they provided enough information so that we could reconstruct what they did, even when they didn't explicitly spell it all out-notice how we deduced both the sample thickness and the geometric constraints! I like that the free parameters are based on what is happening physically in the material. The two displacement parameters are very easy to interpret and well worth the trouble of having to work out the geometry for the half path lengths. This paper was published by

the Scientific Question:
Bacteria reduce nitrogen with the help of an enzyme called ironmolybdenum cofactor, or FeMoco for short. The structure of this enzyme is knownI learned from the paper and from our discussion that there's more than one way to do things right. They could have refined the data simultaneously (at least if the paper had been written today), but they did it separately, and that worked. One step in this process transfers a precursor compound from a protein called NifB to a protein complex called NifEN. Although the precursor was known to contain iron and sulfur, the structure was not known. The goal of Corbett et al., then, was to determine, or at least narrow down, the structure of the precursor.

Why XaFS?:
The precursor has not been isolated, but is only found bound to one of the proteins. Determining its structure by diffraction would therefore require crystallizing an entire protein complex and then solving its structure, a very challenging task. Because the precursor is built around iron atoms, XAFS can provide structural information localized to the precursor.

a Challenge and a Solution:
One of the biggest challenges Corbett et al. faced was that NifEN contains "permanent" iron atoms even before the precursor binds to it. XAFS measured at the iron K edge, then, would be a linear combination of the signals from the precursor iron and the permanent iron.Finding the structure of the precursor is challenging enough; having to find the structure of other regions of the protein at the same time would be formidable.Because proteins are big molecules, the local structure around the permanent iron atoms is probably unchanged by the addition of the precursor. Corbett et al., therefore, applied the concepts of linear combination analysis in reverse: knowing how much of each constituent they had, the spectrum of the "mixture" (protein + precursor), and the spectrum of one of the constituents (ΔnifB NifEN), they could subtract the known fraction of ΔnifB NifEN from the spectrum of the precursor + protein, rescale the residual by the fraction of precursor, and thus arrive at the χ(k) for just the precursor.

Possible Structures:
The structure of the iron cluster in FeMoco is shown in This is a daring procedure-I love it! But does it work? There are uncertainties in the number of atoms of iron per molecule-in fact Corbett et al. assumed the ratio of permanent iron to total iron was 1:2, which is not quite the best fit values of 8.5:16.1. Throw in the usual uncertainties associated with normalization, and there's the potential for the derived spectrum to be a bit off. To address this concern, Corbett et al. first tested the method on a similar system where all the structures were known. Showing that the method works on a similar system is a crucial part of their paper.16.3 An Iron-Molybdenum Cofactor Precursor 395 in Figure

experimental Considerations:
The concentration of iron in the protein as prepared is given in the paper as 5 mM for NifEN and 3 mM for ΔnifB NifEN. Using the atomic mass of iron, we can calculate that 5 mM corresponds to 0.28 mg/mL. Iron has a mass absorption coefficient of roughly 200 cm 2 /g above its K edge, while carbon (a proxy for the other elements in the protein and accompanying materials) is roughly 6 cm 2 /g. Assuming that the rest of the components have a density of about 1,000 mg/mL, we can compute the fraction of absorption due to iron as on the order of (200 × 0.28)/ (200 × 0.28 + 6 × 999.7) = 0.9%. Thus, self-absorption is not a significant concern.Proteins are fragile samples, vulnerable to beam damage and other sources of degradation. To help maintain the samples, they were added to a solution comprising a Tris•HCl buffer, imidazole, dithionite, and glycerol. This solution was flash frozen and measured in fluorescence at 10 K. The low temperature served the following two purposes: it minimized the 396 Chapter 16 - Case Studies risk of beam damage and allowed data to be collected to high k by minimizing the thermal contribution to MSRDs.With the additional dilution due to the protective solution, the concentration of iron was fairly low, and so a 30-element germanium detector was used for the fluorescence measurement. The biggest source of fluorescent background in this case was likely scatter, because the majority of the atoms in the sample were of low atomic weight. Corbett et al. therefore used a manganese filter and Soller slits (Section 5.6.1 of Chapter 5).The signal to noise ratio in their resulting data was impressive, allowing them to use the range of χ(k) from 2 to 16 Å -1 for analysis.

Fingerprinting:
Corbett et al. used fingerprinting on the pre-edge features to provide a preliminary understanding of the structure of the precursor. They used pseudo-Voigt functions (Section 6.3.2 of Chapter 6) to model the features, allowing the energies, widths, and amplitudes to vary. The rise of the edge itself, which acted as a background for the pre-edge features, was also modeled by a pseudo-Voigt function.As it turned out, the differences between the precursor and other tested protein-bound iron-sulfur clusters were clear; although almost all other clusters showed a pre-edge peak or peaks centered around 7112.2 eV, the precursor had an extra pre-edge peak centered at 7113.6 eV. The only other substance tested that showed this extra peak was FeMoco itself.Because the other tested materials featured iron tetrahedrally coordinated to sulfur, the second peak suggests that the precursor, like the final FeMoco product, had a substantial contribution from the flattened trigonal pyramidal geometry.

the Models for eXaFS:
Given the evidence from fingerprinting, Corbett et al. decided to try fitting the EXAFS data using models based on the structure of FeMoco itself.One candidate would be the structure shown in FigureAnother possibility would be that the molybdenum atom was substituted by an iron atom (the "8Fe" model) or were simply missing/replaced by organic structures (the "7Fe" model). Finally, both the molybdenum and 16.3 An Iron-Molybdenum Cofactor Precursor 397 the iron atom furthest from it could be missing (we'll call that the "6Fe" model, in agreement with Corbett et al.'s naming scheme).At this point, it is helpful to examine the FeMoco structure more closely, to examine characteristics of the radial distribution function around iron in preparation for choosing paths.Each iron atom has three or four sulfur near neighbors. Most of them also have a nitrogen neighbor, but Corbett et al. decided to disregard the nitrogen on the basis that the scattering would be weak compared to the sulfur and iron.Beyond that, the alignment of the two central triangles of iron atoms creates an unusual distribution of iron-iron distances. There are "short" iron-iron distances along the sides of the triangles, between an atom in one triangle and the corresponding atom in the other triangle, and between the apical iron atoms, if any, and the iron atoms in the triangle near it. There are also "long" iron-iron distances, from one atom of a triangle to one of the noncorresponding atoms in the other triangle. In addition, the FeMoco structure is not entirely symmetric, with the result that the short iron-iron distances range from 2.58 to 2.67 Å, whereas the long distances range from 3.69 to 3.72 Å.In the 6Fe model, each iron atom has only three neighbors at the short iron-iron distance, whereas in the 7Fe model or FeMoco itself that rises to four neighbors for the three atoms in the triangle nearest the apical iron atom. The authors essentially use a fingerprinting technique to reject the 6Fe model, saying "the intensity of the short-range iron-iron scattering precludes a six-iron cluster"That leaves the 7Fe and 8Fe models to try. They are very similar, so we'll detail them together.

Paths:
The model included single-scattering paths for the nearneighbor sulfurs and the short and long irons. The short and long iron atoms were allowed two paths each, to reflect the kind of modest spread of distances seen in the FeMoco crystal structure. There were thus a total of five single-scattering paths. An examination of the structures suggests that the contributions from multiple-scattering paths with half path lengths shorter than 4 Å is modest; there are no focused paths, and with the exception of those involving the apical sulfur, there are no short triangle paths. Corbett et al. therefore chose to neglect multiple scattering in their fits.  • This gives a total of 13 free parameters.

Constraints:
The most interesting constraint is on the coordination numbers. For each model, the coordination number was fixed at the average specified by the model structure.For example, consider the coordination of the short iron-iron path in the Fe7 model. The apical iron has three iron neighbors in that range; the iron atoms in the triangle below it each have four iron neighbors in that range; and the iron atoms in the lower triangle again each have three iron neighbors in that range. The average coordination for the short iron path in the Fe7 is therefore (1•3 + 3•4 + 3•3)/7 = 3•4. In contrast, the Fe8 model would have (2•3 + 6•4)/8 = 3•8.As discussed under "paths," however, the short iron path was split into two, and the fraction of the scattering assigned to each was a free parameter. If we call that parameter x, then the degeneracy for the shorter of the short iron paths under the Fe7 model is 3.4x and the degeneracy for the longer of the short iron paths under that model is 3.4(1-x).The other constraint of note is S o 2 , which was constrained to be 1.0.

BoX 16.4 the S O 2 ConStraInt:
1.0 is a little high for S o 2 -wouldn't it have been better to constrain it to something like 0.9? Better yet, the authors could have extracted S o 2 from their test fit to FeMoco-the coordination numbers for that structure are known.If they were fitting coordination numbers, that would mean the coordination numbers they found would come out too low by the same ratio as the overestimate in S o 2 . But within each model, they weren't fitting coordination numbers, so the effect would more likely show up as fitted values for MSRDs that were a little too large. On the other hand, the difference between the 7Fe and the 8Fe models does depend on coordination numbers! So their fits will have a modest bias toward the less-coordinated model; for example, the 7Fe.As we'll see, the systematic error introduced by this choice actually works opposite the direction of their final conclusion. So they made a conservative choice. When in doubt, make the conservative choice; it makes your conclusion stronger in the end! 16.3 An Iron-Molybdenum Cofactor Precursor 399Of course, when fitting in χ(k), there is no explicit R range to substitute into the Nyquist criterion (Section 11.1.1 of Chapter 11). But it's reasonably easy to get an estimate. Inspection of Figure

drawing Conclusions:
Their fit to the 7Fe and 8Fe models yields R values of 0.049 and 0.044, respectively. Those values, according to our informal guidelines in TableLet's apply the Hamilton test, and see if the 8Fe fit is significantly better than the 7Fe one.It's not entirely clear how to define b in this case, because of the way paths are averaged. So let's start by assigning b the minimum value we can, 0.5, and see if the Hamilton test shows a significant difference in that case. a is half the number of degrees of freedom, or around 5. And r is 0.044/0.049 = 0.90. I 0.90 (5,0.5) = 0.32, so the 8Fe is not statistically better at the 95% confidence level.Although they do not explicitly perform the Hamilton test, the authors make the point clearly:"Although the 7Fe model cannot be excluded on this basis, the 8Fe model is considered to be the more likely structure because it is aNotice that if we had used the statistic the authors called F, we would have had to square it to apply the Hamilton test! 400 Chapter 16 - Case Studies better match both to the EXAFS fit results and previous biochemical studies."The previous studies provide additional evidence that, along with the EXAFS results, gives the edge to the 8Fe model.In the supporting information, the authors also report results of several fits using several other models for the paths beyond the short iron, such as using another shell of sulfur atoms rather than the long iron paths. Each of these fits had a larger R factor than the 8Fe model, although again the differences did not reach the level of statistical significance. Nevertheless, this exploration of the fitting space is helpful because it shows that the authors considered multiple possibilities, and the 8Fe model still appears preferable.What then, can be concluded?• It is very likely that the iron-sulfur cluster in the precursor is structurally similar to the cluster in FeComo, in particular favoring flattened trigonal pyramidal coordination over solely tetrahedral.• It is quite unlikely that the precursor includes molybdenum.

Presentation:
The paper includes figures giving k 3 -weighted χ(k) for all data. Normalized XANES is included in the supporting information, with the detail of the pre-edge included in the published article. Also in the published article, second derivative plots of the pre-edge features are shown to emphasize the multiple peaks. Key fits are presented both in k 3 -weighted χ(k) and the magnitude of the Fourier transform.Remember-the choice of S o 2 as 1.0 probably biased the results slightly toward the 7Fe model. The fact that the 8Fe model still fits better is a point in its favor.One interesting wrinkle in the way the data were presented is as follows: E o was given relative to 7130 eV, an energy somewhat higher than the white line. This causes the fitted E o shifts to be tabulated as values around -10 eV-usually a red flag! But in this case, because the reference energy was chosen so high, they are reasonable.

BoX 16.5 What We Learned and LIKed:
This is great! They were fearless, trying new things, but then they tested the techniques on a known structure to show that it worked! The authors were very careful to provide thorough information, so we could understand precisely what they had done. This is particularly important when developing new techniques.

the Scientific Question:
Mixed metal ferrites such as manganese zinc ferrite (MZFO) have important magnetic and electrical properties, which depend crucially on how the cations are distributed between tetrahedral and octahedral sites. Nanoparticulate ferrites were thought to have different characteristics than their bulk counterparts, some of which were thought to be associated with different cation distribution.For this study, MZFO nanoparticles were prepared using two different protocols and compared to three differently processed bulk MZFO samples. The goal was to compare the site occupancies of these samples.

BoX 16.5 What We Learned and LIKed (Continued ):
The reverse linear combination approach is fascinating. I expect it will prove helpful in a number of biological systems! Was this top-down or bottom-up, my friends? It is an interesting hybrid. The paths look like the typical result of a bottom-up approach, but the constraints on coordination number were more reminiscent of a top-down technique.What spectacular data! The combination of low-temperature and their organic cocktail gave data almost as good as what one gets from a foil! I learned how to report conclusions; some of which are more certain than others.The constraints for coordination number were firmly based in the physical structures. I liked the technique of averaging coordination number of the atoms in the structure.

Why XaFS?:
Because the atomic numbers of manganese and iron differ by only one, they appear nearly indistinguishable to XRD. In addition, the diffraction peaks for the nanoparticulate samples are broadened. The combination means that XRD cannot tell us the site occupancy of these samples.Mössbauer spectroscopy could provide the distribution of the iron cations, but manganese does not have a Mössbauer-active isotope.XAFS, on the other hand, is element specific. By collecting and simultaneously refining data on the manganese, iron, and zinc K edges, the site occupancy of each can be determined.

the Structure:
MZFO adopts the spinel structure, which was discussed in Section 6.2 of Chapter 6. Briefly, cations can occupy sites that are tetrahedrally coordinated to oxygen, or sites that are octahedrally coordinated. In a typical spinel, twice as many cations sit in octahedral sites as tetrahedral ones.More precisely, most spinels adopt the space group Fd3 m. The fractional coordinates of the tetrahedral sites are (0, 0, 0), the octahedral sites are (5/8, 5/8, 5/8), and the oxygen sites are (u, u, u), where u is called the oxygen parameter and is usually around 0.38

experimental Considerations:
The most remarkable experimental aspect of this study is that two of the samples and one of the standards were measured twice, using two different beamlines. The characteristics of the beamlines were quite different; for instance, harmonic rejection on one was provided by a mirror, whereas on the other, 25% detuning was used. This comparison provided an interesting check on the measurement and subsequent analysis.In addition, for each protocol, two samples were prepared, allowing investigation into the reliability of the synthesis.

the Model:
For this analysis, six theoretical standards based on the spinel structure had to be built: a tetrahedral and an octahedral absorber for each of the three cation edges. All edges were weighted equally in the simultaneous refinement; that is, the measurement error ε was assumed to be the same for the purposes of finding a fit.The site occupancy of the scattering atoms was addressed by using the mixed model technique from Section 13.7.3 of Chapter 13.Fd3m is a space group that can be described by more than one setting. Thus, the oxygen parameter is sometimes defined, so that it is 0.125 less than in our definition.These authors used feffit (a predecessor to ifeffit) to analyze their data.The paper indicates that the samples were from one to four absorption lengths thick at all three edges. That's a bit thick, and, what's more, the detuning was only 25%. Fortunately, the comparison to a beamline with a harmonic rejection mirror helps us to see if harmonics are causing a problem. It is sometimes said that EXAFS cannot distinguish between elements with atomic numbers within five of each other. But that statement needs to be understood in the proper context. It is true that it is difficult to distinguish between elements with atomic numbers that close to each other in a fit in which bond length and coordination number are free to vary. But that is in part because the bond length parameter can compensate for small differences in the phase shift due to the scattering element, and the coordination number parameter can compensate for small differences in the scattering amplitude. Thus, it might be hard to discriminate between a manganese and a zinc scatterer in such a fit. But using the wrong scatterer introduces systematic errors into both bond length and coordination number, even if the atomic number of that scatterer is within five of that of the actual element. Thus, whenever possible, one should use the actual scattering element in theoretical standards.Whenever possible and convenient, Robert. Sometimes pretending all your manganese, iron, and zinc scatterers are, say, cobalt (an element more or less in the middle of the range they span) is good enough for a rough fit. For careful work, you're right. But there's something to be said for quick and dirty when called for.I will leave that kind of thing to you, Dysnomia. I am desirous of being neither quick nor dirty.Either way, it is important that each absorbing species has its own theoretical standard! Even if all scatterers were to be treated as cobalt, we should use the correct absorbing atom for each edge.With so many paths, it's understandable that Calvin et al. did not provide an enumeration. But in retrospect, it might have been helpful had they provided the number of single-and multiple-scattering paths in each theoretical standard, so that readers could know what level of detail was present in the models.404 Chapter 16 - Case Studies would need no modification.) That suggests a total of roughly a hundred single-scattering paths. Add in multiple-scattering paths and the total number of paths was likely in the hundreds.16.4.6.2 Free parameters The free parameters used were as follows:• S o 2 and E o for each edge (six parameters) • A lattice parameter • The oxygen parameter • The fraction by which the distance to the nearest oxygen differs from the average for the manganese cations, and a similar parameter for the zinc cations (two parameters)• MSRDs: nearest-neighbor manganese absorber to oxygen; nearestneighbor iron absorber to oxygen; nearest-neighbor zinc absorber to oxygen; and all more distant paths (four parameters)• Tetrahedral site occupancy for zinc and manganese ions (two parameters)This yields a total of 16 parameters.

Constraints:
The list of free parameters implicitly includes rather dramatic constraints. For example, both manganese and iron cations in spinels exhibit valence disorder; that is, they are likely to exist as a mixture of +2 and +3 ions in a spinel (zinc should be only +2). This means there could be 10 different nearest-neighbor oxygen distances: tetrahedrally coordinated Mn 2+ , octahedrally coordinated Mn 2+ , tetrahedrally coordinated Mn 3+ , octahedrally coordinated Mn 3 , tetrahedrally coordinated Fe 2+ , octahedrally coordinated Fe 2+ , tetrahedrally coordinated Fe 3+ , octahedrally coordinated Fe 3+ , tetrahedrally coordinated Zn 2+ , and octahedrally coordinated Zn 2+ . If each were assigned their own MSRD and bond length, that would mean 20 free parameters just to handle this nearest-neighbor distance! Although measuring three edges helps, it is unlikely that so much information could be extracted from the first peak in the magnitude of the Fourier transform! Calvin et al. took several steps to constrain this proliferation of free parameters, while still allowing some of the variability they represent to be fit.First of all, valence disorder was relegated to MSRD only. In other words, the difference between the Mn 2+ -O distance and the Mn 3+ -O distance was allowed to manifest as a broadening of the nearest-neighbor peak, rather than as distinct distances.The differences in the nearest-neighbor oxygen distances for the three elements were treated as if it were a fractional difference that was independent of tetrahedral or octahedral coordination. For example, the manganese cations might be found to be 1% further from the oxygen atoms, on average, than the zinc cations were, but this 1% would be applied equally to tetrahedral and octahedral sites.With hundreds of paths, three edges, but only 16 free parameters, this is about as top-down as it gets, friends! If this paper was written now, some sort of histogram method (Section 13.5 of Chapter 13) might have been applied to better model the distribution of bond distances.

Manganese Zinc Ferrite, an Example of Fitting Site Occupancy 405:
Finally, an examination of the list of free parameters shows that only manganese and zinc were assigned parameters that allowed their nearestneighbor oxygen distance to vary from the average. Why not iron as well?The answer is that, despite jokes to the contrary, not everything can be above average. If we know how much the zinc and manganese deviate from the average, and if we know the relative amount of zinc, manganese, and iron, then we can calculate the deviation of the iron from the average. As it turns out, Calvin et al. did know the relative amount of the three cations; there are a number of methods for finding the stoichiometry of a compoundAnother important constraint relates to the site occupancy. Although the fraction of manganese ions and zinc ions present in tetrahedral sites are both free parameters, the fraction of iron atoms is then determined by the stoichiometry of the compound and the sites that are left available.An example of such calculation is worth doing in detail. Suppose the fraction of manganese, zinc, and iron cations in tetrahedral sites is T M , T Z , and T F, respectively. Similarly, we can designate the fractions in octahedral sites as O M , O Z , and O F . Finally, let's designate the mole fraction of each element in the sample as X M , X Z , and X F . We know that each cation must be in one site or the other, thus,We also know, because we have a spinel, that the total occupancy in the octahedral sites must be twice that of the tetrahedral sites (neglecting vacancies!):Substituting EquationFinally, multiple-scattering paths were constrained using a heuristic method that added no new free parameters (Section 14.5 of Chapter 14).406  Degrees of freedom Not only were three edges used, but somewhat different ranges of k-space were used on different samples, consistent with the trade-offs discussed in Chapter 11. All edges were fit from 1.0 to 5.5 Å in the Fourier transform.For the manganese edge, the k-range was typically around 3-8 Å -1 , yielding about 14 points by the Nyquist criterion. For the iron and zinc edges, the k-ranges were generally somewhat larger.Because separate edges represent independent data sets for the purpose of computing degrees of freedom, there were typically 50 or more independent points per analysis, considerably more than the 16 free parameters.

drawing Conclusions:
The purpose of this paper was somewhat different than the two examined in the previous sections of this chapter. Rather than attempting to choose between models, this study sought quantitative analysis. Thus, the primary answers are provided by the values of the fitted parameters, rather than the statistical quality of the fits. The repetition of the analysis on two beamlines helped confirm the reliability of the determinations, and their agreement with magnetic measurements spoke to their validity.In order to reach conclusions about the values of fitted parameters, it is important to examine the uncertainties in those parameters. One of the samples, for example, yielded 42 ± 11% of the manganese in tetrahedral sites when measured on one beamline and 50 ± 11% on the other. Those values are consistent, as their error bars overlap. The corresponding values for the standard measured on both beamlines were 15 ± 5% and 18 ± 4% on the other also consistent. But the values for the sample and the standard lie far outside of each others' uncertainties, indicating that the cation distributions are significantly different.Another interesting result was the MSRDs of the nearest-neighbor oxygens. In every case (all standards, all samples, and all beamlines), the MSRD for the manganese-oxygen near-neighbor path was larger than zinc-oxygen, usually significantly so. And in nearly every case, the ironoxygen MSRD fell in between. The paper speculates that this may be a marker for valence disorder, as zinc ions should always be of a single valence, whereas manganese ions would be expected to be present as a mixture of +2 and +3.

Presentation:
The paper includes graphs of k-weighted χ(k) for all data and the real part of the Fourier transform for all data and all fits. An example of unnormalized absorption is presented. To facilitate comparison with fingerprinting studies in the literature, the magnitudes of the Fourier transforms of the manganese edge of two of the samples were also shown. One of the greatest risks was posed by sulfur compounds in the wood, as they could potentially convert to sulfuric acid now that the ship had been freed from the seafloor.

BoX 16.7 What We Learned and LIKed:
I like seeing some of these older papers. They're still convincing, but we can think of ways to do the same study even better now!The list of simplifying assumptions that went into their model was much appreciated. It was thorough and direct.The site occupancy constraints were very elegant and helped couple together the data from the three edges.Me? I liked the near-neighbor parameters and constraints. The authors found a way to reflect some of the distortions caused by the different cations without using so many free parameters that the fit became unstable.I like that the MSRDs were given a physical interpretation.If you look at the paper, you'll see that there are a few parameters in one sample that the authors don't report values for-they just say they were "not stable with respect to changes in the data range analyzed." That's much better than reporting a value that isn't justified! Knowing that they were willing to tell us when they couldn't get a reliable value for a free parameter increases my confidence that the parameters they do report are good! 408 Chapter 16 - Case StudiesThis paper by

the Scientific Question:
Although several questions were addressed by the paper, the main focus was on the identity and amounts of sulfur compounds as a function of depth in wood cores taken from the Mary Rose.

Why XaFS?:
This is a problem especially well suited for XAFS. Slices from the heterogenous wood cores can be measured in situ through microprobe, and because XAFS is element-specific, the sulfur-containing species present can be identified even though they are a small percentage of the total sample.

experimental Considerations:
The sulfur concentration was around 1% by mass in most of the measured wood, except for 3.5% in a sample taken from a wooden gunshield. Iron was also found, although the amount showed considerable variation, with typical values around 0.2% by mass (2.8% in the gunshield). Approximating the rest of the material as carbon, and using a mass absorption coefficient above the sulfur K edge of 2000 cm 2 /g for sulfur, 900 cm 2 /g for iron, and 160 cm 2 /g for carbon, we can calculate that the fraction of absorption due to sulfur from the gunshield is roughly (2000 × 0.035)/(2000 × 0.035 + 900 × 0.028 + 160 × 0.937) = 29%! Even the more typical samples would have about 10% of the absorption due to sulfur. One option, then, would be powdering the samples and making transmission measurements; one absorption length of the wood corresponds to roughly 50 μm above the sulfur K edge.Another option would be to measure fluorescence in the thin limit (Section 3.4 of Chapter 3); this is the choice Sandström et al. made. They therefore ground segments from the wood cores into very fine powder and brushed them on sulfur-free tape. At the sulfur K edge, the absorption length of air is only about 3 cm, so the measurement region was filled with helium gas, which has an absorption length order of magnitude larger.A sodium thiosulfate energy reference was measured before and after each sample, assuring the careful alignment necessary for principal component and linear combination analyses.Sandström et al. also measured sections of the wood using a microprobe technique, to see where in the wood's cellular structure the sulfur compounds were concentrated. For this purpose, they cut slices a few microns thick from the wood (once again, ensuring the thin limit) and measured in fluorescence using an energy-discriminating detector.

Principal Component analysis:
The paper mentions a preliminary principal component analysis (PCA) of the data from the wood cores. (As described in Section 8.9 of Chapter 8, PCA of this kind sometimes doesn't even make it into the final paper!)The PCA did not include the gunshield, which had a significantly different elemental composition than the wood cores. In this case, Sandström et al. find "at least" six significant components.

Linear Combination analysis:
Rather than using solid compounds as standards, Sandström et al. preferred to use dilute solutions, finding that they serve as a better analogue to the materials as found in wood. The standards they used were meant to be representative of various classes of sulfur compounds, rather than exact matches, and are shown in TableOne advantage of measuring in the thin limit of fluorescence is that homogeneity is not much of an issue. Unlike in transmission, the sample can be filled with pinholes, as long as it's thin. Not counting the two forms of cystine separately (two forms were included partially as a test of which was more appropriate), there are seven standards. Linear combination analysis found each in at least one core sample, consistent with the large number of constituents suggested by PCA.For the gunshield, melanterite (FeSO 4 • 7H 2 O) was used instead of sodium sulfate as a sulfate standard, because it was presumably closer to the form present in the iron-rich gunshield.

drawing Conclusions:
Like the previous case study, this is not a study attempting to choose between two different possibilities. Preservation of cultural artifacts depends on the details, and this study provides them in the form of the distribution of sulfur species as a function of depth of core and choice of wood. Information on the distribution of the sulfur compounds on the microscale was provided by microprobe XANES, and the results of the linear combination analysis were corroborated by x-ray photoelectron spectroscopy.

Presentation:
Normalized XANES spectra for all data are provided, some with the main paper and some in the supporting information. A few representative linear combination fits are shown, with the rest being tabulated. Microprobe maps are shown at different energies to reveal distribution of sulfur-bearing species; once again, some maps are included in the main paper and some in the supporting information.

BoX 16.8 What We Learned and LIKed:
So many kinds of data! I could read this paper over and over and over again, and keep learning more about the system! I appreciate how careful they were to maintain energy calibration (crucial for linear combination analysis!) and to explain how they did it.I appreciate that they showed a picture of the gunshield, friends. That sounds like a small thing, but it helps put an unfamiliar artifact in context and also gives a visual impression of its condition. Ressler,This paper is an important part of the literature not just because of the conclusions it drew about automobile exhaust, but because it was a clear and early exemplar of the use of XANES PCA to attack a difficult speciation question.

the Scientific Question:
In order to understand the health impacts of manganese particulates in automobile exhaust, it is crucial to know the speciation.The primary question addressed by this study is this speciation. Secondary questions were the effect on this speciation of different simulated driving patterns.

BoX 16.8 What We Learned and LIKed (Continued ):
The test of solid versus aqueous cystine as a standard for linear combination analysis was one of many interesting features! I liked seeing how PCA can be used as a preliminary probe. Sometimes I feel that XAFS papers can lose sight of the system being studied and get bogged down in mechanics. In this case, though, the authors always kept their eye on the ball: the role of sulfur in the wood recovered from the Mary Rose.412 Chapter 16 - Case Studies

Why XaFS?:
Ressler et al. address this question directly in the introduction to their paper. They describe an attempt to get XRD data, and wryly state the results: "Studies of four relatively heavily loaded samples (292-4244 μg) showed only background from the filter substrate material." X-ray photoelectron spectroscopy (XPS) did suggest that oxygen, phosphorus, and sulfur were associated with the manganese, but did not provide more definite speciation.The low concentration, small sample size, and possible poor crystallinity led to XAFS.

experimental Considerations:
The samples were, in essence, soot collected from automobile exhausts during testing. We can use the mass given for the "heavily loaded" diffraction samples to estimate the number of absorption lengths in the soot layer.Let's start with the upper end of the range at 4,000 μg. Presuming the sample to be the diameter of an exhaust pipe on the 1997 Ford Taurus sedans used in the study, the area would be about 20 cm 2 . Soot from auto mobile exhaust is mostly carbon-the manganese species, while the focus of the study, were likely to be present at low concentration.Above the manganese K edge, carbon has a mass absorption coefficient of around 8 cm 2 /g. The number of absorption lengths is thus approximately (8 cm 2 /g)(4,000 μg)/(20 cm 2 ) ≈ 0.002. That's a thin sample! Even if the sample were pure manganese, a similar computation only arrives at around 0.1 absorption lengths. Fluorescence can be used without fear of self-absorption.The samples were measured using a Lytle detector. An energy-discriminating detector was not called for in this case; because the sample was thin, the amount of scatter was probably small. And because the other elements in the sample were dominated by those with low atomic numbers, air would act to filter out much of the background.Because the samples were thin, it was straightforward to measure a reference foil simultaneously in transmission.Samples were obtained from two different cars (but identical models), at two different mileages, using four different simulated driving patterns. Twelve of the sixteen possible combinations of these variables were used.Twelve measurements were made in part not only to gain information about the dependence on the variables that had been changed, but also to provide enough related spectra to perform robust PCA. Because three constituents were expected from PCA, the identification of three by target transform indicates that the complete set has been found.A cluster plot of the second component versus the first revealed a natural division of the samples into two groups. Intriguingly, this division into groups does not correspond with any of the experimental parameters, except that the less-populated group included spectra from only one of the two cars. This suggests there is a "hidden variable" controlling the mix of constituents in the exhaust.

Linear Combination analysis:
With the constituents identified, linear combination analysis using the XANES becomes straightforward. This analysis reveals that all three constituents are present in all of the samples, but to different extents.The smaller of the two groups identified on the cluster plot were distinguished by having more than a quarter of their spectral weight provided by Mn 3 O 4 .

Fingerprinting:
Ressler et al. also used fingerprinting to estimate the average manganese valence of the samples. They chose to use an arctangent function to fit the edge and found that the standards established a linear relationship between edge energy and valence. By comparing with the edge energy of the standards, they were able to conclude that all of the standards exhibited a manganese valence of roughly 2.1 to 2.2. Because manganese in Mn 3 O 4 , MnSO 4 •H 2 O, and hureaulite have a valence of +3, +2, and +2, respectively, this can be compared to the fractions found from linear combination analysis. For example, according to linear combination analysis, 43% of the manganese in sample 12 was present as Mn 3 O 4 , 24% as MnSO 4 •H 2 O, and 33% as hureaulite. This implies an average manganese valence of 0.43•3 + 0.24•2 + 0.33•2 = 2.4. Although slightly higher than the value extracted from fingerprinting the position of the edge, this is reasonably good agreement for a comparison of this type.Ressler et al. use EquationWhen they performed linear combination analyses, Ressler et al. allowed for an energy shift of the references to correct for misalignment, but threw out any references that needed to be shifted by large amounts as unphysical.414 Chapter 16 - Case StudiesIn addition to XANES, qualitative fingerprinting was also applied to the magnitude of the k 3 -weighted Fourier transform of the EXAFS. The Fourier transform of the Mn 3 O 4 standard shows a prominent peak at about 3.0 Å, which is absent in the other two linear-combination standards. This peak is also seen in the group of samples with high Mn 3 O 4 concentrations, but not in those with lower concentrations.

drawing Conclusions:
The target transforms leave little doubt that Mn 3 O 4 , MnSO 4 • H 2 O, and hureaulite, or very similar compounds, account for nearly all of the manganese species produced in the auto mobile exhaust studied.The finding of two different groups of samples, one higher in Mn 3 O 4 concentration than the other, is an interesting result. Perhaps surprisingly, which group a sample falls into does not depend in any obvious way on parameters such as driving pattern or mileage.Thus, the primary scientific question-the identities of the manganese species admitted-has been answered. The secondary question as to what controls this speciation led to a mystery that would require further studies to solve.

BoX 16.9 What We Learned and LIKed:
The use of quick-XAFS in a non-time-resolved study is fun! Hey, if it works, it works!The authors took the time to lay out the method of PCA. Their care and clarity has helped make this an important paper in the XAFS literature.In this paper, the conversion between the percentages found directly by linear combination analysis and percent by weight would seem a bit convoluted to a synthetic chemist, because the discussion of the conversion is mixed together with an empirical demonstration of the accuracy of the linear combination approach for a sequence of known mixtures. Of course, you won't be alone. There is a community of others using XAFS for their work, and like any community, we can all help each other. The website xafs.org is an excellent starting point for learning about the resources that are available.There is also another sense in which I hope you won't be alone. In the preface, I introduced the characters who have accompanied us through this text and explained how they are a composite of the important characteristics of every successful scientist who understands XAFS. That's you too, now.So the next time you try to use XAFS to analyze a system that is of interest to you, ask yourself, when you are stuck and feeling overwhelmed, BoX 16.9 What We Learned and LIKed (Continued )The technique of allowing constituent spectra in linear combination to vary in energy calibration, but throwing them out if they showed unphysical shifts, shows a good use of a "reality check."Verifying their linear combination technique on a series of mixtures of known proportions leant credence to their results.Those are interesting samples. I'd never thought of collecting and measuring a sample like that! I like surprises! And the results showing two groups of samples were a surprise! The way they used XPS to help choose target transforms for PCA, and then used those results to perform linear combination analyses, is an excellent example of how techniques can be chained together.

Software LiSt:
TableNote: OS, Operating systems; PF, peak fitting routines (for fingerprinting); LCA, linear combination analysis; PCA, principal component analysis; T. Std, theoretical standards. a F, free; L, Paid license. b W, Windows; M, Mac; L, Linux; †, contact developers. c X, primarily for XANES; E, primarily for EXAFS; B, suitable for both. d L, linear combination analysis for specified sets of standards; CF, combinatorial linear combination analysis. e F, uses FEFF output for theoretical standards; D, uses FDMNES output; N, computes its own theoretical standards. f Allows for fitting parameters using user-specified algebraic expressions. Other modeling programs require you to use a pre-set list of path parameters, in some cases supplemented with a limited selection of constraint options. g Allows multiple data set simultaneous refinements. h Allows restraints. Brown labels refer to regions used to identify the location of components; e.g., a beamline scientist might refer to some particular control as being "at the front end," or "in the hutch."     . Edge fraction)Figure

